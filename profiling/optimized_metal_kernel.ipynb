{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the kernel we get directly from auto-tuning. See profile.ipynb\n",
    "\n",
    "```\n",
    "import tvm\n",
    "\n",
    "from tvm import relax\n",
    "from tvm.relax.frontend import nn\n",
    "\n",
    "from typing import Optional\n",
    "from tvm import te\n",
    "from tvm import dlight as dl\n",
    "from tvm.target import Target\n",
    "import numpy as np\n",
    "import tempfile\n",
    "\n",
    "import timeit\n",
    "\n",
    "from mlc_dac.layers import CachedWNConv1d\n",
    "\n",
    "conv1d = CachedWNConv1d(512, 512, 7, stride=1, dilation=9, padding=0)\n",
    "mod, params = conv1d.export_tvm(\n",
    "    {\"forward\": {\"x\": nn.spec.Tensor((5, 512, 62), \"float32\")}},\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "trials = 2000\n",
    "target = Target.from_device(\"metal\")\n",
    "\n",
    "with target, tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    seq = tvm.transform.Sequential(\n",
    "        [\n",
    "            relax.get_pipeline(\"zero\"),\n",
    "            relax.transform.MetaScheduleTuneTIR(work_dir=tmp_dir, max_trials_global=trials),\n",
    "            relax.transform.MetaScheduleApplyDatabase(work_dir=tmp_dir),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    mod = seq(mod)\n",
    "\n",
    "mod.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.script import ir as I\n",
    "from tvm.script import tir as T\n",
    "from tvm.script import relax as R\n",
    "\n",
    "@I.ir_module\n",
    "class Module:\n",
    "    @T.prim_func\n",
    "    def cached_padding_1d_crop(var_x: T.handle, var_res: T.handle):\n",
    "        T.func_attr({\"op_pattern\": 0})\n",
    "        b, c, out = T.int32(), T.int32(), T.int32()\n",
    "        x = T.match_buffer(var_x, (b, c, out))\n",
    "        n = T.int32()\n",
    "        res = T.match_buffer(var_res, (b, c, n))\n",
    "        # with T.block(\"root\"):\n",
    "        for bb, cc, nn in T.grid(b, c, n):\n",
    "            with T.block(\"res_crop\"):\n",
    "                vb, vc, vn = T.axis.remap(\"SSS\", [bb, cc, nn])\n",
    "                T.reads(x[vb, vc, vn])\n",
    "                T.writes(res[vb, vc, vn])\n",
    "                res[vb, vc, vn] = x[vb, vc, vn]\n",
    "\n",
    "    @T.prim_func\n",
    "    def cached_padding_1d_init(var_cache: T.handle):\n",
    "        T.func_attr({\"op_pattern\": 0})\n",
    "        b, c, p = T.int32(), T.int32(), T.int32()\n",
    "        cache = T.match_buffer(var_cache, (b, c, p))\n",
    "        # with T.block(\"root\"):\n",
    "        for bb, cc, pp in T.grid(b, c, p):\n",
    "            with T.block(\"cache_init\"):\n",
    "                vb, vc, vp = T.axis.remap(\"SSS\", [bb, cc, pp])\n",
    "                T.reads()\n",
    "                T.writes(cache[vb, vc, vp])\n",
    "                cache[vb, vc, vp] = T.float32(0.0)\n",
    "\n",
    "    @T.prim_func\n",
    "    def cached_padding_1d_update(var_cache: T.handle, var_data: T.handle, var_res: T.handle):\n",
    "        T.func_attr({\"op_pattern\": 8})\n",
    "        B, c, p = T.int32(), T.int32(), T.int32()\n",
    "        cache = T.match_buffer(var_cache, (B, c, p))\n",
    "        b, n = T.int32(), T.int32()\n",
    "        data = T.match_buffer(var_data, (b, c, n))\n",
    "        out = T.int32()\n",
    "        res = T.match_buffer(var_res, (b, c, out))\n",
    "        # with T.block(\"root\"):\n",
    "        for bb, cc, oo in T.grid(b, c, out):\n",
    "            with T.block(\"res_update\"):\n",
    "                vb, vc, vo = T.axis.remap(\"SSS\", [bb, cc, oo])\n",
    "                T.reads(cache[vb, vc, vo], data[vb, vc, vo - p])\n",
    "                T.writes(res[vb, vc, vo])\n",
    "                res[vb, vc, vo] = T.if_then_else(vo < p, cache[vb, vc, vo], data[vb, vc, vo - p])\n",
    "        for bb, cc, pp in T.grid(b, c, p):\n",
    "            with T.block(\"cache_update\"):\n",
    "                vb, vc, vp = T.axis.remap(\"SSS\", [bb, cc, pp])\n",
    "                T.reads(res[vb, vc, out - p + vp])\n",
    "                T.writes(cache[vb, vc, vp])\n",
    "                cache[vb, vc, vp] = res[vb, vc, out - p + vp]\n",
    "\n",
    "    @T.prim_func(private=True)\n",
    "    def fused_conv1d_add(x: T.Buffer((1, 512, 62), \"float32\"), weight: T.Buffer((512, 512, 7), \"float32\"), bias: T.Buffer((1, 512, 1), \"float32\"), out: T.Buffer((1, 512, 8), \"float32\")):\n",
    "        T.func_attr({\"tir.is_scheduled\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
    "        # with T.block(\"root\"):\n",
    "        out_local = T.alloc_buffer((1, 512, 8), scope=\"local\")\n",
    "        pad_shared = T.alloc_buffer((1, 512, 62), scope=\"shared\")\n",
    "        weight_shared = T.alloc_buffer((512, 512, 7), scope=\"shared\")\n",
    "        for lbx in T.thread_binding(64, thread=\"blockIdx.x\", annotations={\"pragma_auto_unroll_max_step\": 64, \"pragma_unroll_explicit\": 1}):\n",
    "            for ltx in T.thread_binding(64, thread=\"threadIdx.x\"):\n",
    "                for yy in range(1):\n",
    "                    with T.block(\"conv1d_init\"):\n",
    "                        v_ff = T.axis.spatial(512, lbx * 8 + ltx // 8)\n",
    "                        v_yy = T.axis.spatial(8, ltx % 8 + yy)\n",
    "                        T.reads()\n",
    "                        T.writes(out_local[0, v_ff, v_yy])\n",
    "                        out_local[0, v_ff, v_yy] = T.float32(0.0)\n",
    "                for ic_outer in range(8):\n",
    "                    for shared_fused_outer in range(16):\n",
    "                        for shared_tid in T.thread_binding(64, thread=\"threadIdx.x\"):\n",
    "                            for vec_idx in T.vectorized(4):\n",
    "                                with T.block(\"pad_shared\"):\n",
    "                                    v1 = T.axis.spatial(512, ic_outer * 64 + (shared_fused_outer * 256 + shared_tid * 4 + vec_idx) // 62)\n",
    "                                    v2 = T.axis.spatial(62, (shared_fused_outer * 256 + shared_tid * 4 + vec_idx) % 62)\n",
    "                                    T.where((shared_fused_outer * 64 + shared_tid) * 4 + vec_idx < 3968)\n",
    "                                    T.reads(x[0, v1, v2])\n",
    "                                    T.writes(pad_shared[0, v1, v2])\n",
    "                                    pad_shared[0, v1, v2] = x[0, v1, v2]\n",
    "                    for weight_outer in range(56):\n",
    "                        for shared_tid in T.thread_binding(64, thread=\"threadIdx.x\"):\n",
    "                            with T.block(\"wnconv1d_shared\"):\n",
    "                                v0 = T.axis.spatial(512, lbx * 8 + (weight_outer * 64 + shared_tid) // 448)\n",
    "                                v1 = T.axis.spatial(512, ic_outer * 64 + (weight_outer * 64 + shared_tid) % 448 // 7)\n",
    "                                v2 = T.axis.spatial(7, (weight_outer * 64 + shared_tid) % 7)\n",
    "                                T.reads(weight[v0, v1, v2])\n",
    "                                T.writes(weight_shared[v0, v1, v2])\n",
    "                                weight_shared[v0, v1, v2] = weight[v0, v1, v2]\n",
    "                    for rc_1, yy, rc_2, ry_2 in T.grid(16, 1, 4, 7):\n",
    "                        with T.block(\"conv1d_ncw_update\"):\n",
    "                            v_ff = T.axis.spatial(512, lbx * 8 + ltx // 8)\n",
    "                            v_yy = T.axis.spatial(8, ltx % 8 + yy)\n",
    "                            v_rc = T.axis.reduce(512, ic_outer * 64 + rc_1 * 4 + rc_2)\n",
    "                            v_ry = T.axis.reduce(7, ry_2)\n",
    "                            T.reads(out_local[0, v_ff, v_yy], pad_shared[0, v_rc, v_ry * 9 + v_yy], weight_shared[v_ff, v_rc, v_ry])\n",
    "                            T.writes(out_local[0, v_ff, v_yy])\n",
    "                            out_local[0, v_ff, v_yy] = out_local[0, v_ff, v_yy] + pad_shared[0, v_rc, v_ry * 9 + v_yy] * weight_shared[v_ff, v_rc, v_ry]\n",
    "                for yy in range(1):\n",
    "                    with T.block(\"conv1d_ncw_intermediate_local\"):\n",
    "                        v1 = T.axis.spatial(512, lbx * 8 + ltx // 8)\n",
    "                        v2 = T.axis.spatial(8, ltx % 8 + yy)\n",
    "                        T.reads(out_local[0, v1, v2], bias[0, v1, 0])\n",
    "                        T.writes(out[0, v1, v2])\n",
    "                        out[0, v1, v2] = out_local[0, v1, v2] + bias[0, v1, 0]\n",
    "\n",
    "    @T.prim_func(private=True)\n",
    "    def fused_tir_sqrt_divide_multiply(lv4: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), \"float32\"), weight_v: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), \"float32\"), weight_g: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), \"float32\"), T_multiply_intermediate: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), \"float32\")):\n",
    "        T.func_attr({\"tir.is_scheduled\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
    "        # with T.block(\"root\"):\n",
    "        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread=\"blockIdx.x\"):\n",
    "            for ax0_ax1_ax2_fused_2 in T.thread_binding(T.int64(1024), thread=\"threadIdx.x\"):\n",
    "                for ax0_ax1_ax2_fused_0 in range(T.int64(7)):\n",
    "                    with T.block(\"T_divide\"):\n",
    "                        v_ax0 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(262144) + ax0_ax1_ax2_fused_1 * T.int64(1024) + ax0_ax1_ax2_fused_2) // T.int64(3584))\n",
    "                        v_ax1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(262144) + ax0_ax1_ax2_fused_1 * T.int64(1024) + ax0_ax1_ax2_fused_2) % T.int64(3584) // T.int64(7))\n",
    "                        v_ax2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(262144) + ax0_ax1_ax2_fused_1 * T.int64(1024) + ax0_ax1_ax2_fused_2) % T.int64(7))\n",
    "                        T.reads(weight_g[v_ax0, T.int64(0), T.int64(0)], weight_v[v_ax0, v_ax1, v_ax2], lv4[v_ax0, T.int64(0), T.int64(0)])\n",
    "                        T.writes(T_multiply_intermediate[v_ax0, v_ax1, v_ax2])\n",
    "                        T_multiply_intermediate[v_ax0, v_ax1, v_ax2] = weight_g[v_ax0, T.int64(0), T.int64(0)] * (weight_v[v_ax0, v_ax1, v_ax2] / T.sqrt(lv4[v_ax0, T.int64(0), T.int64(0)]))\n",
    "\n",
    "    @T.prim_func(private=True)\n",
    "    def fused_tir_square_sum(weight_v: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), \"float32\"), lv3_red_intermediate: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), \"float32\")):\n",
    "        T.func_attr({\"tir.is_scheduled\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
    "        # with T.block(\"root\"):\n",
    "        for ax0_ax1_ax2_fused in T.thread_binding(T.int64(512), thread=\"blockIdx.x\", annotations={\"pragma_auto_unroll_max_step\": 512, \"pragma_unroll_explicit\": 1}):\n",
    "            for k1_k2_fused_0 in range(T.int64(112)):\n",
    "                for k1_k2_fused_1 in T.thread_binding(T.int64(32), thread=\"threadIdx.x\"):\n",
    "                    with T.block(\"lv3_red\"):\n",
    "                        v_ax0 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_fused)\n",
    "                        v_ax1 = T.axis.spatial(T.int64(1), T.int64(0))\n",
    "                        v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))\n",
    "                        v_k1 = T.axis.reduce(T.int64(512), (k1_k2_fused_0 * T.int64(32) + k1_k2_fused_1) // T.int64(7))\n",
    "                        v_k2 = T.axis.reduce(T.int64(7), (k1_k2_fused_0 * T.int64(32) + k1_k2_fused_1) % T.int64(7))\n",
    "                        T.reads(weight_v[v_ax0, v_k1, v_k2])\n",
    "                        T.writes(lv3_red_intermediate[v_ax0, v_ax1, v_ax2])\n",
    "                        with T.init():\n",
    "                            lv3_red_intermediate[v_ax0, v_ax1, v_ax2] = T.float32(0.0)\n",
    "                        lv3_red_intermediate[v_ax0, v_ax1, v_ax2] = lv3_red_intermediate[v_ax0, v_ax1, v_ax2] + weight_v[v_ax0, v_k1, v_k2] * weight_v[v_ax0, v_k1, v_k2]\n",
    "\n",
    "    @T.prim_func(private=True)\n",
    "    def reshape(bias: T.Buffer((T.int64(512),), \"float32\"), T_reshape: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), \"float32\")):\n",
    "        T.func_attr({\"op_pattern\": 2, \"tir.is_scheduled\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
    "        # with T.block(\"root\"):\n",
    "        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(2), thread=\"blockIdx.x\"):\n",
    "            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread=\"threadIdx.x\"):\n",
    "                with T.block(\"T_reshape\"):\n",
    "                    v_ax0 = T.axis.spatial(T.int64(1), T.int64(0))\n",
    "                    v_ax1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_fused_0 * T.int64(256) + ax0_ax1_ax2_fused_1)\n",
    "                    v_ax2 = T.axis.spatial(T.int64(1), T.int64(0))\n",
    "                    T.reads(bias[(v_ax1 + v_ax2) % T.int64(512)])\n",
    "                    T.writes(T_reshape[v_ax0, v_ax1, v_ax2])\n",
    "                    T_reshape[v_ax0, v_ax1, v_ax2] = bias[(v_ax1 + v_ax2) % T.int64(512)]\n",
    "\n",
    "    @R.function\n",
    "    def _initialize_effect() -> R.Tuple(R.Object, R.Object, R.Object):\n",
    "        cls = Module\n",
    "        with R.dataflow():\n",
    "            _io: R.Object = R.null_value()\n",
    "            cache_cache: R.Object = R.call_pure_packed(\"vm.builtin.cached_padding_1d_create\", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))\n",
    "            downsampling_delay_cache: R.Object = R.call_pure_packed(\"vm.builtin.cached_padding_1d_create\", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))\n",
    "            gv: R.Tuple(R.Object, R.Object, R.Object) = _io, cache_cache, downsampling_delay_cache\n",
    "            R.output(gv)\n",
    "        return gv\n",
    "\n",
    "    @R.function\n",
    "    def forward(x: R.Tensor((1, 512, 62), dtype=\"float32\"), _io: R.Object, cache_cache: R.Object, downsampling_delay_cache: R.Object, weight_g: R.Tensor((512, 1, 1), dtype=\"float32\"), weight_v: R.Tensor((512, 512, 7), dtype=\"float32\"), bias: R.Tensor((512,), dtype=\"float32\")) -> R.Tuple(R.Tensor((1, 512, 8), dtype=\"float32\"), R.Tuple(R.Object, R.Object, R.Object)):\n",
    "        R.func_attr({\"num_input\": 4})\n",
    "        cls = Module\n",
    "        with R.dataflow():\n",
    "            lv1: R.Tensor((1, 512, 62), dtype=\"float32\") = R.call_pure_packed(\"vm.builtin.cached_padding_1d_update\", downsampling_delay_cache, x, sinfo_args=(R.Tensor((1, 512, 62), dtype=\"float32\"),))\n",
    "            lv2: R.Tensor((1, 512, 62), dtype=\"float32\") = R.call_pure_packed(\"vm.builtin.cached_padding_1d_update\", cache_cache, lv1, sinfo_args=(R.Tensor((1, 512, 62), dtype=\"float32\"),))\n",
    "            lv = R.call_tir(cls.fused_tir_square_sum, (weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype=\"float32\"))\n",
    "            lv1_1 = R.call_tir(cls.fused_tir_sqrt_divide_multiply, (lv, weight_v, weight_g), out_sinfo=R.Tensor((512, 512, 7), dtype=\"float32\"))\n",
    "            lv8 = R.call_tir(cls.reshape, (bias,), out_sinfo=R.Tensor((1, 512, 1), dtype=\"float32\"))\n",
    "            lv2_1 = R.call_tir(cls.fused_conv1d_add, (lv2, lv1_1, lv8), out_sinfo=R.Tensor((1, 512, 8), dtype=\"float32\"))\n",
    "            gv1: R.Tuple(R.Tensor((1, 512, 8), dtype=\"float32\"), R.Tuple(R.Object, R.Object, R.Object)) = lv2_1, (_io, cache_cache, downsampling_delay_cache)\n",
    "            R.output(gv1)\n",
    "        return gv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we build and run the auto-tuned TIR kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "   0.1427       0.1405       0.1633       0.1308       0.0111                  \n",
      "[[[ 2.824526   -1.3132825   3.8338013  ... -1.8539417  -0.52261025\n",
      "   -0.92020506]\n",
      "  [ 2.191527    1.5215199   1.6404716  ...  1.4687572   0.7257114\n",
      "    2.0899653 ]\n",
      "  [ 1.780653    1.9595389   1.6723082  ... -0.44428122  1.0687006\n",
      "    0.28043497]\n",
      "  ...\n",
      "  [-2.4418955  -2.6515665  -1.9757025  ... -1.0010937  -1.3631712\n",
      "   -0.07432413]\n",
      "  [ 2.2440364   1.9657502   1.8604834  ... -1.0799818   1.519831\n",
      "    0.00400889]\n",
      "  [-0.43892244  0.04964514 -1.30905    ... -1.3583254  -1.4844707\n",
      "   -0.57526046]]]\n",
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "   0.1312       0.1333       0.1388       0.1177       0.0072                  \n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "\n",
    "from tvm import relax\n",
    "from tvm.relax.frontend import nn\n",
    "\n",
    "from typing import Optional\n",
    "from tvm import te\n",
    "from tvm import dlight as dl\n",
    "from tvm.target import Target\n",
    "import numpy as np\n",
    "\n",
    "mod = Module\n",
    "\n",
    "device = tvm.metal()\n",
    "target = Target.from_device(\"metal\")\n",
    "with target:\n",
    "    seq = dl.ApplyDefaultSchedule(\n",
    "        dl.gpu.Fallback(),\n",
    "    )\n",
    "    vm_mod = seq(mod)\n",
    "\n",
    "ex = relax.build(vm_mod, target)\n",
    "vm = relax.VirtualMachine(ex, device, profile=True)\n",
    "effects = vm.module[\"_initialize_effect\"]()\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "weight_g = np.random.randn(512, 1, 1).astype(\"float32\")\n",
    "weight_v = np.random.randn(512, 512, 7).astype(\"float32\")\n",
    "bias = np.random.randn(512).astype(\"float32\")\n",
    "\n",
    "tvm_params = [weight_g, weight_v, bias]\n",
    "tvm_params = [tvm.nd.array(param, device=device) for param in tvm_params]\n",
    "\n",
    "audio_data = np.random.randn(1, 512, 62).astype(\"float32\")\n",
    "audio_data = tvm.nd.array(audio_data, device=device)\n",
    "\n",
    "time_eval = vm.time_evaluator(\"forward\", device, 10, 5)(audio_data, *effects, *tvm_params)\n",
    "print(time_eval)\n",
    "\n",
    "out = vm.module[\"forward\"](audio_data, *effects, *tvm_params)\n",
    "print(out[0].asnumpy())\n",
    "\n",
    "time_eval = vm.time_evaluator(\"forward\", device, 10, 5)(audio_data, *effects, *tvm_params)\n",
    "print(time_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try to write a kernel with the optimized schedule (Notice the tiling size is the same) for generic input shapes. The `conv1d` kernel takes shapes as inputs and generates a TIR kernel with the schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[01:37:15] /Users/cfruan/Documents/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[01:37:15] /Users/cfruan/Documents/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[01:37:15] /Users/cfruan/Documents/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n"
     ]
    }
   ],
   "source": [
    "from tvm.script import ir as I\n",
    "from tvm.script import tir as T\n",
    "from tvm.script import relax as R\n",
    "\n",
    "def get_conv1d_fn(x, weight, stride, dilation):\n",
    "    NUM_BLKS = 64\n",
    "    bdx = 64\n",
    "\n",
    "    (_, C_in, N), model_dtype = x.shape, x.dtype\n",
    "    (C_out, C_in, K), storage_dtype = weight.shape, weight.dtype\n",
    "    O = (N - dilation * (K - 1) - 1) // stride + 1\n",
    "\n",
    "    out_ch_per_block = (C_out + NUM_BLKS - 1) // NUM_BLKS\n",
    "    threads_per_out_ch = bdx // out_ch_per_block\n",
    "    spatial_per_thread = (O + threads_per_out_ch - 1) // threads_per_out_ch\n",
    "\n",
    "    assert C_in % 16 == 0, \"C_in must be a multiple of 16\"\n",
    "    in_ch_per_block = C_in // 8\n",
    "    weight_per_block = K * in_ch_per_block\n",
    "\n",
    "    rc1 = 16\n",
    "    rc2 = in_ch_per_block // rc1\n",
    "\n",
    "    vec_blk = 4 * bdx\n",
    "\n",
    "    @T.prim_func()\n",
    "    def fused_conv1d_add(var_x: T.handle, weight: T.Buffer((C_out, C_in, K), storage_dtype), bias: T.Buffer((C_out), storage_dtype), var_out: T.handle):\n",
    "        # T.func_attr({\"tir.is_scheduled\": T.bool(True), \"tir.noalias\": T.bool(True)})\n",
    "        x = T.match_buffer(var_x, (1, C_in, N), model_dtype)\n",
    "        out = T.match_buffer(var_out, (1, C_out, O), model_dtype)\n",
    "        \n",
    "        out_local = T.alloc_buffer((1, C_out, O), scope=\"local\")\n",
    "        pad_shared = T.alloc_buffer((1, C_in, N), scope=\"shared\")\n",
    "        weight_shared = T.alloc_buffer((C_out, C_in, K), scope=\"shared\")\n",
    "\n",
    "        for lbx in T.thread_binding(NUM_BLKS, thread=\"blockIdx.x\", annotations={\"pragma_auto_unroll_max_step\": 64, \"pragma_unroll_explicit\": 1}):\n",
    "            for vtx in T.thread_binding(T.int64(1), thread=\"vthread.x\"):\n",
    "                for ltx in T.thread_binding(bdx, thread=\"threadIdx.x\"):\n",
    "                    for yy in range(spatial_per_thread):\n",
    "                        with T.block(\"conv1d_init\"):\n",
    "                            v_ff = T.axis.spatial(C_out, lbx * out_ch_per_block + ltx // threads_per_out_ch)\n",
    "                            v_yy = T.axis.spatial(O, ltx % threads_per_out_ch * spatial_per_thread + yy)\n",
    "                            T.reads()\n",
    "                            T.writes(out_local[0, v_ff, v_yy])\n",
    "                            out_local[0, v_ff, v_yy] = T.float32(0.0)\n",
    "                    for ic_outer in range(T.int32(8)):\n",
    "                        for shared_fused_outer in range(T.ceildiv(in_ch_per_block * N, bdx * 4)):\n",
    "                            for shared_tid in T.thread_binding(bdx, thread=\"threadIdx.x\"):\n",
    "                                for vec_idx in T.vectorized(T.int32(4)):\n",
    "                                    with T.block(\"pad_shared\"):\n",
    "                                        v1 = T.axis.spatial(C_in, ic_outer * in_ch_per_block + (shared_fused_outer * vec_blk + shared_tid * T.int32(4) + vec_idx) // N)\n",
    "                                        v2 = T.axis.spatial(N, (shared_fused_outer * vec_blk + shared_tid * T.int32(4) + vec_idx) % N)\n",
    "                                        T.where((shared_fused_outer * bdx + shared_tid) * T.int32(4) + vec_idx < in_ch_per_block * N)\n",
    "                                        T.reads(x[0, v1, v2])\n",
    "                                        T.writes(pad_shared[0, v1, v2])\n",
    "                                        pad_shared[0, v1, v2] = x[0, v1, v2]\n",
    "                        for weight_outer in range(T.ceildiv(weight_per_block * out_ch_per_block, bdx)):\n",
    "                            for shared_tid in T.thread_binding(bdx, thread=\"threadIdx.x\"):\n",
    "                                with T.block(\"wnconv1d_shared\"):\n",
    "                                    v0 = T.axis.spatial(C_out, lbx * out_ch_per_block + (weight_outer * bdx + shared_tid) // weight_per_block)\n",
    "                                    v1 = T.axis.spatial(C_in, ic_outer * in_ch_per_block + (weight_outer * bdx + shared_tid) % weight_per_block // K)\n",
    "                                    v2 = T.axis.spatial(K, (weight_outer * bdx + shared_tid) % K)\n",
    "                                    T.reads(weight[v0, v1, v2])\n",
    "                                    T.writes(weight_shared[v0, v1, v2])\n",
    "                                    weight_shared[v0, v1, v2] = weight[v0, v1, v2]\n",
    "                        for rc_1, yy, rc_2, ry_2 in T.grid(rc1, spatial_per_thread, rc2, K):\n",
    "                            with T.block(\"conv1d_ncw_update\"):\n",
    "                                v_ff = T.axis.spatial(C_out, lbx * out_ch_per_block + ltx // threads_per_out_ch)\n",
    "                                v_yy = T.axis.spatial(O, ltx % threads_per_out_ch * spatial_per_thread + yy)\n",
    "                                v_rc = T.axis.reduce(C_in, ic_outer * in_ch_per_block + rc_1 * rc2 + rc_2)\n",
    "                                v_ry = T.axis.reduce(K, ry_2)\n",
    "                                T.reads(out_local[0, v_ff, v_yy], pad_shared[0, v_rc, v_ry * dilation + v_yy * stride], weight_shared[v_ff, v_rc, v_ry])\n",
    "                                T.writes(out_local[0, v_ff, v_yy])\n",
    "                                out_local[0, v_ff, v_yy] = out_local[0, v_ff, v_yy] + pad_shared[0, v_rc, v_ry * dilation + v_yy * stride] * weight_shared[v_ff, v_rc, v_ry]\n",
    "                    for yy in range(spatial_per_thread):\n",
    "                        with T.block(\"conv1d_ncw_intermediate_local\"):\n",
    "                            v1 = T.axis.spatial(C_out, lbx * out_ch_per_block + ltx // threads_per_out_ch)\n",
    "                            v2 = T.axis.spatial(O, ltx % threads_per_out_ch * spatial_per_thread + yy)\n",
    "                            T.reads(out_local[0, v1, v2], bias[v1])\n",
    "                            T.writes(out[0, v1, v2])\n",
    "                            out[0, v1, v2] = out_local[0, v1, v2] + bias[v1]\n",
    "\n",
    "    return fused_conv1d_add\n",
    "\n",
    "    # return nn.op.tensor_ir_op(\n",
    "    #     fused_conv1d_add,\n",
    "    #     \"fused_conv1d_add\",\n",
    "    #     args=[x, weight, bias],\n",
    "    #     out=nn.Tensor.placeholder((1, C_out, O), dtype=model_dtype),\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then evaluate the performance and correctness of the generated kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
       "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
       "\n",
       "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">fused_conv1d_add</span>(x: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">62</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), weight: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">7</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), bias: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">512</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), out: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">8</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
       "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
       "        out_local <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">8</span>), scope<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;local&quot;</span>)\n",
       "        pad_shared <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">62</span>), scope<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;shared&quot;</span>)\n",
       "        weight_shared <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">512</span>, <span style=\"color: #008000\">7</span>), scope<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;shared&quot;</span>)\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> lbx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(<span style=\"color: #008000\">64</span>, thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;blockIdx.x&quot;</span>, annotations<span style=\"color: #AA22FF; font-weight: bold\">=</span>{<span style=\"color: #BA2121\">&quot;pragma_auto_unroll_max_step&quot;</span>: <span style=\"color: #008000\">64</span>, <span style=\"color: #BA2121\">&quot;pragma_unroll_explicit&quot;</span>: <span style=\"color: #008000\">1</span>}):\n",
       "            <span style=\"color: #008000; font-weight: bold\">for</span> vtx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;vthread.x&quot;</span>):\n",
       "                <span style=\"color: #008000; font-weight: bold\">for</span> ltx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(<span style=\"color: #008000\">64</span>, thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;threadIdx.x&quot;</span>):\n",
       "                    <span style=\"color: #008000; font-weight: bold\">for</span> yy <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">1</span>):\n",
       "                        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv1d_init&quot;</span>):\n",
       "                            v_ff <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">512</span>, lbx <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ltx <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>)\n",
       "                            v_yy <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">8</span>, ltx <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> yy)\n",
       "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads()\n",
       "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(out_local[<span style=\"color: #008000\">0</span>, v_ff, v_yy])\n",
       "                            out_local[<span style=\"color: #008000\">0</span>, v_ff, v_yy] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0.0</span>)\n",
       "                    <span style=\"color: #008000; font-weight: bold\">for</span> ic_outer <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">8</span>):\n",
       "                        <span style=\"color: #008000; font-weight: bold\">for</span> shared_fused_outer <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">16</span>):\n",
       "                            <span style=\"color: #008000; font-weight: bold\">for</span> shared_tid <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(<span style=\"color: #008000\">64</span>, thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;threadIdx.x&quot;</span>):\n",
       "                                <span style=\"color: #008000; font-weight: bold\">for</span> vec_idx <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">4</span>):\n",
       "                                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;pad_shared&quot;</span>):\n",
       "                                        v1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">512</span>, ic_outer <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> (shared_fused_outer <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">256</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> shared_tid <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> vec_idx) <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">62</span>)\n",
       "                                        v2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">62</span>, (shared_fused_outer <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">256</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> shared_tid <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> vec_idx) <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">62</span>)\n",
       "                                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>where((shared_fused_outer <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> shared_tid) <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> vec_idx <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">3968</span>)\n",
       "                                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(x[<span style=\"color: #008000\">0</span>, v1, v2])\n",
       "                                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(pad_shared[<span style=\"color: #008000\">0</span>, v1, v2])\n",
       "                                        pad_shared[<span style=\"color: #008000\">0</span>, v1, v2] <span style=\"color: #AA22FF; font-weight: bold\">=</span> x[<span style=\"color: #008000\">0</span>, v1, v2]\n",
       "                        <span style=\"color: #008000; font-weight: bold\">for</span> weight_outer <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">56</span>):\n",
       "                            <span style=\"color: #008000; font-weight: bold\">for</span> shared_tid <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(<span style=\"color: #008000\">64</span>, thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;threadIdx.x&quot;</span>):\n",
       "                                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;wnconv1d_shared&quot;</span>):\n",
       "                                    v0 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">512</span>, lbx <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> (weight_outer <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> shared_tid) <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">448</span>)\n",
       "                                    v1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">512</span>, ic_outer <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> (weight_outer <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> shared_tid) <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">448</span> <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">7</span>)\n",
       "                                    v2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">7</span>, (weight_outer <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> shared_tid) <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">7</span>)\n",
       "                                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(weight[v0, v1, v2])\n",
       "                                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(weight_shared[v0, v1, v2])\n",
       "                                    weight_shared[v0, v1, v2] <span style=\"color: #AA22FF; font-weight: bold\">=</span> weight[v0, v1, v2]\n",
       "                        <span style=\"color: #008000; font-weight: bold\">for</span> rc_1, yy, rc_2, ry_2 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">7</span>):\n",
       "                            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv1d_ncw_update&quot;</span>):\n",
       "                                v_ff <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">512</span>, lbx <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ltx <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>)\n",
       "                                v_yy <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">8</span>, ltx <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> yy)\n",
       "                                v_rc <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">512</span>, ic_outer <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">64</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> rc_1 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> rc_2)\n",
       "                                v_ry <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">7</span>, ry_2)\n",
       "                                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(out_local[<span style=\"color: #008000\">0</span>, v_ff, v_yy], pad_shared[<span style=\"color: #008000\">0</span>, v_rc, v_ry <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">9</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_yy], weight_shared[v_ff, v_rc, v_ry])\n",
       "                                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(out_local[<span style=\"color: #008000\">0</span>, v_ff, v_yy])\n",
       "                                out_local[<span style=\"color: #008000\">0</span>, v_ff, v_yy] <span style=\"color: #AA22FF; font-weight: bold\">=</span> out_local[<span style=\"color: #008000\">0</span>, v_ff, v_yy] <span style=\"color: #AA22FF; font-weight: bold\">+</span> pad_shared[<span style=\"color: #008000\">0</span>, v_rc, v_ry <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">9</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> v_yy] <span style=\"color: #AA22FF; font-weight: bold\">*</span> weight_shared[v_ff, v_rc, v_ry]\n",
       "                    <span style=\"color: #008000; font-weight: bold\">for</span> yy <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">1</span>):\n",
       "                        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;conv1d_ncw_intermediate_local&quot;</span>):\n",
       "                            v1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">512</span>, lbx <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ltx <span style=\"color: #AA22FF; font-weight: bold\">//</span> <span style=\"color: #008000\">8</span>)\n",
       "                            v2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">8</span>, ltx <span style=\"color: #AA22FF; font-weight: bold\">%</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> yy)\n",
       "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(out_local[<span style=\"color: #008000\">0</span>, v1, v2], bias[v1])\n",
       "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(out[<span style=\"color: #008000\">0</span>, v1, v2])\n",
       "                            out[<span style=\"color: #008000\">0</span>, v1, v2] <span style=\"color: #AA22FF; font-weight: bold\">=</span> out_local[<span style=\"color: #008000\">0</span>, v1, v2] <span style=\"color: #AA22FF; font-weight: bold\">+</span> bias[v1]\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "   0.0754       0.0722       0.0927       0.0692       0.0088                  \n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "\n",
    "from tvm import relax\n",
    "\n",
    "from tvm import dlight as dl\n",
    "from tvm.target import Target\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "mod = tvm.IRModule()\n",
    "mod[\"fused_conv1d_add\"] = get_conv1d_fn(\n",
    "    tvm.te.placeholder((1, 512, 62), dtype=\"float32\"),\n",
    "    tvm.te.placeholder((512, 512, 7), dtype=\"float32\"),\n",
    "    1,\n",
    "    9,\n",
    ")\n",
    "mod.show()\n",
    "\n",
    "device = tvm.metal()\n",
    "target = Target.from_device(\"metal\")\n",
    "with target:\n",
    "    seq = dl.ApplyDefaultSchedule(\n",
    "        dl.gpu.Fallback(),\n",
    "    )\n",
    "    vm_mod = seq(mod)\n",
    "\n",
    "rt_mod = tvm.build(vm_mod, target=target)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "weight = np.random.randn(512, 512, 7).astype(\"float32\")\n",
    "bias = np.random.randn(512).astype(\"float32\")\n",
    "\n",
    "tvm_params = [weight, bias]\n",
    "tvm_params = [tvm.nd.array(param, device=device) for param in tvm_params]\n",
    "\n",
    "out = tvm.nd.empty((1, 512, 8), device=device)\n",
    "\n",
    "audio_data = np.random.randn(1, 512, 62).astype(\"float32\")\n",
    "audio_data = tvm.nd.array(audio_data, device=device)\n",
    "\n",
    "time_eval = rt_mod.time_evaluator(\"fused_conv1d_add\", device, 10, 5)(audio_data, *tvm_params, out)\n",
    "print(time_eval)\n",
    "\n",
    "rt_mod[\"fused_conv1d_add\"](audio_data, *tvm_params, out)\n",
    "out_relax = out.asnumpy()\n",
    "out_torch = torch.nn.functional.conv1d(torch.tensor(audio_data.asnumpy()), torch.tensor(weight), torch.tensor(bias), stride=1, dilation=9)\n",
    "print(np.allclose(out_relax, out_torch.numpy(), atol=1e-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now measure the speedup before and after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch with MPS\n",
      "0.0425ms\n",
      "Torch with CPU\n",
      "0.6068ms\n",
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "   0.4154       0.3763       0.5601       0.3617       0.0747                  \n",
      "TVM without tuning\n",
      "0.1573ms\n",
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "   0.0820       0.0784       0.0890       0.0770       0.0052                  \n",
      "TVM after tuning\n",
      "0.0235ms\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "weight = np.random.randn(512, 512, 7).astype(\"float32\")\n",
    "bias = np.random.randn(512).astype(\"float32\")\n",
    "data = np.random.randn(1, 512, 62).astype(\"float32\")\n",
    "\n",
    "# Pytorch MPS\n",
    "\n",
    "import torch\n",
    "from tvm.relax.frontend import nn\n",
    "\n",
    "weight_torch = torch.tensor(weight).to(\"mps\")\n",
    "bias_torch = torch.tensor(bias).to(\"mps\")\n",
    "data_torch = torch.tensor(data).to(\"mps\")\n",
    "\n",
    "conv1d_torch = torch.nn.Conv1d(512, 512, 7, stride=1, padding=0, dilation=9).to(\"mps\")\n",
    "conv1d_torch.weight.data = weight_torch\n",
    "conv1d_torch.bias.data = bias_torch\n",
    "\n",
    "# Pytorch CPU\n",
    "\n",
    "weight_torch_cpu = torch.tensor(weight).to(\"cpu\")\n",
    "bias_torch_cpu = torch.tensor(bias).to(\"cpu\")\n",
    "data_torch_cpu = torch.tensor(data).to(\"cpu\")\n",
    "\n",
    "conv1d_torch_cpu = torch.nn.Conv1d(512, 512, 7, stride=1, padding=0, dilation=9).to(\"cpu\")\n",
    "conv1d_torch_cpu.weight.data = weight_torch_cpu\n",
    "conv1d_torch_cpu.bias.data = bias_torch_cpu\n",
    "\n",
    "import time\n",
    "\n",
    "def time_it(func, warmup=10, runs=100):\n",
    "    with torch.no_grad():\n",
    "        # Warmup\n",
    "        for _ in range(warmup):\n",
    "            func()\n",
    "        \n",
    "        # Timing\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(runs):\n",
    "            func()\n",
    "        avg_time = (time.perf_counter() - start) / runs\n",
    "    return f\"{avg_time*1000:.4f}ms\"\n",
    "\n",
    "print(\"Torch with MPS\")\n",
    "result = time_it(lambda: conv1d_torch(data_torch))\n",
    "print(result)\n",
    "\n",
    "print(\"Torch with CPU\")\n",
    "result = time_it(lambda: conv1d_torch_cpu(data_torch_cpu))\n",
    "print(result)\n",
    "\n",
    "# Before\n",
    "\n",
    "relax_mod = nn.Conv1D(512, 512, 7, stride=1, padding=0, dilation=9)\n",
    "mod, _ = relax_mod.export_tvm({\n",
    "    \"forward\": {\n",
    "        \"x\": nn.spec.Tensor([1, 512, 62], \"float32\"),\n",
    "    }\n",
    "})\n",
    "\n",
    "device = tvm.metal()\n",
    "target = Target.from_device(\"metal\")\n",
    "with target:\n",
    "    seq = tvm.transform.Sequential(\n",
    "        [\n",
    "            tvm.relax.transform.LegalizeOps(),\n",
    "            tvm.relax.transform.AnnotateTIROpPattern(),\n",
    "            tvm.relax.transform.FoldConstant(),\n",
    "            tvm.relax.transform.FuseOps(),\n",
    "            tvm.relax.transform.FuseTIR(),\n",
    "            dl.ApplyDefaultSchedule(\n",
    "                dl.gpu.Matmul(),\n",
    "                dl.gpu.GEMV(),\n",
    "                dl.gpu.Reduction(),\n",
    "                dl.gpu.GeneralReduction(),\n",
    "                dl.gpu.Fallback(),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    vm_mod = seq(mod)\n",
    "\n",
    "ex = relax.build(vm_mod, target)\n",
    "vm = relax.VirtualMachine(ex, device, profile=True)\n",
    "\n",
    "tvm_params = [weight, bias]\n",
    "tvm_params = [tvm.nd.array(param, device=device) for param in tvm_params]\n",
    "\n",
    "data = tvm.nd.array(data, device=device)\n",
    "\n",
    "time_eval = vm.time_evaluator(\"forward\", device, 10, 5)(data, *tvm_params)\n",
    "print(time_eval)\n",
    "\n",
    "def time_tvm(fn, data, tvm_params, warmup=10, runs=100):\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        fn(data, *tvm_params)\n",
    "    \n",
    "    # Timing\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(runs):\n",
    "        fn(data, *tvm_params)\n",
    "    avg_time = (time.perf_counter() - start) / runs\n",
    "    return f\"{avg_time*1000:.4f}ms\"\n",
    "\n",
    "print(\"TVM without tuning\")\n",
    "avg_time = time_tvm(vm[\"forward\"], data, tvm_params)\n",
    "print(avg_time)\n",
    "\n",
    "# After\n",
    "\n",
    "\n",
    "bb = relax.BlockBuilder()\n",
    "conv1d_fn = get_conv1d_fn(\n",
    "    tvm.te.placeholder((1, 512, 62), dtype=\"float32\"),\n",
    "    tvm.te.placeholder((512, 512, 7), dtype=\"float32\"),\n",
    "    1,\n",
    "    9,\n",
    ")\n",
    "\n",
    "x = relax.Var(\"x\", R.Tensor([1, 512, 62], \"float32\"))\n",
    "weight = relax.Var(\"weight\", R.Tensor([512, 512, 7], \"float32\"))\n",
    "bias = relax.Var(\"bias\", R.Tensor([512], \"float32\"))\n",
    "\n",
    "with bb.function(\"forward\", [x, weight, bias]):\n",
    "    with bb.dataflow():\n",
    "        tir_conv1d = bb.add_func(conv1d_fn, \"fused_conv1d_add\")\n",
    "        gv = bb.emit(\n",
    "            relax.call_tir(tir_conv1d, [x, weight, bias], out_sinfo=relax.TensorStructInfo([1, 512, 8], \"float32\"))\n",
    "        )\n",
    "        bb.emit_output(gv)\n",
    "    bb.emit_func_output(gv)\n",
    "\n",
    "mod = bb.get()\n",
    "\n",
    "with target:\n",
    "    seq = tvm.transform.Sequential(\n",
    "        [\n",
    "            tvm.relax.transform.LegalizeOps(),\n",
    "            tvm.relax.transform.AnnotateTIROpPattern(),\n",
    "            tvm.relax.transform.FoldConstant(),\n",
    "            tvm.relax.transform.FuseOps(),\n",
    "            tvm.relax.transform.FuseTIR(),\n",
    "            dl.ApplyDefaultSchedule(\n",
    "                dl.gpu.Matmul(),\n",
    "                dl.gpu.GEMV(),\n",
    "                dl.gpu.Reduction(),\n",
    "                dl.gpu.GeneralReduction(),\n",
    "                dl.gpu.Fallback(),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    vm_mod = seq(mod)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "weight = np.random.randn(512, 512, 7).astype(\"float32\")\n",
    "bias = np.random.randn(512).astype(\"float32\")\n",
    "\n",
    "tvm_params = [weight, bias]\n",
    "tvm_params = [tvm.nd.array(param, device=device) for param in tvm_params]\n",
    "\n",
    "ex = relax.build(vm_mod, target=target)\n",
    "vm = relax.VirtualMachine(ex, device, profile=True)\n",
    "time_eval_after = vm.time_evaluator(\"forward\", device, 10, 5)(data, *tvm_params)\n",
    "print(time_eval_after)\n",
    "\n",
    "\n",
    "print(\"TVM after tuning\")\n",
    "avg_time = avg_time = time_tvm(vm[\"forward\"], data, tvm_params)\n",
    "print(avg_time)\n",
    "\n",
    "# report = vm.profile(\"forward\", data, *tvm_params)\n",
    "# print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
