{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlc_dac.dac import DAC\n",
    "\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.relax.frontend.torch import dynamo_capture_subgraphs\n",
    "from tvm.relax.frontend.torch import from_fx\n",
    "from tvm.script import relax as R\n",
    "\n",
    "import torch\n",
    "\n",
    "dac = DAC(512)\n",
    "mod, named_params = dac.export_tvm(dac.get_default_spec(), debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_initialize_effect\n",
      "decode\n",
      "encode\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_relax_funcnames(mod: tvm.IRModule):\n",
    "    for global_var, func in mod.functions.items():\n",
    "        if isinstance(func, relax.Function):\n",
    "            print(global_var.name_hint)\n",
    "    print()\n",
    "\n",
    "\n",
    "print_relax_funcnames(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:22:22] /Users/cfruan/Documents/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[22:22:35] /Users/cfruan/Documents/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n"
     ]
    }
   ],
   "source": [
    "print(mod.script(show_meta=True), file=open(\"../dist/before_scheduling.py\", \"w+\"))\n",
    "\n",
    "import pickle\n",
    "import tvm.dlight as dl\n",
    "\n",
    "pickle.dump(mod, open(\"../dist/before_scheduling.pkl\", \"wb\"))\n",
    "\n",
    "target = tvm.target.Target(\"apple/m2-gpu\")\n",
    "\n",
    "with target:\n",
    "    seq = tvm.transform.Sequential(\n",
    "        [\n",
    "            relax.get_pipeline(),\n",
    "            dl.ApplyDefaultSchedule(\n",
    "                dl.gpu.Matmul(),\n",
    "                dl.gpu.GEMV(),\n",
    "                dl.gpu.Reduction(),\n",
    "                dl.gpu.GeneralReduction(),\n",
    "                dl.gpu.Fallback(),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    mod = seq(mod)\n",
    "\n",
    "ex = relax.build(mod, target=target)\n",
    "ex.export_library(\"../dist/deploy-untuned-metal.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = pickle.load(open(\"../dist/after_scheduling.pkl\", \"rb\"))\n",
    "print(mod.script(show_meta=True), file=open(\"../dist/after_scheduling.py\", \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_initialize_effect\n",
      "decode\n",
      "encode_transform_params  # <=== This is the weight parameter computation function for \"encode\"\n",
      "decode_transform_params  # <=== This is the weight parameter computation function for \"decode\"\n",
      "encode\n"
     ]
    }
   ],
   "source": [
    "for global_var, function in mod.functions.items():\n",
    "    if isinstance(function, relax.Function):\n",
    "        if global_var.name_hint.endswith(\"_transform_params\"):\n",
    "            print(\n",
    "                global_var.name_hint,\n",
    "                f' # <=== This is the weight parameter computation function for \"{global_var.name_hint[:-17]}\"',\n",
    "            )\n",
    "        else:\n",
    "            print(global_var.name_hint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def split_transform_deploy_mod(\n",
    "    mod: tvm.IRModule, model_names: List[str]\n",
    ") -> Tuple[tvm.IRModule, tvm.IRModule]:\n",
    "    mod_transform = tvm.IRModule()\n",
    "    mod_deploy = tvm.IRModule()\n",
    "\n",
    "    transform_func_names = [name + \"_transform_params\" for name in model_names]\n",
    "    for gv in mod.functions:\n",
    "        func = mod[gv]\n",
    "        if isinstance(func, tvm.tir.PrimFunc):\n",
    "            mod_transform[gv] = func\n",
    "            mod_deploy[gv] = func\n",
    "        elif gv.name_hint in transform_func_names:\n",
    "            mod_transform[gv] = func\n",
    "        else:\n",
    "            mod_deploy[gv] = func\n",
    "\n",
    "    mod_transform = relax.transform.DeadCodeElimination(transform_func_names)(\n",
    "        mod_transform\n",
    "    )\n",
    "    mod_deploy = relax.transform.DeadCodeElimination(model_names)(mod_deploy)\n",
    "\n",
    "    return mod_transform, mod_deploy\n",
    "\n",
    "model_names = [\"encode\", \"decode\"]\n",
    "\n",
    "mod_transform, mod_deploy = split_transform_deploy_mod(\n",
    "    mod, model_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IRModule for build stage:\n",
      "encode_transform_params\n",
      "decode_transform_params\n",
      "\n",
      "In IRModule for deployment stage:\n",
      "_initialize_effect\n",
      "decode\n",
      "encode\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"In IRModule for build stage:\")\n",
    "print_relax_funcnames(mod_transform)\n",
    "\n",
    "print(\"In IRModule for deployment stage:\")\n",
    "print_relax_funcnames(mod_deploy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mod_deploy.script(show_meta=True), file=open(\"../dist/deploy.py\", \"w+\"))\n",
    "\n",
    "pickle.dump(mod_deploy, open(\"../dist/deploy.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:38:19] /Users/cfruan/Documents/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[16:38:19] /Users/cfruan/Documents/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[16:38:20] /Users/cfruan/Documents/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n"
     ]
    }
   ],
   "source": [
    "from tvm.runtime import Device\n",
    "from tvm.relax.frontend.nn import Parameter\n",
    "from tvm.contrib import tvmjs\n",
    "import tvm.dlight as dl\n",
    "\n",
    "\n",
    "def load_params(\n",
    "    model_weight_path: str, device: Device, named_params: List[Tuple[str, Parameter]]\n",
    "):\n",
    "    params, _ = tvmjs.load_ndarray_cache(model_weight_path, device)\n",
    "    param_names = [name for name, _ in named_params]\n",
    "\n",
    "    plist = []\n",
    "    for param_name in param_names:\n",
    "        param_name = param_name.replace(\".layers.\", \".\")\n",
    "        param_name = param_name.replace(\".branches.0.\", \".\")\n",
    "        plist.append(params[param_name])\n",
    "    return plist\n",
    "\n",
    "\n",
    "device = tvm.metal()\n",
    "params = load_params(\"../weights\", device, named_params)\n",
    "\n",
    "\n",
    "def transform_params(\n",
    "    mod_transform: tvm.IRModule, model_params\n",
    ") -> Dict[str, List[tvm.nd.NDArray]]:\n",
    "    with tvm.target.Target(\"apple/m2-gpu\"):\n",
    "        mod_transform = dl.ApplyDefaultSchedule(\n",
    "            dl.gpu.Fallback(),\n",
    "        )(mod_transform)\n",
    "    ex = relax.build(mod_transform, target=\"apple/m2-gpu\")\n",
    "    vm = relax.vm.VirtualMachine(ex, tvm.metal())\n",
    "    new_params = dict()\n",
    "    for name, params in model_params.items():\n",
    "        new_params[name] = vm[name + \"_transform_params\"]([params])\n",
    "    return new_params\n",
    "\n",
    "new_params = transform_params(mod_transform, {\"encode\": params, \"decode\": params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start storing to cache ../dist/params\n",
      "[0241] saving decode_88 \n",
      "All finished, 6 total shards committed, record saved to ../dist/params/ndarray-cache.json\n",
      "Also saved a bf16 record to ../dist/params/ndarray-cache-b16.json\n"
     ]
    }
   ],
   "source": [
    "def save_params(params: Dict[str, List[tvm.nd.NDArray]], artifact_path: str) -> None:\n",
    "    from tvm.contrib import tvmjs\n",
    "\n",
    "    meta_data = {}\n",
    "    param_dict = {}\n",
    "    for model in [\"encode\", \"decode\"]:\n",
    "        meta_data[f\"{model}ParamSize\"] = len(params[model])\n",
    "        for i, nd in enumerate(params[model]):\n",
    "            param_dict[f\"{model}_{i}\"] = nd\n",
    "    tvmjs.dump_ndarray_cache(param_dict, f\"{artifact_path}/params\", meta_data=meta_data)\n",
    "\n",
    "save_params(new_params, artifact_path=\"../dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:50:45] /Users/cfruan/Documents/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n",
      "[16:50:47] /Users/cfruan/Documents/tvm-unity/src/target/llvm/llvm_instance.cc:226: Error: Using LLVM 19.1.1 with `-mcpu=apple-latest` is not valid in `-mtriple=arm64-apple-macos`, using default `-mcpu=generic`\n"
     ]
    }
   ],
   "source": [
    "mod_deploy = pickle.load(open(\"../dist/deploy.pkl\", \"rb\"))\n",
    "\n",
    "target = tvm.target.Target(\"apple/m2-gpu\")\n",
    "\n",
    "with target:\n",
    "    mod_deploy = tvm.tir.transform.DefaultGPUSchedule()(mod_deploy)\n",
    "\n",
    "ex = relax.build(mod_deploy, target=target)\n",
    "ex.export_library(\"../dist/deploy-metal.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.contrib import tvmjs\n",
    "\n",
    "\n",
    "def load_transformed_params(artifact_path: str, device) -> Dict[str, List[tvm.nd.NDArray]]:\n",
    "    from tvm.contrib import tvmjs\n",
    "\n",
    "    pdict = {}\n",
    "    params, meta = tvmjs.load_ndarray_cache(f\"{artifact_path}/params\", device)\n",
    "    for model in [\"encode\", \"decode\"]:\n",
    "        plist = []\n",
    "        size = meta[f\"{model}ParamSize\"]\n",
    "        for i in range(size):\n",
    "            plist.append(params[f\"{model}_{i}\"])\n",
    "        pdict[model] = plist\n",
    "    return pdict\n",
    "\n",
    "device = tvm.metal()\n",
    "const_params_dict = load_transformed_params(\"../dist\", device)\n",
    "ex = tvm.runtime.load_module(\"../dist/deploy-metal.so\")\n",
    "\n",
    "vm = relax.vm.VirtualMachine(ex, device, profile=True)\n",
    "forward_fn = vm[\"encode\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "  13.2346      12.9692      14.0863      12.9023       0.4425                  \n",
      "Profile saved to profile_stream.csv\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "effects = vm[\"_initialize_effect\"]()\n",
    "\n",
    "audio_data = np.random.randn(1, 1, 512).astype(\"float32\")\n",
    "audio_data = tvm.nd.array(audio_data, device=device)\n",
    "res, effects = forward_fn(audio_data, *effects, *const_params_dict[\"encode\"])\n",
    "\n",
    "time_eval = vm.time_evaluator(\"encode\", device, 10, 5)(audio_data, *effects, *const_params_dict[\"encode\"])\n",
    "print(time_eval)\n",
    "\n",
    "report = vm.profile(\"encode\", audio_data, *effects, *const_params_dict[\"encode\"])\n",
    "csv = report.csv()\n",
    "\n",
    "with open(\"profile_stream.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(csv)\n",
    "    print(\"Profile saved to profile_stream.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
