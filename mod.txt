# from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def fused_broadcast_to1_maximum1_tir_sqrt13_divide30(sum37: T.Buffer((T.int64(1024), T.int64(1)), "float32"), quantizer_quantizers_0_codebook_weight1: T.Buffer((T.int64(1024), T.int64(8)), "float32"), T_divide_intermediate: T.Buffer((T.int64(1024), T.int64(8)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(8), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(T.int64(1024), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.reads(quantizer_quantizers_0_codebook_weight1[v0, v1], sum37[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = quantizer_quantizers_0_codebook_weight1[v0, v1] / T.sqrt(T.max(sum37[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_broadcast_to2_maximum2_tir_sqrt12_divide29(p_sum64: T.handle, p_reshape238: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        sum64 = T.match_buffer(p_sum64, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        reshape238 = T.match_buffer(p_reshape238, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        T_divide_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(reshape238[v0, v1], sum64[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = reshape238[v0, v1] / T.sqrt(T.max(sum64[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_broadcast_to2_maximum2_tir_sqrt12_divide32(p_sum28: T.handle, p_reshape86: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        sum28 = T.match_buffer(p_sum28, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        reshape86 = T.match_buffer(p_reshape86, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        T_divide_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(reshape86[v0, v1], sum28[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = reshape86[v0, v1] / T.sqrt(T.max(sum28[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_broadcast_to2_maximum2_tir_sqrt14_divide29(p_sum68: T.handle, p_reshape242: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        sum68 = T.match_buffer(p_sum68, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        reshape242 = T.match_buffer(p_reshape242, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        T_divide_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(reshape242[v0, v1], sum68[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = reshape242[v0, v1] / T.sqrt(T.max(sum68[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_broadcast_to2_maximum_tir_sqrt12_divide29(p_sum12: T.handle, p_reshape70: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        sum12 = T.match_buffer(p_sum12, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        reshape70 = T.match_buffer(p_reshape70, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        T_divide_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(reshape70[v0, v1], sum12[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = reshape70[v0, v1] / T.sqrt(T.max(sum12[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_broadcast_to2_maximum_tir_sqrt12_divide32(p_sum44: T.handle, p_reshape218: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        sum44 = T.match_buffer(p_sum44, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        reshape218 = T.match_buffer(p_reshape218, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        T_divide_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(reshape218[v0, v1], sum44[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = reshape218[v0, v1] / T.sqrt(T.max(sum44[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_broadcast_to2_maximum_tir_sqrt14_divide32(p_sum60: T.handle, p_reshape234: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        sum60 = T.match_buffer(p_sum60, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        reshape234 = T.match_buffer(p_reshape234, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        T_divide_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(reshape234[v0, v1], sum60[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = reshape234[v0, v1] / T.sqrt(T.max(sum60[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_broadcast_to_maximum2_tir_sqrt12_divide29(p_sum20: T.handle, p_reshape78: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        sum20 = T.match_buffer(p_sum20, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        reshape78 = T.match_buffer(p_reshape78, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        T_divide_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(reshape78[v0, v1], sum20[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = reshape78[v0, v1] / T.sqrt(T.max(sum20[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_broadcast_to_maximum2_tir_sqrt12_divide32(p_sum40: T.handle, p_reshape214: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        sum40 = T.match_buffer(p_sum40, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        reshape214 = T.match_buffer(p_reshape214, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        T_divide_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(reshape214[v0, v1], sum40[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = reshape214[v0, v1] / T.sqrt(T.max(sum40[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_broadcast_to_maximum_tir_sqrt12_divide29(p_sum36: T.handle, p_reshape210: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        sum36 = T.match_buffer(p_sum36, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        reshape210 = T.match_buffer(p_reshape210, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        T_divide_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(reshape210[v0, v1], sum36[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = reshape210[v0, v1] / T.sqrt(T.max(sum36[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_broadcast_to_maximum_tir_sqrt12_divide32(p_sum52: T.handle, p_reshape226: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        sum52 = T.match_buffer(p_sum52, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        reshape226 = T.match_buffer(p_reshape226, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        T_divide_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(reshape226[v0, v1], sum52[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = reshape226[v0, v1] / T.sqrt(T.max(sum52[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_broadcast_to_maximum_tir_sqrt14_divide32(p_sum48: T.handle, p_reshape222: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        sum48 = T.match_buffer(p_sum48, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        reshape222 = T.match_buffer(p_reshape222, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        T_divide_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(reshape222[v0, v1], sum48[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = reshape222[v0, v1] / T.sqrt(T.max(sum48[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_conv1d10_add5_add6(p_reshape279: T.handle, wnconv1d136: T.Buffer((T.int64(192), T.int64(192), T.int64(1)), "float32"), lv967: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_conv1d_transpose6: T.handle, p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape279 = T.match_buffer(p_reshape279, (batch_size, T.int64(192), seq_len * T.int64(256)))
        conv1d_transpose6 = T.match_buffer(p_conv1d_transpose6, (batch_size, T.int64(192), seq_len * T.int64(256)))
        T_add_intermediate_1 = T.match_buffer(p_output0, (batch_size, T.int64(192), seq_len * T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(192), seq_len * T.int64(256)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(3), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(49152) * batch_size) // (seq_len * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(256), ax0_ax1_ax2_fused % (seq_len * T.int64(256)) + ax2)
                            v3 = T.axis.reduce(T.int64(192), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(49152)) // (seq_len * T.int64(49152)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(49152)) // (seq_len * T.int64(256)) < T.int64(192))
                            T.reads(reshape279[v0, v3, v2], wnconv1d136[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + reshape279[v0, v3, v2] * wnconv1d136[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add_1"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192) * batch_size) // (seq_len * T.int64(256) * T.int64(192)))
                        v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)))
                        v2 = T.axis.spatial(seq_len * T.int64(256), ax0_ax1_ax2_fused % (seq_len * T.int64(256)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv967[T.int64(0), v1, T.int64(0)], conv1d_transpose6[v0, v1, v2])
                        T.writes(T_add_intermediate_1[v0, v1, v2])
                        T_add_intermediate_1[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv967[T.int64(0), v1, T.int64(0)] + conv1d_transpose6[v0, v1, v2]

    @T.prim_func(private=True)
    def fused_conv1d11_add5(p_reshape281: T.handle, wnconv1d137: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32"), lv974: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape281 = T.match_buffer(p_reshape281, (batch_size, T.int64(192), seq_len * T.int64(256)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(192), seq_len * T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(192), seq_len * T.int64(256)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(21), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(49152) * batch_size) // (seq_len * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(256), ax0_ax1_ax2_fused % (seq_len * T.int64(256)) + ax2)
                            v3 = T.axis.reduce(T.int64(192), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(49152)) // (seq_len * T.int64(49152)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(49152)) // (seq_len * T.int64(256)) < T.int64(192))
                            T.reads(reshape281[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], wnconv1d137[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(9) <= v4 * T.int64(3) + v2 and v4 * T.int64(3) + v2 < seq_len * T.int64(256) + T.int64(9), reshape281[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], T.float32(0.0)) * wnconv1d137[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192) * batch_size) // (seq_len * T.int64(256) * T.int64(192)))
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)))
                            v2 = T.axis.spatial(seq_len * T.int64(256), ax0_ax1_ax2_fused % (seq_len * T.int64(256)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv974[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv974[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d12_add5(p_reshape285: T.handle, wnconv1d139: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32"), lv988: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape285 = T.match_buffer(p_reshape285, (batch_size, T.int64(192), seq_len * T.int64(256)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(192), seq_len * T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(192), seq_len * T.int64(256)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(21), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(49152) * batch_size) // (seq_len * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(256), ax0_ax1_ax2_fused % (seq_len * T.int64(256)) + ax2)
                            v3 = T.axis.reduce(T.int64(192), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(49152)) // (seq_len * T.int64(49152)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(49152)) // (seq_len * T.int64(256)) < T.int64(192))
                            T.reads(reshape285[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], wnconv1d139[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(27) <= v4 * T.int64(9) + v2 and v4 * T.int64(9) + v2 < seq_len * T.int64(256) + T.int64(27), reshape285[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], T.float32(0.0)) * wnconv1d139[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192) * batch_size) // (seq_len * T.int64(256) * T.int64(192)))
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)))
                            v2 = T.axis.spatial(seq_len * T.int64(256), ax0_ax1_ax2_fused % (seq_len * T.int64(256)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv988[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv988[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d13_add7(p_reshape291: T.handle, wnconv1d141: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32"), lv1009: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape291 = T.match_buffer(p_reshape291, (batch_size, T.int64(96), seq_len * T.int64(512)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(96), seq_len * T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(96), seq_len * T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(11), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(49152) * batch_size) // (seq_len * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(512), ax0_ax1_ax2_fused % (seq_len * T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(96), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(49152)) // (seq_len * T.int64(49152)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(49152)) // (seq_len * T.int64(512)) < T.int64(96) and ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1 < T.int64(672))
                            T.reads(reshape291[v0, v3, v2 + v4 - T.int64(3)], wnconv1d141[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len * T.int64(512) + T.int64(3), reshape291[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d141[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96) * batch_size) // (seq_len * T.int64(512) * T.int64(96)))
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)))
                            v2 = T.axis.spatial(seq_len * T.int64(512), ax0_ax1_ax2_fused % (seq_len * T.int64(512)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv1009[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv1009[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d14_add7_add8(p_reshape293: T.handle, wnconv1d142: T.Buffer((T.int64(96), T.int64(96), T.int64(1)), "float32"), lv1016: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_conv1d_transpose7: T.handle, p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape293 = T.match_buffer(p_reshape293, (batch_size, T.int64(96), seq_len * T.int64(512)))
        conv1d_transpose7 = T.match_buffer(p_conv1d_transpose7, (batch_size, T.int64(96), seq_len * T.int64(512)))
        T_add_intermediate_1 = T.match_buffer(p_output0, (batch_size, T.int64(96), seq_len * T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(96), seq_len * T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(2), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(49152) * batch_size) // (seq_len * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(512), ax0_ax1_ax2_fused % (seq_len * T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(96), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(49152)) // (seq_len * T.int64(49152)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(49152)) // (seq_len * T.int64(512)) < T.int64(96) and ax3_fused_0 * T.int64(64) + ax3_fused_1 < T.int64(96))
                            T.reads(reshape293[v0, v3, v2], wnconv1d142[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + reshape293[v0, v3, v2] * wnconv1d142[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add_1"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96) * batch_size) // (seq_len * T.int64(512) * T.int64(96)))
                        v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)))
                        v2 = T.axis.spatial(seq_len * T.int64(512), ax0_ax1_ax2_fused % (seq_len * T.int64(512)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv1016[T.int64(0), v1, T.int64(0)], conv1d_transpose7[v0, v1, v2])
                        T.writes(T_add_intermediate_1[v0, v1, v2])
                        T_add_intermediate_1[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv1016[T.int64(0), v1, T.int64(0)] + conv1d_transpose7[v0, v1, v2]

    @T.prim_func(private=True)
    def fused_conv1d15_add7(p_reshape295: T.handle, wnconv1d143: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32"), lv1023: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape295 = T.match_buffer(p_reshape295, (batch_size, T.int64(96), seq_len * T.int64(512)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(96), seq_len * T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(96), seq_len * T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(11), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(49152) * batch_size) // (seq_len * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(512), ax0_ax1_ax2_fused % (seq_len * T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(96), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(49152)) // (seq_len * T.int64(49152)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(49152)) // (seq_len * T.int64(512)) < T.int64(96) and ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1 < T.int64(672))
                            T.reads(reshape295[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], wnconv1d143[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(9) <= v4 * T.int64(3) + v2 and v4 * T.int64(3) + v2 < seq_len * T.int64(512) + T.int64(9), reshape295[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], T.float32(0.0)) * wnconv1d143[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96) * batch_size) // (seq_len * T.int64(512) * T.int64(96)))
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)))
                            v2 = T.axis.spatial(seq_len * T.int64(512), ax0_ax1_ax2_fused % (seq_len * T.int64(512)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv1023[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv1023[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d16_add7(p_reshape299: T.handle, wnconv1d145: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32"), lv1037: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape299 = T.match_buffer(p_reshape299, (batch_size, T.int64(96), seq_len * T.int64(512)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(96), seq_len * T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(96), seq_len * T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(11), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(49152) * batch_size) // (seq_len * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(512), ax0_ax1_ax2_fused % (seq_len * T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(96), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(49152)) // (seq_len * T.int64(49152)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(49152)) // (seq_len * T.int64(512)) < T.int64(96) and ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1 < T.int64(672))
                            T.reads(reshape299[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], wnconv1d145[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(27) <= v4 * T.int64(9) + v2 and v4 * T.int64(9) + v2 < seq_len * T.int64(512) + T.int64(27), reshape299[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], T.float32(0.0)) * wnconv1d145[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96) * batch_size) // (seq_len * T.int64(512) * T.int64(96)))
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)))
                            v2 = T.axis.spatial(seq_len * T.int64(512), ax0_ax1_ax2_fused % (seq_len * T.int64(512)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv1037[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv1037[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d17_reshape10_add9_tir_tanh(p_reshape303: T.handle, wnconv1d147: T.Buffer((T.int64(1), T.int64(96), T.int64(7)), "float32"), decoder_model_6_bias2: T.Buffer((T.int64(1),), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape303 = T.match_buffer(p_reshape303, (batch_size, T.int64(96), seq_len * T.int64(512)))
        compute_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1), seq_len * T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(1), seq_len * T.int64(512)), scope="shared")
        for ax0_ax1_fused in T.thread_binding(batch_size * seq_len * T.int64(512), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax2_ax3_fused_0 in T.serial(T.int64(11), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_fused % (seq_len * T.int64(512) * batch_size) // (seq_len * T.int64(512)) + ax0)
                            v1 = T.axis.spatial(seq_len * T.int64(512), ax0_ax1_fused % (seq_len * T.int64(512)) + ax1)
                            v2 = T.axis.reduce(T.int64(96), (ax2_ax3_fused_0 * T.int64(64) + ax2_ax3_fused_1) // T.int64(7))
                            v3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(64) + ax2_ax3_fused_1) % T.int64(7))
                            T.where(ax0_ax1_fused % (seq_len * batch_size * T.int64(512)) // (seq_len * T.int64(512)) < batch_size and ax2_ax3_fused_0 * T.int64(64) + ax2_ax3_fused_1 < T.int64(672))
                            T.reads(reshape303[v0, v2, v1 + v3 - T.int64(3)], wnconv1d147[T.int64(0), v2, v3])
                            T.writes(conv1d_ncw_intermediate_shared[v0, T.int64(0), v1])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, T.int64(0), v1] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, T.int64(0), v1] = conv1d_ncw_intermediate_shared[v0, T.int64(0), v1] + T.if_then_else(T.int64(3) <= v1 + v3 and v1 + v3 < seq_len * T.int64(512) + T.int64(3), reshape303[v0, v2, v1 + v3 - T.int64(3)], T.float32(0.0)) * wnconv1d147[T.int64(0), v2, v3]
            for ax2 in range(T.int64(1)):
                for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("compute"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_fused % (seq_len * T.int64(512) * batch_size) // (seq_len * T.int64(512)))
                            v1 = T.axis.spatial(seq_len * T.int64(512), ax0_ax1_fused % (seq_len * T.int64(512)))
                            v2 = T.axis.spatial(T.int64(1), ax2)
                            v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                            T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, T.int64(0), v1], decoder_model_6_bias2[T.int64(0)])
                            T.writes(compute_intermediate[v0, T.int64(0), v1])
                            compute_intermediate[v0, T.int64(0), v1] = T.tanh(conv1d_ncw_intermediate_shared[v0, T.int64(0), v1] + decoder_model_6_bias2[T.int64(0)])

    @T.prim_func(private=True)
    def fused_conv1d18_add10(p_audio_data: T.handle, wnconv1d74: T.Buffer((T.int64(64), T.int64(1), T.int64(7)), "float32"), lv531: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, seq_len = T.int64(), T.int64()
        audio_data = T.match_buffer(p_audio_data, (batch_size, T.int64(1), seq_len))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(64), seq_len))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(64), seq_len), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(64), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * batch_size) // (seq_len * T.int64(64)) + ax0)
                            v1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) // seq_len + ax1)
                            v2 = T.axis.spatial(seq_len, ax0_ax1_ax2_fused % seq_len + ax2)
                            v3 = T.axis.reduce(T.int64(7), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(64)) // (seq_len * T.int64(64)) < batch_size and ax3_fused_0 * T.int64(64) + ax3_fused_1 < T.int64(7))
                            T.reads(audio_data[v0, T.int64(0), v2 + v3 - T.int64(3)], wnconv1d74[v1, T.int64(0), v3])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v3 and v2 + v3 < seq_len + T.int64(3), audio_data[v0, T.int64(0), v2 + v3 - T.int64(3)], T.float32(0.0)) * wnconv1d74[v1, T.int64(0), v3]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * batch_size) // (seq_len * T.int64(64)))
                        v1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) // seq_len)
                        v2 = T.axis.spatial(seq_len, ax0_ax1_ax2_fused % seq_len)
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv531[T.int64(0), v1, T.int64(0)])
                        T.writes(T_add_intermediate[v0, v1, v2])
                        T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv531[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d19_add10(p_reshape153: T.handle, wnconv1d75: T.Buffer((T.int64(64), T.int64(64), T.int64(7)), "float32"), lv538: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, seq_len = T.int64(), T.int64()
        reshape153 = T.match_buffer(p_reshape153, (batch_size, T.int64(64), seq_len))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(64), seq_len))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(64), seq_len), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(64), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(7), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * batch_size) // (seq_len * T.int64(64)) + ax0)
                            v1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) // seq_len + ax1)
                            v2 = T.axis.spatial(seq_len, ax0_ax1_ax2_fused % seq_len + ax2)
                            v3 = T.axis.reduce(T.int64(64), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(64)) // (seq_len * T.int64(64)) < batch_size)
                            T.reads(reshape153[v0, v3, v2 + v4 - T.int64(3)], wnconv1d75[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len + T.int64(3), reshape153[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d75[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * batch_size) // (seq_len * T.int64(64)))
                            v1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) // seq_len)
                            v2 = T.axis.spatial(seq_len, ax0_ax1_ax2_fused % seq_len)
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv538[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv538[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d1_add1(p_reshape249: T.handle, wnconv1d123: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32"), lv862: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape249 = T.match_buffer(p_reshape249, (batch_size, T.int64(768), seq_len * T.int64(8)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(768), seq_len * T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(768), seq_len * T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(6144), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(84), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(6144) * batch_size) // (seq_len * T.int64(6144)) + ax0)
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(8), ax0_ax1_ax2_fused % (seq_len * T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(768), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(6144)) // (seq_len * T.int64(6144)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(6144)) // (seq_len * T.int64(8)) < T.int64(768))
                            T.reads(reshape249[v0, v3, v2 + v4 - T.int64(3)], wnconv1d123[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len * T.int64(8) + T.int64(3), reshape249[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d123[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768) * batch_size) // (seq_len * T.int64(8) * T.int64(768)))
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)))
                            v2 = T.axis.spatial(seq_len * T.int64(8), ax0_ax1_ax2_fused % (seq_len * T.int64(8)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv862[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv862[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d20_add10_add11(p_reshape155: T.handle, wnconv1d76: T.Buffer((T.int64(64), T.int64(64), T.int64(1)), "float32"), lv545: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32"), p_conv1d74: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, seq_len = T.int64(), T.int64()
        reshape155 = T.match_buffer(p_reshape155, (batch_size, T.int64(64), seq_len))
        conv1d74 = T.match_buffer(p_conv1d74, (batch_size, T.int64(64), seq_len))
        T_add_intermediate_1 = T.match_buffer(p_output0, (batch_size, T.int64(64), seq_len))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(64), seq_len), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(64), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * batch_size) // (seq_len * T.int64(64)) + ax0)
                            v1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) // seq_len + ax1)
                            v2 = T.axis.spatial(seq_len, ax0_ax1_ax2_fused % seq_len + ax2)
                            v3 = T.axis.reduce(T.int64(64), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(64)) // (seq_len * T.int64(64)) < batch_size)
                            T.reads(reshape155[v0, v3, v2], wnconv1d76[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + reshape155[v0, v3, v2] * wnconv1d76[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add_1"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * batch_size) // (seq_len * T.int64(64)))
                        v1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) // seq_len)
                        v2 = T.axis.spatial(seq_len, ax0_ax1_ax2_fused % seq_len)
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv545[T.int64(0), v1, T.int64(0)], conv1d74[v0, v1, v2])
                        T.writes(T_add_intermediate_1[v0, v1, v2])
                        T_add_intermediate_1[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv545[T.int64(0), v1, T.int64(0)] + conv1d74[v0, v1, v2]

    @T.prim_func(private=True)
    def fused_conv1d21_add10(p_reshape157: T.handle, wnconv1d77: T.Buffer((T.int64(64), T.int64(64), T.int64(7)), "float32"), lv552: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, seq_len = T.int64(), T.int64()
        reshape157 = T.match_buffer(p_reshape157, (batch_size, T.int64(64), seq_len))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(64), seq_len))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(64), seq_len), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(64), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(7), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * batch_size) // (seq_len * T.int64(64)) + ax0)
                            v1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) // seq_len + ax1)
                            v2 = T.axis.spatial(seq_len, ax0_ax1_ax2_fused % seq_len + ax2)
                            v3 = T.axis.reduce(T.int64(64), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(64)) // (seq_len * T.int64(64)) < batch_size)
                            T.reads(reshape157[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], wnconv1d77[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(9) <= v4 * T.int64(3) + v2 and v4 * T.int64(3) + v2 < seq_len + T.int64(9), reshape157[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], T.float32(0.0)) * wnconv1d77[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * batch_size) // (seq_len * T.int64(64)))
                            v1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) // seq_len)
                            v2 = T.axis.spatial(seq_len, ax0_ax1_ax2_fused % seq_len)
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv552[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv552[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d22_add10(p_reshape161: T.handle, wnconv1d79: T.Buffer((T.int64(64), T.int64(64), T.int64(7)), "float32"), lv566: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, seq_len = T.int64(), T.int64()
        reshape161 = T.match_buffer(p_reshape161, (batch_size, T.int64(64), seq_len))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(64), seq_len))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(64), seq_len), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(64), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(7), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * batch_size) // (seq_len * T.int64(64)) + ax0)
                            v1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) // seq_len + ax1)
                            v2 = T.axis.spatial(seq_len, ax0_ax1_ax2_fused % seq_len + ax2)
                            v3 = T.axis.reduce(T.int64(64), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(64)) // (seq_len * T.int64(64)) < batch_size)
                            T.reads(reshape161[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], wnconv1d79[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(27) <= v4 * T.int64(9) + v2 and v4 * T.int64(9) + v2 < seq_len + T.int64(27), reshape161[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], T.float32(0.0)) * wnconv1d79[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * batch_size) // (seq_len * T.int64(64)))
                            v1 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) // seq_len)
                            v2 = T.axis.spatial(seq_len, ax0_ax1_ax2_fused % seq_len)
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv566[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv566[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d23_add12(p_reshape165: T.handle, wnconv1d81: T.Buffer((T.int64(128), T.int64(64), T.int64(4)), "float32"), lv580: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, seq_len = T.int64(), T.int64()
        reshape165 = T.match_buffer(p_reshape165, (batch_size, T.int64(64), seq_len))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(128), seq_len // T.int64(2)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(128), seq_len // T.int64(2)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(2)) * T.int64(128), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(4), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128) * batch_size) // (seq_len // T.int64(2) * T.int64(128)) + ax0)
                            v1 = T.axis.spatial(T.int64(128), ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(2), ax0_ax1_ax2_fused % (seq_len // T.int64(2)) + ax2)
                            v3 = T.axis.reduce(T.int64(64), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(4))
                            v4 = T.axis.reduce(T.int64(4), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(4))
                            T.where(ax0_ax1_ax2_fused % (seq_len // T.int64(2) * batch_size * T.int64(128)) // (seq_len // T.int64(2) * T.int64(128)) < batch_size)
                            T.reads(reshape165[v0, v3, v2 * T.int64(2) + v4 - T.int64(1)], wnconv1d81[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(1) <= v2 * T.int64(2) + v4 and v2 * T.int64(2) + v4 <= seq_len, reshape165[v0, v3, v2 * T.int64(2) + v4 - T.int64(1)], T.float32(0.0)) * wnconv1d81[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128) * batch_size) // (seq_len // T.int64(2) * T.int64(128)))
                            v1 = T.axis.spatial(T.int64(128), ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)))
                            v2 = T.axis.spatial(seq_len // T.int64(2), ax0_ax1_ax2_fused % (seq_len // T.int64(2)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv580[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv580[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d24_add12(p_reshape167: T.handle, wnconv1d82: T.Buffer((T.int64(128), T.int64(128), T.int64(7)), "float32"), lv587: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape167 = T.match_buffer(p_reshape167, (batch_size, T.int64(128), seq_len // T.int64(2)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(128), seq_len // T.int64(2)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(128), seq_len // T.int64(2)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(2)) * T.int64(128), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(14), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128) * batch_size) // (seq_len // T.int64(2) * T.int64(128)) + ax0)
                            v1 = T.axis.spatial(T.int64(128), ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(2), ax0_ax1_ax2_fused % (seq_len // T.int64(2)) + ax2)
                            v3 = T.axis.reduce(T.int64(128), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(2) * batch_size * T.int64(128)) // (seq_len // T.int64(2) * T.int64(128)) and ax0_ax1_ax2_fused % (seq_len // T.int64(2) * batch_size * T.int64(128)) // (seq_len // T.int64(2) * T.int64(128)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)) and ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)) < T.int64(128) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(2)) and ax0_ax1_ax2_fused % (seq_len // T.int64(2)) < seq_len // T.int64(2))
                            T.reads(reshape167[v0, v3, v2 + v4 - T.int64(3)], wnconv1d82[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len // T.int64(2) + T.int64(3), reshape167[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d82[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128) * batch_size) // (seq_len // T.int64(2) * T.int64(128)))
                            v1 = T.axis.spatial(T.int64(128), ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)))
                            v2 = T.axis.spatial(seq_len // T.int64(2), ax0_ax1_ax2_fused % (seq_len // T.int64(2)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv587[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv587[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d25_add12_add13(p_reshape169: T.handle, wnconv1d83: T.Buffer((T.int64(128), T.int64(128), T.int64(1)), "float32"), lv594: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32"), p_conv1d81: T.handle, p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape169 = T.match_buffer(p_reshape169, (batch_size, T.int64(128), seq_len // T.int64(2)))
        conv1d81 = T.match_buffer(p_conv1d81, (batch_size, T.int64(128), seq_len // T.int64(2)))
        T_add_intermediate_1 = T.match_buffer(p_output0, (batch_size, T.int64(128), seq_len // T.int64(2)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(128), seq_len // T.int64(2)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(2)) * T.int64(128), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(2), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128) * batch_size) // (seq_len // T.int64(2) * T.int64(128)) + ax0)
                            v1 = T.axis.spatial(T.int64(128), ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(2), ax0_ax1_ax2_fused % (seq_len // T.int64(2)) + ax2)
                            v3 = T.axis.reduce(T.int64(128), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(2) * batch_size * T.int64(128)) // (seq_len // T.int64(2) * T.int64(128)) and ax0_ax1_ax2_fused % (seq_len // T.int64(2) * batch_size * T.int64(128)) // (seq_len // T.int64(2) * T.int64(128)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)) and ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)) < T.int64(128) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(2)) and ax0_ax1_ax2_fused % (seq_len // T.int64(2)) < seq_len // T.int64(2))
                            T.reads(reshape169[v0, v3, v2], wnconv1d83[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + reshape169[v0, v3, v2] * wnconv1d83[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add_1"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128) * batch_size) // (seq_len // T.int64(2) * T.int64(128)))
                        v1 = T.axis.spatial(T.int64(128), ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)))
                        v2 = T.axis.spatial(seq_len // T.int64(2), ax0_ax1_ax2_fused % (seq_len // T.int64(2)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv594[T.int64(0), v1, T.int64(0)], conv1d81[v0, v1, v2])
                        T.writes(T_add_intermediate_1[v0, v1, v2])
                        T_add_intermediate_1[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv594[T.int64(0), v1, T.int64(0)] + conv1d81[v0, v1, v2]

    @T.prim_func(private=True)
    def fused_conv1d26_add12(p_reshape171: T.handle, wnconv1d84: T.Buffer((T.int64(128), T.int64(128), T.int64(7)), "float32"), lv601: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape171 = T.match_buffer(p_reshape171, (batch_size, T.int64(128), seq_len // T.int64(2)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(128), seq_len // T.int64(2)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(128), seq_len // T.int64(2)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(2)) * T.int64(128), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(14), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128) * batch_size) // (seq_len // T.int64(2) * T.int64(128)) + ax0)
                            v1 = T.axis.spatial(T.int64(128), ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(2), ax0_ax1_ax2_fused % (seq_len // T.int64(2)) + ax2)
                            v3 = T.axis.reduce(T.int64(128), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(2) * batch_size * T.int64(128)) // (seq_len // T.int64(2) * T.int64(128)) and ax0_ax1_ax2_fused % (seq_len // T.int64(2) * batch_size * T.int64(128)) // (seq_len // T.int64(2) * T.int64(128)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)) and ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)) < T.int64(128) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(2)) and ax0_ax1_ax2_fused % (seq_len // T.int64(2)) < seq_len // T.int64(2))
                            T.reads(reshape171[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], wnconv1d84[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(9) <= v4 * T.int64(3) + v2 and v4 * T.int64(3) + v2 < seq_len // T.int64(2) + T.int64(9), reshape171[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], T.float32(0.0)) * wnconv1d84[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128) * batch_size) // (seq_len // T.int64(2) * T.int64(128)))
                            v1 = T.axis.spatial(T.int64(128), ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)))
                            v2 = T.axis.spatial(seq_len // T.int64(2), ax0_ax1_ax2_fused % (seq_len // T.int64(2)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv601[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv601[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d27_add12(p_reshape175: T.handle, wnconv1d86: T.Buffer((T.int64(128), T.int64(128), T.int64(7)), "float32"), lv615: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape175 = T.match_buffer(p_reshape175, (batch_size, T.int64(128), seq_len // T.int64(2)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(128), seq_len // T.int64(2)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(128), seq_len // T.int64(2)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(2)) * T.int64(128), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(14), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128) * batch_size) // (seq_len // T.int64(2) * T.int64(128)) + ax0)
                            v1 = T.axis.spatial(T.int64(128), ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(2), ax0_ax1_ax2_fused % (seq_len // T.int64(2)) + ax2)
                            v3 = T.axis.reduce(T.int64(128), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(2) * batch_size * T.int64(128)) // (seq_len // T.int64(2) * T.int64(128)) and ax0_ax1_ax2_fused % (seq_len // T.int64(2) * batch_size * T.int64(128)) // (seq_len // T.int64(2) * T.int64(128)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)) and ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)) < T.int64(128) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(2)) and ax0_ax1_ax2_fused % (seq_len // T.int64(2)) < seq_len // T.int64(2))
                            T.reads(reshape175[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], wnconv1d86[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(27) <= v4 * T.int64(9) + v2 and v4 * T.int64(9) + v2 < seq_len // T.int64(2) + T.int64(27), reshape175[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], T.float32(0.0)) * wnconv1d86[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128) * batch_size) // (seq_len // T.int64(2) * T.int64(128)))
                            v1 = T.axis.spatial(T.int64(128), ax0_ax1_ax2_fused % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)))
                            v2 = T.axis.spatial(seq_len // T.int64(2), ax0_ax1_ax2_fused % (seq_len // T.int64(2)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv615[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv615[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d28_add14(p_reshape179: T.handle, wnconv1d88: T.Buffer((T.int64(256), T.int64(128), T.int64(8)), "float32"), lv629: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape179 = T.match_buffer(p_reshape179, (batch_size, T.int64(128), seq_len // T.int64(2)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(256), seq_len // T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(256), seq_len // T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(8)) * T.int64(256), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(16), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256) * batch_size) // (seq_len // T.int64(8) * T.int64(256)) + ax0)
                            v1 = T.axis.spatial(T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(128), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(8))
                            v4 = T.axis.reduce(T.int64(8), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(8))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8) * batch_size * T.int64(256)) // (seq_len // T.int64(8) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8) * batch_size * T.int64(256)) // (seq_len // T.int64(8) * T.int64(256)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) < T.int64(256) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8)) < seq_len // T.int64(8))
                            T.reads(reshape179[v0, v3, v2 * T.int64(4) + v4 - T.int64(2)], wnconv1d88[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(2) <= v2 * T.int64(4) + v4 and v2 * T.int64(4) + v4 < seq_len // T.int64(2) + T.int64(2), reshape179[v0, v3, v2 * T.int64(4) + v4 - T.int64(2)], T.float32(0.0)) * wnconv1d88[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256) * batch_size) // (seq_len // T.int64(8) * T.int64(256)))
                            v1 = T.axis.spatial(T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)))
                            v2 = T.axis.spatial(seq_len // T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(8)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv629[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv629[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d29_add14(p_reshape181: T.handle, wnconv1d89: T.Buffer((T.int64(256), T.int64(256), T.int64(7)), "float32"), lv636: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape181 = T.match_buffer(p_reshape181, (batch_size, T.int64(256), seq_len // T.int64(8)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(256), seq_len // T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(256), seq_len // T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(8)) * T.int64(256), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(28), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256) * batch_size) // (seq_len // T.int64(8) * T.int64(256)) + ax0)
                            v1 = T.axis.spatial(T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(256), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8) * batch_size * T.int64(256)) // (seq_len // T.int64(8) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8) * batch_size * T.int64(256)) // (seq_len // T.int64(8) * T.int64(256)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) < T.int64(256) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8)) < seq_len // T.int64(8))
                            T.reads(reshape181[v0, v3, v2 + v4 - T.int64(3)], wnconv1d89[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len // T.int64(8) + T.int64(3), reshape181[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d89[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256) * batch_size) // (seq_len // T.int64(8) * T.int64(256)))
                            v1 = T.axis.spatial(T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)))
                            v2 = T.axis.spatial(seq_len // T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(8)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv636[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv636[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d2_add1_add2(p_reshape251: T.handle, wnconv1d124: T.Buffer((T.int64(768), T.int64(768), T.int64(1)), "float32"), lv869: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_conv1d_transpose4: T.handle, p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape251 = T.match_buffer(p_reshape251, (batch_size, T.int64(768), seq_len * T.int64(8)))
        conv1d_transpose4 = T.match_buffer(p_conv1d_transpose4, (batch_size, T.int64(768), seq_len * T.int64(8)))
        T_add_intermediate_1 = T.match_buffer(p_output0, (batch_size, T.int64(768), seq_len * T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(768), seq_len * T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(6144), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(12), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(6144) * batch_size) // (seq_len * T.int64(6144)) + ax0)
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(8), ax0_ax1_ax2_fused % (seq_len * T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(768), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(6144)) // (seq_len * T.int64(6144)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(6144)) // (seq_len * T.int64(8)) < T.int64(768))
                            T.reads(reshape251[v0, v3, v2], wnconv1d124[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + reshape251[v0, v3, v2] * wnconv1d124[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add_1"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768) * batch_size) // (seq_len * T.int64(8) * T.int64(768)))
                        v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)))
                        v2 = T.axis.spatial(seq_len * T.int64(8), ax0_ax1_ax2_fused % (seq_len * T.int64(8)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv869[T.int64(0), v1, T.int64(0)], conv1d_transpose4[v0, v1, v2])
                        T.writes(T_add_intermediate_1[v0, v1, v2])
                        T_add_intermediate_1[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv869[T.int64(0), v1, T.int64(0)] + conv1d_transpose4[v0, v1, v2]

    @T.prim_func(private=True)
    def fused_conv1d30_add14_add15(p_reshape183: T.handle, wnconv1d90: T.Buffer((T.int64(256), T.int64(256), T.int64(1)), "float32"), lv643: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32"), p_conv1d88: T.handle, p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape183 = T.match_buffer(p_reshape183, (batch_size, T.int64(256), seq_len // T.int64(8)))
        conv1d88 = T.match_buffer(p_conv1d88, (batch_size, T.int64(256), seq_len // T.int64(8)))
        T_add_intermediate_1 = T.match_buffer(p_output0, (batch_size, T.int64(256), seq_len // T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(256), seq_len // T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(8)) * T.int64(256), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(4), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256) * batch_size) // (seq_len // T.int64(8) * T.int64(256)) + ax0)
                            v1 = T.axis.spatial(T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(256), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8) * batch_size * T.int64(256)) // (seq_len // T.int64(8) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8) * batch_size * T.int64(256)) // (seq_len // T.int64(8) * T.int64(256)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) < T.int64(256) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8)) < seq_len // T.int64(8))
                            T.reads(reshape183[v0, v3, v2], wnconv1d90[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + reshape183[v0, v3, v2] * wnconv1d90[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add_1"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256) * batch_size) // (seq_len // T.int64(8) * T.int64(256)))
                        v1 = T.axis.spatial(T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)))
                        v2 = T.axis.spatial(seq_len // T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(8)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv643[T.int64(0), v1, T.int64(0)], conv1d88[v0, v1, v2])
                        T.writes(T_add_intermediate_1[v0, v1, v2])
                        T_add_intermediate_1[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv643[T.int64(0), v1, T.int64(0)] + conv1d88[v0, v1, v2]

    @T.prim_func(private=True)
    def fused_conv1d31_add14(p_reshape185: T.handle, wnconv1d91: T.Buffer((T.int64(256), T.int64(256), T.int64(7)), "float32"), lv650: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape185 = T.match_buffer(p_reshape185, (batch_size, T.int64(256), seq_len // T.int64(8)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(256), seq_len // T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(256), seq_len // T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(8)) * T.int64(256), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(28), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256) * batch_size) // (seq_len // T.int64(8) * T.int64(256)) + ax0)
                            v1 = T.axis.spatial(T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(256), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8) * batch_size * T.int64(256)) // (seq_len // T.int64(8) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8) * batch_size * T.int64(256)) // (seq_len // T.int64(8) * T.int64(256)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) < T.int64(256) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8)) < seq_len // T.int64(8))
                            T.reads(reshape185[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], wnconv1d91[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(9) <= v4 * T.int64(3) + v2 and v4 * T.int64(3) + v2 < seq_len // T.int64(8) + T.int64(9), reshape185[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], T.float32(0.0)) * wnconv1d91[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256) * batch_size) // (seq_len // T.int64(8) * T.int64(256)))
                            v1 = T.axis.spatial(T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)))
                            v2 = T.axis.spatial(seq_len // T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(8)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv650[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv650[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d32_add14(p_reshape189: T.handle, wnconv1d93: T.Buffer((T.int64(256), T.int64(256), T.int64(7)), "float32"), lv664: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape189 = T.match_buffer(p_reshape189, (batch_size, T.int64(256), seq_len // T.int64(8)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(256), seq_len // T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(256), seq_len // T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(8)) * T.int64(256), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(28), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256) * batch_size) // (seq_len // T.int64(8) * T.int64(256)) + ax0)
                            v1 = T.axis.spatial(T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(256), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8) * batch_size * T.int64(256)) // (seq_len // T.int64(8) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8) * batch_size * T.int64(256)) // (seq_len // T.int64(8) * T.int64(256)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)) < T.int64(256) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(8)) < seq_len // T.int64(8))
                            T.reads(reshape189[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], wnconv1d93[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(27) <= v4 * T.int64(9) + v2 and v4 * T.int64(9) + v2 < seq_len // T.int64(8) + T.int64(27), reshape189[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], T.float32(0.0)) * wnconv1d93[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256) * batch_size) // (seq_len // T.int64(8) * T.int64(256)))
                            v1 = T.axis.spatial(T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)))
                            v2 = T.axis.spatial(seq_len // T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(8)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv664[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv664[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d33_add16(p_reshape193: T.handle, wnconv1d95: T.Buffer((T.int64(512), T.int64(256), T.int64(16)), "float32"), lv678: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape193 = T.match_buffer(p_reshape193, (batch_size, T.int64(256), seq_len // T.int64(8)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(512), seq_len // T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(512), seq_len // T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(64)) * T.int64(512), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(64), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512) * batch_size) // (seq_len // T.int64(64) * T.int64(512)) + ax0)
                            v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(256), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(16))
                            v4 = T.axis.reduce(T.int64(16), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(16))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64) * batch_size * T.int64(512)) // (seq_len // T.int64(64) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64) * batch_size * T.int64(512)) // (seq_len // T.int64(64) * T.int64(512)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) < T.int64(512) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64)) < seq_len // T.int64(64))
                            T.reads(reshape193[v0, v3, v2 * T.int64(8) + v4 - T.int64(4)], wnconv1d95[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(4) <= v2 * T.int64(8) + v4 and v2 * T.int64(8) + v4 < seq_len // T.int64(8) + T.int64(4), reshape193[v0, v3, v2 * T.int64(8) + v4 - T.int64(4)], T.float32(0.0)) * wnconv1d95[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512) * batch_size) // (seq_len // T.int64(64) * T.int64(512)))
                            v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)))
                            v2 = T.axis.spatial(seq_len // T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(64)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv678[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv678[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d34_add16(p_reshape195: T.handle, wnconv1d96: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), "float32"), lv685: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape195 = T.match_buffer(p_reshape195, (batch_size, T.int64(512), seq_len // T.int64(64)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(512), seq_len // T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(512), seq_len // T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(64)) * T.int64(512), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(56), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512) * batch_size) // (seq_len // T.int64(64) * T.int64(512)) + ax0)
                            v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(512), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64) * batch_size * T.int64(512)) // (seq_len // T.int64(64) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64) * batch_size * T.int64(512)) // (seq_len // T.int64(64) * T.int64(512)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) < T.int64(512) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64)) < seq_len // T.int64(64))
                            T.reads(reshape195[v0, v3, v2 + v4 - T.int64(3)], wnconv1d96[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len // T.int64(64) + T.int64(3), reshape195[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d96[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512) * batch_size) // (seq_len // T.int64(64) * T.int64(512)))
                            v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)))
                            v2 = T.axis.spatial(seq_len // T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(64)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv685[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv685[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d35_add16_add17(p_reshape197: T.handle, wnconv1d97: T.Buffer((T.int64(512), T.int64(512), T.int64(1)), "float32"), lv692: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32"), p_conv1d95: T.handle, p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape197 = T.match_buffer(p_reshape197, (batch_size, T.int64(512), seq_len // T.int64(64)))
        conv1d95 = T.match_buffer(p_conv1d95, (batch_size, T.int64(512), seq_len // T.int64(64)))
        T_add_intermediate_1 = T.match_buffer(p_output0, (batch_size, T.int64(512), seq_len // T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(512), seq_len // T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(64)) * T.int64(512), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(8), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512) * batch_size) // (seq_len // T.int64(64) * T.int64(512)) + ax0)
                            v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(512), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64) * batch_size * T.int64(512)) // (seq_len // T.int64(64) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64) * batch_size * T.int64(512)) // (seq_len // T.int64(64) * T.int64(512)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) < T.int64(512) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64)) < seq_len // T.int64(64))
                            T.reads(reshape197[v0, v3, v2], wnconv1d97[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + reshape197[v0, v3, v2] * wnconv1d97[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add_1"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512) * batch_size) // (seq_len // T.int64(64) * T.int64(512)))
                        v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)))
                        v2 = T.axis.spatial(seq_len // T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(64)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv692[T.int64(0), v1, T.int64(0)], conv1d95[v0, v1, v2])
                        T.writes(T_add_intermediate_1[v0, v1, v2])
                        T_add_intermediate_1[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv692[T.int64(0), v1, T.int64(0)] + conv1d95[v0, v1, v2]

    @T.prim_func(private=True)
    def fused_conv1d36_add16(p_reshape199: T.handle, wnconv1d98: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), "float32"), lv699: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape199 = T.match_buffer(p_reshape199, (batch_size, T.int64(512), seq_len // T.int64(64)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(512), seq_len // T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(512), seq_len // T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(64)) * T.int64(512), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(56), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512) * batch_size) // (seq_len // T.int64(64) * T.int64(512)) + ax0)
                            v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(512), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64) * batch_size * T.int64(512)) // (seq_len // T.int64(64) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64) * batch_size * T.int64(512)) // (seq_len // T.int64(64) * T.int64(512)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) < T.int64(512) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64)) < seq_len // T.int64(64))
                            T.reads(reshape199[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], wnconv1d98[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(9) <= v4 * T.int64(3) + v2 and v4 * T.int64(3) + v2 < seq_len // T.int64(64) + T.int64(9), reshape199[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], T.float32(0.0)) * wnconv1d98[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512) * batch_size) // (seq_len // T.int64(64) * T.int64(512)))
                            v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)))
                            v2 = T.axis.spatial(seq_len // T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(64)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv699[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv699[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d37_add16(p_reshape203: T.handle, wnconv1d100: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), "float32"), lv713: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape203 = T.match_buffer(p_reshape203, (batch_size, T.int64(512), seq_len // T.int64(64)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(512), seq_len // T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(512), seq_len // T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(64)) * T.int64(512), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(56), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512) * batch_size) // (seq_len // T.int64(64) * T.int64(512)) + ax0)
                            v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(512), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64) * batch_size * T.int64(512)) // (seq_len // T.int64(64) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64) * batch_size * T.int64(512)) // (seq_len // T.int64(64) * T.int64(512)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)) < T.int64(512) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(64)) < seq_len // T.int64(64))
                            T.reads(reshape203[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], wnconv1d100[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(27) <= v4 * T.int64(9) + v2 and v4 * T.int64(9) + v2 < seq_len // T.int64(64) + T.int64(27), reshape203[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], T.float32(0.0)) * wnconv1d100[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512) * batch_size) // (seq_len // T.int64(64) * T.int64(512)))
                            v1 = T.axis.spatial(T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)))
                            v2 = T.axis.spatial(seq_len // T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(64)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv713[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv713[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d38_add18(p_reshape207: T.handle, wnconv1d102: T.Buffer((T.int64(1024), T.int64(512), T.int64(16)), "float32"), lv727: T.Buffer((T.int64(1), T.int64(1024), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape207 = T.match_buffer(p_reshape207, (batch_size, T.int64(512), seq_len // T.int64(64)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(1024), seq_len // T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(128), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024) * batch_size) // (seq_len // T.int64(512) * T.int64(1024)) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(512), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(16))
                            v4 = T.axis.reduce(T.int64(16), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(16))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(1024)) // (seq_len // T.int64(512) * T.int64(1024)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(1024)) // (seq_len // T.int64(512) * T.int64(1024)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)) < T.int64(1024) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512)) < seq_len // T.int64(512))
                            T.reads(reshape207[v0, v3, v2 * T.int64(8) + v4 - T.int64(4)], wnconv1d102[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(4) <= v2 * T.int64(8) + v4 and v2 * T.int64(8) + v4 < seq_len // T.int64(64) + T.int64(4), reshape207[v0, v3, v2 * T.int64(8) + v4 - T.int64(4)], T.float32(0.0)) * wnconv1d102[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024) * batch_size) // (seq_len // T.int64(512) * T.int64(1024)))
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)))
                            v2 = T.axis.spatial(seq_len // T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv727[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv727[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d39_add18(p_reshape209: T.handle, wnconv1d103: T.Buffer((T.int64(1024), T.int64(1024), T.int64(3)), "float32"), lv734: T.Buffer((T.int64(1), T.int64(1024), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape209 = T.match_buffer(p_reshape209, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(1024), seq_len // T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(48), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024) * batch_size) // (seq_len // T.int64(512) * T.int64(1024)) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(1024), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(3))
                            v4 = T.axis.reduce(T.int64(3), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(3))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(1024)) // (seq_len // T.int64(512) * T.int64(1024)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(1024)) // (seq_len // T.int64(512) * T.int64(1024)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)) < T.int64(1024) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512)) < seq_len // T.int64(512))
                            T.reads(reshape209[v0, v3, v2 + v4 - T.int64(1)], wnconv1d103[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(1) <= v2 + v4 and v2 + v4 <= seq_len // T.int64(512), reshape209[v0, v3, v2 + v4 - T.int64(1)], T.float32(0.0)) * wnconv1d103[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024) * batch_size) // (seq_len // T.int64(512) * T.int64(1024)))
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)))
                            v2 = T.axis.spatial(seq_len // T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv734[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv734[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d3_add1(p_reshape253: T.handle, wnconv1d125: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32"), lv876: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape253 = T.match_buffer(p_reshape253, (batch_size, T.int64(768), seq_len * T.int64(8)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(768), seq_len * T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(768), seq_len * T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(6144), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(84), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(6144) * batch_size) // (seq_len * T.int64(6144)) + ax0)
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(8), ax0_ax1_ax2_fused % (seq_len * T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(768), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(6144)) // (seq_len * T.int64(6144)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(6144)) // (seq_len * T.int64(8)) < T.int64(768))
                            T.reads(reshape253[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], wnconv1d125[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(9) <= v4 * T.int64(3) + v2 and v4 * T.int64(3) + v2 < seq_len * T.int64(8) + T.int64(9), reshape253[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], T.float32(0.0)) * wnconv1d125[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768) * batch_size) // (seq_len * T.int64(8) * T.int64(768)))
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)))
                            v2 = T.axis.spatial(seq_len * T.int64(8), ax0_ax1_ax2_fused % (seq_len * T.int64(8)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv876[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv876[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d40_add19(p_conv1d103: T.handle, wnconv1d104: T.Buffer((T.int64(8), T.int64(1024), T.int64(1)), "float32"), lv740: T.Buffer((T.int64(1), T.int64(8), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d103 = T.match_buffer(p_conv1d103, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(8), seq_len // T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(8), seq_len // T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(8), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(16), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * batch_size) // (seq_len // T.int64(512) * T.int64(8)) + ax0)
                            v1 = T.axis.spatial(T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) // (seq_len // T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(1024), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(8)) // (seq_len // T.int64(512) * T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(8)) // (seq_len // T.int64(512) * T.int64(8)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) // (seq_len // T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) // (seq_len // T.int64(512)) < T.int64(8) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512)) < seq_len // T.int64(512))
                            T.reads(conv1d103[v0, v3, v2], wnconv1d104[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + conv1d103[v0, v3, v2] * wnconv1d104[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * batch_size) // (seq_len // T.int64(512) * T.int64(8)))
                        v1 = T.axis.spatial(T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) // (seq_len // T.int64(512)))
                        v2 = T.axis.spatial(seq_len // T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv740[T.int64(0), v1, T.int64(0)])
                        T.writes(T_add_intermediate[v0, v1, v2])
                        T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv740[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d41_add18(p_permute_dims39: T.handle, wnconv1d105: T.Buffer((T.int64(1024), T.int64(8), T.int64(1)), "float32"), lv746: T.Buffer((T.int64(1), T.int64(1024), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        permute_dims39 = T.match_buffer(p_permute_dims39, (batch_size, T.int64(8), seq_len // T.int64(512)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(1024), seq_len // T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024) * batch_size) // (seq_len // T.int64(512) * T.int64(1024)) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(8), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(1024)) // (seq_len // T.int64(512) * T.int64(1024)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(1024)) // (seq_len // T.int64(512) * T.int64(1024)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)) < T.int64(1024) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512)) < seq_len // T.int64(512) and ax3_fused_0 * T.int64(64) + ax3_fused_1 < T.int64(8))
                            T.reads(permute_dims39[v0, v3, v2], wnconv1d105[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + permute_dims39[v0, v3, v2] * wnconv1d105[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024) * batch_size) // (seq_len // T.int64(512) * T.int64(1024)))
                        v1 = T.axis.spatial(T.int64(1024), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)))
                        v2 = T.axis.spatial(seq_len // T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv746[T.int64(0), v1, T.int64(0)])
                        T.writes(T_add_intermediate[v0, v1, v2])
                        T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv746[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d42_add23(p_add29: T.handle, wnconv1d48: T.Buffer((T.int64(1536), T.int64(1024), T.int64(7)), "float32"), lv322: T.Buffer((T.int64(1), T.int64(1536), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        add29 = T.match_buffer(p_add29, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1536), seq_len // T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(1536), seq_len // T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1536), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(112), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1536) * batch_size) // (seq_len // T.int64(512) * T.int64(1536)) + ax0)
                            v1 = T.axis.spatial(T.int64(1536), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1536)) // (seq_len // T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(1024), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(1536)) // (seq_len // T.int64(512) * T.int64(1536)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(1536)) // (seq_len // T.int64(512) * T.int64(1536)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1536)) // (seq_len // T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1536)) // (seq_len // T.int64(512)) < T.int64(1536) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512)) < seq_len // T.int64(512))
                            T.reads(add29[v0, v3, v2 + v4 - T.int64(3)], wnconv1d48[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len // T.int64(512) + T.int64(3), add29[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d48[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1536) * batch_size) // (seq_len // T.int64(512) * T.int64(1536)))
                            v1 = T.axis.spatial(T.int64(1536), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1536)) // (seq_len // T.int64(512)))
                            v2 = T.axis.spatial(seq_len // T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv322[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv322[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d43_add24(p_reshape97: T.handle, wnconv1d49: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32"), lv336: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape97 = T.match_buffer(p_reshape97, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(6144), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(84), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144) * batch_size) // (seq_len // T.int64(512) * T.int64(6144)) + ax0)
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(768), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(6144)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(6144)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(8)) < T.int64(768) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) < seq_len // T.int64(512) * T.int64(8))
                            T.reads(reshape97[v0, v3, v2 + v4 - T.int64(3)], wnconv1d49[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len // T.int64(512) * T.int64(8) + T.int64(3), reshape97[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d49[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768) * batch_size) // (seq_len // T.int64(512) * T.int64(8) * T.int64(768)))
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv336[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv336[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d44_add24_add25(p_reshape99: T.handle, wnconv1d50: T.Buffer((T.int64(768), T.int64(768), T.int64(1)), "float32"), lv343: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_conv1d_transpose: T.handle, p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape99 = T.match_buffer(p_reshape99, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        conv1d_transpose = T.match_buffer(p_conv1d_transpose, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        T_add_intermediate_1 = T.match_buffer(p_output0, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(6144), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(12), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144) * batch_size) // (seq_len // T.int64(512) * T.int64(6144)) + ax0)
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(768), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(6144)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(6144)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(8)) < T.int64(768) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) < seq_len // T.int64(512) * T.int64(8))
                            T.reads(reshape99[v0, v3, v2], wnconv1d50[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + reshape99[v0, v3, v2] * wnconv1d50[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add_1"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768) * batch_size) // (seq_len // T.int64(512) * T.int64(8) * T.int64(768)))
                        v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)))
                        v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv343[T.int64(0), v1, T.int64(0)], conv1d_transpose[v0, v1, v2])
                        T.writes(T_add_intermediate_1[v0, v1, v2])
                        T_add_intermediate_1[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv343[T.int64(0), v1, T.int64(0)] + conv1d_transpose[v0, v1, v2]

    @T.prim_func(private=True)
    def fused_conv1d45_add24(p_reshape101: T.handle, wnconv1d51: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32"), lv350: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape101 = T.match_buffer(p_reshape101, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(6144), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(84), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144) * batch_size) // (seq_len // T.int64(512) * T.int64(6144)) + ax0)
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(768), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(6144)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(6144)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(8)) < T.int64(768) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) < seq_len // T.int64(512) * T.int64(8))
                            T.reads(reshape101[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], wnconv1d51[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(9) <= v4 * T.int64(3) + v2 and v4 * T.int64(3) + v2 < seq_len // T.int64(512) * T.int64(8) + T.int64(9), reshape101[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], T.float32(0.0)) * wnconv1d51[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768) * batch_size) // (seq_len // T.int64(512) * T.int64(8) * T.int64(768)))
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv350[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv350[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d46_add24(p_reshape105: T.handle, wnconv1d53: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32"), lv364: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape105 = T.match_buffer(p_reshape105, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(6144), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(84), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144) * batch_size) // (seq_len // T.int64(512) * T.int64(6144)) + ax0)
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(768), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(6144)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(6144)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(8)) < T.int64(768) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) < seq_len // T.int64(512) * T.int64(8))
                            T.reads(reshape105[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], wnconv1d53[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(27) <= v4 * T.int64(9) + v2 and v4 * T.int64(9) + v2 < seq_len // T.int64(512) * T.int64(8) + T.int64(27), reshape105[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], T.float32(0.0)) * wnconv1d53[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768) * batch_size) // (seq_len // T.int64(512) * T.int64(8) * T.int64(768)))
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv364[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv364[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d47_add26(p_reshape111: T.handle, wnconv1d55: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32"), lv385: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape111 = T.match_buffer(p_reshape111, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(24576), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(42), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576) * batch_size) // (seq_len // T.int64(512) * T.int64(24576)) + ax0)
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(384), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(24576)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(24576)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(64)) < T.int64(384) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) < seq_len // T.int64(512) * T.int64(64))
                            T.reads(reshape111[v0, v3, v2 + v4 - T.int64(3)], wnconv1d55[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len // T.int64(512) * T.int64(64) + T.int64(3), reshape111[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d55[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384) * batch_size) // (seq_len // T.int64(512) * T.int64(64) * T.int64(384)))
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv385[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv385[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d48_add26_add27(p_reshape113: T.handle, wnconv1d56: T.Buffer((T.int64(384), T.int64(384), T.int64(1)), "float32"), lv392: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_conv1d_transpose1: T.handle, p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape113 = T.match_buffer(p_reshape113, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        conv1d_transpose1 = T.match_buffer(p_conv1d_transpose1, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        T_add_intermediate_1 = T.match_buffer(p_output0, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(24576), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(6), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576) * batch_size) // (seq_len // T.int64(512) * T.int64(24576)) + ax0)
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(384), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(24576)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(24576)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(64)) < T.int64(384) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) < seq_len // T.int64(512) * T.int64(64))
                            T.reads(reshape113[v0, v3, v2], wnconv1d56[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + reshape113[v0, v3, v2] * wnconv1d56[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add_1"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384) * batch_size) // (seq_len // T.int64(512) * T.int64(64) * T.int64(384)))
                        v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)))
                        v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv392[T.int64(0), v1, T.int64(0)], conv1d_transpose1[v0, v1, v2])
                        T.writes(T_add_intermediate_1[v0, v1, v2])
                        T_add_intermediate_1[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv392[T.int64(0), v1, T.int64(0)] + conv1d_transpose1[v0, v1, v2]

    @T.prim_func(private=True)
    def fused_conv1d49_add26(p_reshape115: T.handle, wnconv1d57: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32"), lv399: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape115 = T.match_buffer(p_reshape115, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(24576), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(42), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576) * batch_size) // (seq_len // T.int64(512) * T.int64(24576)) + ax0)
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(384), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(24576)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(24576)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(64)) < T.int64(384) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) < seq_len // T.int64(512) * T.int64(64))
                            T.reads(reshape115[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], wnconv1d57[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(9) <= v4 * T.int64(3) + v2 and v4 * T.int64(3) + v2 < seq_len // T.int64(512) * T.int64(64) + T.int64(9), reshape115[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], T.float32(0.0)) * wnconv1d57[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384) * batch_size) // (seq_len // T.int64(512) * T.int64(64) * T.int64(384)))
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv399[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv399[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d4_add1(p_reshape257: T.handle, wnconv1d127: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32"), lv890: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape257 = T.match_buffer(p_reshape257, (batch_size, T.int64(768), seq_len * T.int64(8)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(768), seq_len * T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(768), seq_len * T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(6144), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(84), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(6144) * batch_size) // (seq_len * T.int64(6144)) + ax0)
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(8), ax0_ax1_ax2_fused % (seq_len * T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(768), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(6144)) // (seq_len * T.int64(6144)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(6144)) // (seq_len * T.int64(8)) < T.int64(768))
                            T.reads(reshape257[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], wnconv1d127[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(27) <= v4 * T.int64(9) + v2 and v4 * T.int64(9) + v2 < seq_len * T.int64(8) + T.int64(27), reshape257[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], T.float32(0.0)) * wnconv1d127[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768) * batch_size) // (seq_len * T.int64(8) * T.int64(768)))
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)))
                            v2 = T.axis.spatial(seq_len * T.int64(8), ax0_ax1_ax2_fused % (seq_len * T.int64(8)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv890[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv890[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d50_add26(p_reshape119: T.handle, wnconv1d59: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32"), lv413: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape119 = T.match_buffer(p_reshape119, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(24576), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(42), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576) * batch_size) // (seq_len // T.int64(512) * T.int64(24576)) + ax0)
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(384), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(24576)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(24576)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(64)) < T.int64(384) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) < seq_len // T.int64(512) * T.int64(64))
                            T.reads(reshape119[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], wnconv1d59[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(27) <= v4 * T.int64(9) + v2 and v4 * T.int64(9) + v2 < seq_len // T.int64(512) * T.int64(64) + T.int64(27), reshape119[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], T.float32(0.0)) * wnconv1d59[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384) * batch_size) // (seq_len // T.int64(512) * T.int64(64) * T.int64(384)))
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv413[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv413[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d51_add28(p_reshape125: T.handle, wnconv1d61: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32"), lv434: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape125 = T.match_buffer(p_reshape125, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(21), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152) * batch_size) // (seq_len // T.int64(512) * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) + ax2)
                            v3 = T.axis.reduce(T.int64(192), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(256)) < T.int64(192) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) < seq_len // T.int64(512) * T.int64(256))
                            T.reads(reshape125[v0, v3, v2 + v4 - T.int64(3)], wnconv1d61[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len // T.int64(512) * T.int64(256) + T.int64(3), reshape125[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d61[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192) * batch_size) // (seq_len // T.int64(512) * T.int64(256) * T.int64(192)))
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv434[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv434[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d52_add28_add29(p_reshape127: T.handle, wnconv1d62: T.Buffer((T.int64(192), T.int64(192), T.int64(1)), "float32"), lv441: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_conv1d_transpose2: T.handle, p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape127 = T.match_buffer(p_reshape127, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        conv1d_transpose2 = T.match_buffer(p_conv1d_transpose2, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        T_add_intermediate_1 = T.match_buffer(p_output0, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(3), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152) * batch_size) // (seq_len // T.int64(512) * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) + ax2)
                            v3 = T.axis.reduce(T.int64(192), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(256)) < T.int64(192) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) < seq_len // T.int64(512) * T.int64(256))
                            T.reads(reshape127[v0, v3, v2], wnconv1d62[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + reshape127[v0, v3, v2] * wnconv1d62[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add_1"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192) * batch_size) // (seq_len // T.int64(512) * T.int64(256) * T.int64(192)))
                        v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)))
                        v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv441[T.int64(0), v1, T.int64(0)], conv1d_transpose2[v0, v1, v2])
                        T.writes(T_add_intermediate_1[v0, v1, v2])
                        T_add_intermediate_1[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv441[T.int64(0), v1, T.int64(0)] + conv1d_transpose2[v0, v1, v2]

    @T.prim_func(private=True)
    def fused_conv1d53_add28(p_reshape129: T.handle, wnconv1d63: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32"), lv448: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape129 = T.match_buffer(p_reshape129, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(21), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152) * batch_size) // (seq_len // T.int64(512) * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) + ax2)
                            v3 = T.axis.reduce(T.int64(192), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(256)) < T.int64(192) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) < seq_len // T.int64(512) * T.int64(256))
                            T.reads(reshape129[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], wnconv1d63[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(9) <= v4 * T.int64(3) + v2 and v4 * T.int64(3) + v2 < seq_len // T.int64(512) * T.int64(256) + T.int64(9), reshape129[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], T.float32(0.0)) * wnconv1d63[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192) * batch_size) // (seq_len // T.int64(512) * T.int64(256) * T.int64(192)))
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv448[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv448[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d54_add28(p_reshape133: T.handle, wnconv1d65: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32"), lv462: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape133 = T.match_buffer(p_reshape133, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(21), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152) * batch_size) // (seq_len // T.int64(512) * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) + ax2)
                            v3 = T.axis.reduce(T.int64(192), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(256)) < T.int64(192) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) < seq_len // T.int64(512) * T.int64(256))
                            T.reads(reshape133[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], wnconv1d65[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(27) <= v4 * T.int64(9) + v2 and v4 * T.int64(9) + v2 < seq_len // T.int64(512) * T.int64(256) + T.int64(27), reshape133[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], T.float32(0.0)) * wnconv1d65[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192) * batch_size) // (seq_len // T.int64(512) * T.int64(256) * T.int64(192)))
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv462[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv462[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d55_add30(p_reshape139: T.handle, wnconv1d67: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32"), lv483: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape139 = T.match_buffer(p_reshape139, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(11), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152) * batch_size) // (seq_len // T.int64(512) * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(96), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(512)) < T.int64(96) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) < seq_len // T.int64(512) * T.int64(512) and ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1 < T.int64(672))
                            T.reads(reshape139[v0, v3, v2 + v4 - T.int64(3)], wnconv1d67[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len // T.int64(512) * T.int64(512) + T.int64(3), reshape139[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d67[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96) * batch_size) // (seq_len // T.int64(512) * T.int64(512) * T.int64(96)))
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv483[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv483[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d56_add30_add31(p_reshape141: T.handle, wnconv1d68: T.Buffer((T.int64(96), T.int64(96), T.int64(1)), "float32"), lv490: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_conv1d_transpose3: T.handle, p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape141 = T.match_buffer(p_reshape141, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        conv1d_transpose3 = T.match_buffer(p_conv1d_transpose3, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        T_add_intermediate_1 = T.match_buffer(p_output0, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(2), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152) * batch_size) // (seq_len // T.int64(512) * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(96), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(512)) < T.int64(96) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) < seq_len // T.int64(512) * T.int64(512) and ax3_fused_0 * T.int64(64) + ax3_fused_1 < T.int64(96))
                            T.reads(reshape141[v0, v3, v2], wnconv1d68[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + reshape141[v0, v3, v2] * wnconv1d68[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add_1"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96) * batch_size) // (seq_len // T.int64(512) * T.int64(512) * T.int64(96)))
                        v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)))
                        v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv490[T.int64(0), v1, T.int64(0)], conv1d_transpose3[v0, v1, v2])
                        T.writes(T_add_intermediate_1[v0, v1, v2])
                        T_add_intermediate_1[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv490[T.int64(0), v1, T.int64(0)] + conv1d_transpose3[v0, v1, v2]

    @T.prim_func(private=True)
    def fused_conv1d57_add30(p_reshape143: T.handle, wnconv1d69: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32"), lv497: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape143 = T.match_buffer(p_reshape143, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(11), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152) * batch_size) // (seq_len // T.int64(512) * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(96), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(512)) < T.int64(96) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) < seq_len // T.int64(512) * T.int64(512) and ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1 < T.int64(672))
                            T.reads(reshape143[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], wnconv1d69[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(9) <= v4 * T.int64(3) + v2 and v4 * T.int64(3) + v2 < seq_len // T.int64(512) * T.int64(512) + T.int64(9), reshape143[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], T.float32(0.0)) * wnconv1d69[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96) * batch_size) // (seq_len // T.int64(512) * T.int64(512) * T.int64(96)))
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv497[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv497[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d58_add30(p_reshape147: T.handle, wnconv1d71: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32"), lv511: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape147 = T.match_buffer(p_reshape147, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(11), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152) * batch_size) // (seq_len // T.int64(512) * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(96), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(512)) < T.int64(96) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) < seq_len // T.int64(512) * T.int64(512) and ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1 < T.int64(672))
                            T.reads(reshape147[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], wnconv1d71[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(27) <= v4 * T.int64(9) + v2 and v4 * T.int64(9) + v2 < seq_len // T.int64(512) * T.int64(512) + T.int64(27), reshape147[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], T.float32(0.0)) * wnconv1d71[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96) * batch_size) // (seq_len // T.int64(512) * T.int64(512) * T.int64(96)))
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv511[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv511[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d59_reshape10_add32_tir_tanh1(p_reshape151: T.handle, wnconv1d73: T.Buffer((T.int64(1), T.int64(96), T.int64(7)), "float32"), decoder_model_6_bias: T.Buffer((T.int64(1),), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape151 = T.match_buffer(p_reshape151, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        compute_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1), seq_len // T.int64(512) * T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(1), seq_len // T.int64(512) * T.int64(512)), scope="shared")
        for ax0_ax1_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(512), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax2_ax3_fused_0 in T.serial(T.int64(11), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_fused % (seq_len // T.int64(512) * T.int64(512) * batch_size) // (seq_len // T.int64(512) * T.int64(512)) + ax0)
                            v1 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), ax0_ax1_fused % (seq_len // T.int64(512) * T.int64(512)) + ax1)
                            v2 = T.axis.reduce(T.int64(96), (ax2_ax3_fused_0 * T.int64(64) + ax2_ax3_fused_1) // T.int64(7))
                            v3 = T.axis.reduce(T.int64(7), (ax2_ax3_fused_0 * T.int64(64) + ax2_ax3_fused_1) % T.int64(7))
                            T.where(T.int64(0) <= ax0_ax1_fused % (seq_len // T.int64(512) * batch_size * T.int64(512)) // (seq_len // T.int64(512) * T.int64(512)) and ax0_ax1_fused % (seq_len // T.int64(512) * batch_size * T.int64(512)) // (seq_len // T.int64(512) * T.int64(512)) < batch_size and T.int64(0) <= ax0_ax1_fused % (seq_len // T.int64(512) * T.int64(512)) and ax0_ax1_fused % (seq_len // T.int64(512) * T.int64(512)) < seq_len // T.int64(512) * T.int64(512) and ax2_ax3_fused_0 * T.int64(64) + ax2_ax3_fused_1 < T.int64(672))
                            T.reads(reshape151[v0, v2, v1 + v3 - T.int64(3)], wnconv1d73[T.int64(0), v2, v3])
                            T.writes(conv1d_ncw_intermediate_shared[v0, T.int64(0), v1])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, T.int64(0), v1] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, T.int64(0), v1] = conv1d_ncw_intermediate_shared[v0, T.int64(0), v1] + T.if_then_else(T.int64(3) <= v1 + v3 and v1 + v3 < seq_len // T.int64(512) * T.int64(512) + T.int64(3), reshape151[v0, v2, v1 + v3 - T.int64(3)], T.float32(0.0)) * wnconv1d73[T.int64(0), v2, v3]
            for ax2 in range(T.int64(1)):
                for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("compute"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_fused % (seq_len // T.int64(512) * T.int64(512) * batch_size) // (seq_len // T.int64(512) * T.int64(512)))
                            v1 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), ax0_ax1_fused % (seq_len // T.int64(512) * T.int64(512)))
                            v2 = T.axis.spatial(T.int64(1), ax2)
                            v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                            T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, T.int64(0), v1], decoder_model_6_bias[T.int64(0)])
                            T.writes(compute_intermediate[v0, T.int64(0), v1])
                            compute_intermediate[v0, T.int64(0), v1] = T.tanh(conv1d_ncw_intermediate_shared[v0, T.int64(0), v1] + decoder_model_6_bias[T.int64(0)])

    @T.prim_func(private=True)
    def fused_conv1d5_add3(p_reshape263: T.handle, wnconv1d129: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32"), lv911: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape263 = T.match_buffer(p_reshape263, (batch_size, T.int64(384), seq_len * T.int64(64)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(384), seq_len * T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(384), seq_len * T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(24576), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(42), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(24576) * batch_size) // (seq_len * T.int64(24576)) + ax0)
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(384), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(24576)) // (seq_len * T.int64(24576)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(24576)) // (seq_len * T.int64(64)) < T.int64(384))
                            T.reads(reshape263[v0, v3, v2 + v4 - T.int64(3)], wnconv1d129[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len * T.int64(64) + T.int64(3), reshape263[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d129[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384) * batch_size) // (seq_len * T.int64(64) * T.int64(384)))
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)))
                            v2 = T.axis.spatial(seq_len * T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv911[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv911[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d6_add3_add4(p_reshape265: T.handle, wnconv1d130: T.Buffer((T.int64(384), T.int64(384), T.int64(1)), "float32"), lv918: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_conv1d_transpose5: T.handle, p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape265 = T.match_buffer(p_reshape265, (batch_size, T.int64(384), seq_len * T.int64(64)))
        conv1d_transpose5 = T.match_buffer(p_conv1d_transpose5, (batch_size, T.int64(384), seq_len * T.int64(64)))
        T_add_intermediate_1 = T.match_buffer(p_output0, (batch_size, T.int64(384), seq_len * T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(384), seq_len * T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(24576), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(6), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(24576) * batch_size) // (seq_len * T.int64(24576)) + ax0)
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(384), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(24576)) // (seq_len * T.int64(24576)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(24576)) // (seq_len * T.int64(64)) < T.int64(384))
                            T.reads(reshape265[v0, v3, v2], wnconv1d130[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + reshape265[v0, v3, v2] * wnconv1d130[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add_1"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384) * batch_size) // (seq_len * T.int64(64) * T.int64(384)))
                        v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)))
                        v2 = T.axis.spatial(seq_len * T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv918[T.int64(0), v1, T.int64(0)], conv1d_transpose5[v0, v1, v2])
                        T.writes(T_add_intermediate_1[v0, v1, v2])
                        T_add_intermediate_1[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv918[T.int64(0), v1, T.int64(0)] + conv1d_transpose5[v0, v1, v2]

    @T.prim_func(private=True)
    def fused_conv1d7_add3(p_reshape267: T.handle, wnconv1d131: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32"), lv925: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape267 = T.match_buffer(p_reshape267, (batch_size, T.int64(384), seq_len * T.int64(64)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(384), seq_len * T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(384), seq_len * T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(24576), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(42), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(24576) * batch_size) // (seq_len * T.int64(24576)) + ax0)
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(384), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(24576)) // (seq_len * T.int64(24576)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(24576)) // (seq_len * T.int64(64)) < T.int64(384))
                            T.reads(reshape267[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], wnconv1d131[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(9) <= v4 * T.int64(3) + v2 and v4 * T.int64(3) + v2 < seq_len * T.int64(64) + T.int64(9), reshape267[v0, v3, v4 * T.int64(3) + v2 - T.int64(9)], T.float32(0.0)) * wnconv1d131[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384) * batch_size) // (seq_len * T.int64(64) * T.int64(384)))
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)))
                            v2 = T.axis.spatial(seq_len * T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv925[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv925[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d8_add3(p_reshape271: T.handle, wnconv1d133: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32"), lv939: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape271 = T.match_buffer(p_reshape271, (batch_size, T.int64(384), seq_len * T.int64(64)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(384), seq_len * T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(384), seq_len * T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(24576), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(42), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(24576) * batch_size) // (seq_len * T.int64(24576)) + ax0)
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(384), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(24576)) // (seq_len * T.int64(24576)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(24576)) // (seq_len * T.int64(64)) < T.int64(384))
                            T.reads(reshape271[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], wnconv1d133[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(27) <= v4 * T.int64(9) + v2 and v4 * T.int64(9) + v2 < seq_len * T.int64(64) + T.int64(27), reshape271[v0, v3, v4 * T.int64(9) + v2 - T.int64(27)], T.float32(0.0)) * wnconv1d133[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384) * batch_size) // (seq_len * T.int64(64) * T.int64(384)))
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)))
                            v2 = T.axis.spatial(seq_len * T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv939[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv939[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d9_add5(p_reshape277: T.handle, wnconv1d135: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32"), lv960: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape277 = T.match_buffer(p_reshape277, (batch_size, T.int64(192), seq_len * T.int64(256)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(192), seq_len * T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(192), seq_len * T.int64(256)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(21), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(49152) * batch_size) // (seq_len * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(256), ax0_ax1_ax2_fused % (seq_len * T.int64(256)) + ax2)
                            v3 = T.axis.reduce(T.int64(192), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(49152)) // (seq_len * T.int64(49152)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(49152)) // (seq_len * T.int64(256)) < T.int64(192))
                            T.reads(reshape277[v0, v3, v2 + v4 - T.int64(3)], wnconv1d135[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len * T.int64(256) + T.int64(3), reshape277[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d135[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192) * batch_size) // (seq_len * T.int64(256) * T.int64(192)))
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)))
                            v2 = T.axis.spatial(seq_len * T.int64(256), ax0_ax1_ax2_fused % (seq_len * T.int64(256)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv960[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv960[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d_add(p_z: T.handle, wnconv1d122: T.Buffer((T.int64(1536), T.int64(1024), T.int64(7)), "float32"), lv848: T.Buffer((T.int64(1), T.int64(1536), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, seq_len = T.int64(), T.int64()
        z = T.match_buffer(p_z, (batch_size, T.int64(1024), seq_len))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1536), seq_len))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(1536), seq_len), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(1536), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(112), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(1536) * batch_size) // (seq_len * T.int64(1536)) + ax0)
                            v1 = T.axis.spatial(T.int64(1536), ax0_ax1_ax2_fused % (seq_len * T.int64(1536)) // seq_len + ax1)
                            v2 = T.axis.spatial(seq_len, ax0_ax1_ax2_fused % seq_len + ax2)
                            v3 = T.axis.reduce(T.int64(1024), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(7))
                            v4 = T.axis.reduce(T.int64(7), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(7))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(1536)) // (seq_len * T.int64(1536)) < batch_size)
                            T.reads(z[v0, v3, v2 + v4 - T.int64(3)], wnconv1d122[v1, v3, v4])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(3) <= v2 + v4 and v2 + v4 < seq_len + T.int64(3), z[v0, v3, v2 + v4 - T.int64(3)], T.float32(0.0)) * wnconv1d122[v1, v3, v4]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(1536) * batch_size) // (seq_len * T.int64(1536)))
                            v1 = T.axis.spatial(T.int64(1536), ax0_ax1_ax2_fused % (seq_len * T.int64(1536)) // seq_len)
                            v2 = T.axis.spatial(seq_len, ax0_ax1_ax2_fused % seq_len)
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(conv1d_ncw_intermediate_shared[v0, v1, v2], lv848[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + lv848[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d_transpose1_add3(p_reshape261: T.handle, wnconvtranspose1d5: T.Buffer((T.int64(768), T.int64(384), T.int64(16)), "float32"), lv904: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape261 = T.match_buffer(p_reshape261, (batch_size, T.int64(768), seq_len * T.int64(8)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(384), seq_len * T.int64(64)))
        # with T.block("root"):
        compute_intermediate_shared = T.alloc_buffer((batch_size, T.int64(384), seq_len * T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(24576), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(192), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("compute"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(24576) * batch_size) // (seq_len * T.int64(24576)) + ax0)
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(768), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(16))
                            v4 = T.axis.reduce(T.int64(16), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(16))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(24576)) // (seq_len * T.int64(24576)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(24576)) // (seq_len * T.int64(64)) < T.int64(384))
                            T.reads(reshape261[v0, v3, (v2 + v4 - T.int64(11)) // T.int64(8)], wnconvtranspose1d5[v3, v1, v4 * T.int64(-1) + T.int64(15)])
                            T.writes(compute_intermediate_shared[v0, v1, v2])
                            with T.init():
                                compute_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            compute_intermediate_shared[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(11) <= v2 + v4 and v2 + v4 < seq_len * T.int64(64) + T.int64(4), T.if_then_else((v2 + v4 - T.int64(11)) % T.int64(8) == T.int64(0), reshape261[v0, v3, (v2 + v4 - T.int64(11)) // T.int64(8)], T.float32(0.0)), T.float32(0.0)) * wnconvtranspose1d5[v3, v1, v4 * T.int64(-1) + T.int64(15)]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384) * batch_size) // (seq_len * T.int64(64) * T.int64(384)))
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)))
                            v2 = T.axis.spatial(seq_len * T.int64(64), ax0_ax1_ax2_fused % (seq_len * T.int64(64)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(compute_intermediate_shared[v0, v1, v2], lv904[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + lv904[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d_transpose2_add5(p_reshape275: T.handle, wnconvtranspose1d6: T.Buffer((T.int64(384), T.int64(192), T.int64(8)), "float32"), lv953: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape275 = T.match_buffer(p_reshape275, (batch_size, T.int64(384), seq_len * T.int64(64)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(192), seq_len * T.int64(256)))
        # with T.block("root"):
        compute_intermediate_shared = T.alloc_buffer((batch_size, T.int64(192), seq_len * T.int64(256)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(48), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("compute"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(49152) * batch_size) // (seq_len * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(256), ax0_ax1_ax2_fused % (seq_len * T.int64(256)) + ax2)
                            v3 = T.axis.reduce(T.int64(384), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(8))
                            v4 = T.axis.reduce(T.int64(8), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(8))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(49152)) // (seq_len * T.int64(49152)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(49152)) // (seq_len * T.int64(256)) < T.int64(192))
                            T.reads(reshape275[v0, v3, (v2 + v4 - T.int64(5)) // T.int64(4)], wnconvtranspose1d6[v3, v1, v4 * T.int64(-1) + T.int64(7)])
                            T.writes(compute_intermediate_shared[v0, v1, v2])
                            with T.init():
                                compute_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            compute_intermediate_shared[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(5) <= v2 + v4 and v2 + v4 < seq_len * T.int64(256) + T.int64(2), T.if_then_else((v2 + v4 - T.int64(5)) % T.int64(4) == T.int64(0), reshape275[v0, v3, (v2 + v4 - T.int64(5)) // T.int64(4)], T.float32(0.0)), T.float32(0.0)) * wnconvtranspose1d6[v3, v1, v4 * T.int64(-1) + T.int64(7)]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192) * batch_size) // (seq_len * T.int64(256) * T.int64(192)))
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)))
                            v2 = T.axis.spatial(seq_len * T.int64(256), ax0_ax1_ax2_fused % (seq_len * T.int64(256)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(compute_intermediate_shared[v0, v1, v2], lv953[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + lv953[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d_transpose3_add7(p_reshape289: T.handle, wnconvtranspose1d7: T.Buffer((T.int64(192), T.int64(96), T.int64(4)), "float32"), lv1002: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape289 = T.match_buffer(p_reshape289, (batch_size, T.int64(192), seq_len * T.int64(256)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(96), seq_len * T.int64(512)))
        # with T.block("root"):
        compute_intermediate_shared = T.alloc_buffer((batch_size, T.int64(96), seq_len * T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(12), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("compute"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(49152) * batch_size) // (seq_len * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(512), ax0_ax1_ax2_fused % (seq_len * T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(192), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(4))
                            v4 = T.axis.reduce(T.int64(4), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(4))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(49152)) // (seq_len * T.int64(49152)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(49152)) // (seq_len * T.int64(512)) < T.int64(96))
                            T.reads(reshape289[v0, v3, (v2 + v4 - T.int64(2)) // T.int64(2)], wnconvtranspose1d7[v3, v1, v4 * T.int64(-1) + T.int64(3)])
                            T.writes(compute_intermediate_shared[v0, v1, v2])
                            with T.init():
                                compute_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            compute_intermediate_shared[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(2) <= v2 + v4 and v2 + v4 <= seq_len * T.int64(512), T.if_then_else((v2 + v4 - T.int64(2)) % T.int64(2) == T.int64(0), reshape289[v0, v3, (v2 + v4 - T.int64(2)) // T.int64(2)], T.float32(0.0)), T.float32(0.0)) * wnconvtranspose1d7[v3, v1, v4 * T.int64(-1) + T.int64(3)]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96) * batch_size) // (seq_len * T.int64(512) * T.int64(96)))
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)))
                            v2 = T.axis.spatial(seq_len * T.int64(512), ax0_ax1_ax2_fused % (seq_len * T.int64(512)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(compute_intermediate_shared[v0, v1, v2], lv1002[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + lv1002[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d_transpose4_add24(p_reshape95: T.handle, wnconvtranspose1d: T.Buffer((T.int64(1536), T.int64(768), T.int64(16)), "float32"), lv329: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape95 = T.match_buffer(p_reshape95, (batch_size, T.int64(1536), seq_len // T.int64(512)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        # with T.block("root"):
        compute_intermediate_shared = T.alloc_buffer((batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(6144), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(384), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("compute"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144) * batch_size) // (seq_len // T.int64(512) * T.int64(6144)) + ax0)
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(1536), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(16))
                            v4 = T.axis.reduce(T.int64(16), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(16))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(6144)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(6144)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(6144)) // (seq_len // T.int64(512) * T.int64(8)) < T.int64(768) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)) < seq_len // T.int64(512) * T.int64(8))
                            T.reads(reshape95[v0, v3, (v2 + v4 - T.int64(11)) // T.int64(8)], wnconvtranspose1d[v3, v1, v4 * T.int64(-1) + T.int64(15)])
                            T.writes(compute_intermediate_shared[v0, v1, v2])
                            with T.init():
                                compute_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            compute_intermediate_shared[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(11) <= v2 + v4 and v2 + v4 < seq_len // T.int64(512) * T.int64(8) + T.int64(4), T.if_then_else((v2 + v4 - T.int64(11)) % T.int64(8) == T.int64(0), reshape95[v0, v3, (v2 + v4 - T.int64(11)) // T.int64(8)], T.float32(0.0)), T.float32(0.0)) * wnconvtranspose1d[v3, v1, v4 * T.int64(-1) + T.int64(15)]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768) * batch_size) // (seq_len // T.int64(512) * T.int64(8) * T.int64(768)))
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(8), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(8)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(compute_intermediate_shared[v0, v1, v2], lv329[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + lv329[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d_transpose5_add26(p_reshape109: T.handle, wnconvtranspose1d1: T.Buffer((T.int64(768), T.int64(384), T.int64(16)), "float32"), lv378: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape109 = T.match_buffer(p_reshape109, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        # with T.block("root"):
        compute_intermediate_shared = T.alloc_buffer((batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(24576), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(192), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("compute"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576) * batch_size) // (seq_len // T.int64(512) * T.int64(24576)) + ax0)
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) + ax2)
                            v3 = T.axis.reduce(T.int64(768), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(16))
                            v4 = T.axis.reduce(T.int64(16), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(16))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(24576)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(24576)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(24576)) // (seq_len // T.int64(512) * T.int64(64)) < T.int64(384) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)) < seq_len // T.int64(512) * T.int64(64))
                            T.reads(reshape109[v0, v3, (v2 + v4 - T.int64(11)) // T.int64(8)], wnconvtranspose1d1[v3, v1, v4 * T.int64(-1) + T.int64(15)])
                            T.writes(compute_intermediate_shared[v0, v1, v2])
                            with T.init():
                                compute_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            compute_intermediate_shared[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(11) <= v2 + v4 and v2 + v4 < seq_len // T.int64(512) * T.int64(64) + T.int64(4), T.if_then_else((v2 + v4 - T.int64(11)) % T.int64(8) == T.int64(0), reshape109[v0, v3, (v2 + v4 - T.int64(11)) // T.int64(8)], T.float32(0.0)), T.float32(0.0)) * wnconvtranspose1d1[v3, v1, v4 * T.int64(-1) + T.int64(15)]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384) * batch_size) // (seq_len // T.int64(512) * T.int64(64) * T.int64(384)))
                            v1 = T.axis.spatial(T.int64(384), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(64), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(64)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(compute_intermediate_shared[v0, v1, v2], lv378[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + lv378[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d_transpose6_add28(p_reshape123: T.handle, wnconvtranspose1d2: T.Buffer((T.int64(384), T.int64(192), T.int64(8)), "float32"), lv427: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape123 = T.match_buffer(p_reshape123, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        # with T.block("root"):
        compute_intermediate_shared = T.alloc_buffer((batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(48), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("compute"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152) * batch_size) // (seq_len // T.int64(512) * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) + ax2)
                            v3 = T.axis.reduce(T.int64(384), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(8))
                            v4 = T.axis.reduce(T.int64(8), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(8))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(256)) < T.int64(192) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)) < seq_len // T.int64(512) * T.int64(256))
                            T.reads(reshape123[v0, v3, (v2 + v4 - T.int64(5)) // T.int64(4)], wnconvtranspose1d2[v3, v1, v4 * T.int64(-1) + T.int64(7)])
                            T.writes(compute_intermediate_shared[v0, v1, v2])
                            with T.init():
                                compute_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            compute_intermediate_shared[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(5) <= v2 + v4 and v2 + v4 < seq_len // T.int64(512) * T.int64(256) + T.int64(2), T.if_then_else((v2 + v4 - T.int64(5)) % T.int64(4) == T.int64(0), reshape123[v0, v3, (v2 + v4 - T.int64(5)) // T.int64(4)], T.float32(0.0)), T.float32(0.0)) * wnconvtranspose1d2[v3, v1, v4 * T.int64(-1) + T.int64(7)]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192) * batch_size) // (seq_len // T.int64(512) * T.int64(256) * T.int64(192)))
                            v1 = T.axis.spatial(T.int64(192), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(256), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(256)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(compute_intermediate_shared[v0, v1, v2], lv427[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + lv427[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d_transpose7_add30(p_reshape137: T.handle, wnconvtranspose1d3: T.Buffer((T.int64(192), T.int64(96), T.int64(4)), "float32"), lv476: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape137 = T.match_buffer(p_reshape137, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        # with T.block("root"):
        compute_intermediate_shared = T.alloc_buffer((batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(49152), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(12), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("compute"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152) * batch_size) // (seq_len // T.int64(512) * T.int64(49152)) + ax0)
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(192), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(4))
                            v4 = T.axis.reduce(T.int64(4), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(4))
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(49152)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(49152)) // (seq_len // T.int64(512) * T.int64(512)) < T.int64(96) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)) < seq_len // T.int64(512) * T.int64(512))
                            T.reads(reshape137[v0, v3, (v2 + v4 - T.int64(2)) // T.int64(2)], wnconvtranspose1d3[v3, v1, v4 * T.int64(-1) + T.int64(3)])
                            T.writes(compute_intermediate_shared[v0, v1, v2])
                            with T.init():
                                compute_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            compute_intermediate_shared[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(2) <= v2 + v4 and v2 + v4 <= seq_len // T.int64(512) * T.int64(512), T.if_then_else((v2 + v4 - T.int64(2)) % T.int64(2) == T.int64(0), reshape137[v0, v3, (v2 + v4 - T.int64(2)) // T.int64(2)], T.float32(0.0)), T.float32(0.0)) * wnconvtranspose1d3[v3, v1, v4 * T.int64(-1) + T.int64(3)]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96) * batch_size) // (seq_len // T.int64(512) * T.int64(512) * T.int64(96)))
                            v1 = T.axis.spatial(T.int64(96), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)))
                            v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(512)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(compute_intermediate_shared[v0, v1, v2], lv476[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + lv476[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d_transpose_add1(p_reshape247: T.handle, wnconvtranspose1d4: T.Buffer((T.int64(1536), T.int64(768), T.int64(16)), "float32"), lv855: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, seq_len = T.int64(), T.int64()
        reshape247 = T.match_buffer(p_reshape247, (batch_size, T.int64(1536), seq_len))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(768), seq_len * T.int64(8)))
        # with T.block("root"):
        compute_intermediate_shared = T.alloc_buffer((batch_size, T.int64(768), seq_len * T.int64(8)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * seq_len * T.int64(6144), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_ax4_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_ax4_fused_0 in T.serial(T.int64(384), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("compute"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(6144) * batch_size) // (seq_len * T.int64(6144)) + ax0)
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)) + ax1)
                            v2 = T.axis.spatial(seq_len * T.int64(8), ax0_ax1_ax2_fused % (seq_len * T.int64(8)) + ax2)
                            v3 = T.axis.reduce(T.int64(1536), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) // T.int64(16))
                            v4 = T.axis.reduce(T.int64(16), (ax3_ax4_fused_0 * T.int64(64) + ax3_ax4_fused_1) % T.int64(16))
                            T.where(ax0_ax1_ax2_fused % (seq_len * batch_size * T.int64(6144)) // (seq_len * T.int64(6144)) < batch_size and ax0_ax1_ax2_fused % (seq_len * T.int64(6144)) // (seq_len * T.int64(8)) < T.int64(768))
                            T.reads(reshape247[v0, v3, (v2 + v4 - T.int64(11)) // T.int64(8)], wnconvtranspose1d4[v3, v1, v4 * T.int64(-1) + T.int64(15)])
                            T.writes(compute_intermediate_shared[v0, v1, v2])
                            with T.init():
                                compute_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            compute_intermediate_shared[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + T.if_then_else(T.int64(11) <= v2 + v4 and v2 + v4 < seq_len * T.int64(8) + T.int64(4), T.if_then_else((v2 + v4 - T.int64(11)) % T.int64(8) == T.int64(0), reshape247[v0, v3, (v2 + v4 - T.int64(11)) // T.int64(8)], T.float32(0.0)), T.float32(0.0)) * wnconvtranspose1d4[v3, v1, v4 * T.int64(-1) + T.int64(15)]
            for ax3 in range(T.int64(1)):
                for ax4_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax4_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("T_add"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768) * batch_size) // (seq_len * T.int64(8) * T.int64(768)))
                            v1 = T.axis.spatial(T.int64(768), ax0_ax1_ax2_fused % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)))
                            v2 = T.axis.spatial(seq_len * T.int64(8), ax0_ax1_ax2_fused % (seq_len * T.int64(8)))
                            v3 = T.axis.spatial(T.int64(1), ax3)
                            v4 = T.axis.spatial(T.int64(1), ax4_0 * T.int64(64) + ax4_1)
                            T.where(ax4_0 * T.int64(64) + ax4_1 < T.int64(1))
                            T.reads(compute_intermediate_shared[v0, v1, v2], lv855[T.int64(0), v1, T.int64(0)])
                            T.writes(T_add_intermediate[v0, v1, v2])
                            T_add_intermediate[v0, v1, v2] = compute_intermediate_shared[v0, v1, v2] + lv855[T.int64(0), v1, T.int64(0)]

    @T.prim_func(private=True)
    def fused_matmul1_multiply29_subtract2_add22(p_divide12: T.handle, permute_dims25: T.Buffer((T.int64(8), T.int64(1024)), "float32"), p_sum26: T.handle, permute_dims26: T.Buffer((T.int64(1), T.int64(1024)), "float32"), p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        divide12 = T.match_buffer(p_divide12, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        sum26 = T.match_buffer(p_sum26, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1024)))
        # with T.block("root"):
        matmul_intermediate_shared = T.alloc_buffer((batch_size * (seq_len // T.int64(512)), T.int64(1024)), scope="shared")
        for ax0_ax1_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("matmul"):
                            v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024) + ax1)
                            v2 = T.axis.reduce(T.int64(8), ax2_fused_0 * T.int64(64) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(64) + ax2_fused_1 < T.int64(8))
                            T.reads(divide12[v0, v2], permute_dims25[v2, v1])
                            T.writes(matmul_intermediate_shared[v0, v1])
                            with T.init():
                                matmul_intermediate_shared[v0, v1] = T.float32(0.0)
                            matmul_intermediate_shared[v0, v1] = matmul_intermediate_shared[v0, v1] + divide12[v0, v2] * permute_dims25[v2, v1]
            for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add"):
                        v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024))
                        v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024))
                        v2 = T.axis.spatial(T.int64(1), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(1))
                        T.reads(sum26[v0, T.int64(0)], matmul_intermediate_shared[v0, v1], permute_dims26[T.int64(0), v1])
                        T.writes(T_add_intermediate[v0, v1])
                        T_add_intermediate[v0, v1] = sum26[v0, T.int64(0)] - matmul_intermediate_shared[v0, v1] * T.float32(2.0) + permute_dims26[T.int64(0), v1]

    @T.prim_func(private=True)
    def fused_matmul1_multiply29_subtract_add22(p_divide24: T.handle, permute_dims49: T.Buffer((T.int64(8), T.int64(1024)), "float32"), p_sum50: T.handle, permute_dims50: T.Buffer((T.int64(1), T.int64(1024)), "float32"), p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        divide24 = T.match_buffer(p_divide24, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        sum50 = T.match_buffer(p_sum50, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1024)))
        # with T.block("root"):
        matmul_intermediate_shared = T.alloc_buffer((batch_size * (seq_len // T.int64(512)), T.int64(1024)), scope="shared")
        for ax0_ax1_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("matmul"):
                            v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024) + ax1)
                            v2 = T.axis.reduce(T.int64(8), ax2_fused_0 * T.int64(64) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(64) + ax2_fused_1 < T.int64(8))
                            T.reads(divide24[v0, v2], permute_dims49[v2, v1])
                            T.writes(matmul_intermediate_shared[v0, v1])
                            with T.init():
                                matmul_intermediate_shared[v0, v1] = T.float32(0.0)
                            matmul_intermediate_shared[v0, v1] = matmul_intermediate_shared[v0, v1] + divide24[v0, v2] * permute_dims49[v2, v1]
            for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add"):
                        v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024))
                        v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024))
                        v2 = T.axis.spatial(T.int64(1), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(1))
                        T.reads(sum50[v0, T.int64(0)], matmul_intermediate_shared[v0, v1], permute_dims50[T.int64(0), v1])
                        T.writes(T_add_intermediate[v0, v1])
                        T_add_intermediate[v0, v1] = sum50[v0, T.int64(0)] - matmul_intermediate_shared[v0, v1] * T.float32(2.0) + permute_dims50[T.int64(0), v1]

    @T.prim_func(private=True)
    def fused_matmul1_multiply31_subtract2_add20(p_divide26: T.handle, permute_dims53: T.Buffer((T.int64(8), T.int64(1024)), "float32"), p_sum54: T.handle, permute_dims54: T.Buffer((T.int64(1), T.int64(1024)), "float32"), p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        divide26 = T.match_buffer(p_divide26, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        sum54 = T.match_buffer(p_sum54, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1024)))
        # with T.block("root"):
        matmul_intermediate_shared = T.alloc_buffer((batch_size * (seq_len // T.int64(512)), T.int64(1024)), scope="shared")
        for ax0_ax1_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("matmul"):
                            v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024) + ax1)
                            v2 = T.axis.reduce(T.int64(8), ax2_fused_0 * T.int64(64) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(64) + ax2_fused_1 < T.int64(8))
                            T.reads(divide26[v0, v2], permute_dims53[v2, v1])
                            T.writes(matmul_intermediate_shared[v0, v1])
                            with T.init():
                                matmul_intermediate_shared[v0, v1] = T.float32(0.0)
                            matmul_intermediate_shared[v0, v1] = matmul_intermediate_shared[v0, v1] + divide26[v0, v2] * permute_dims53[v2, v1]
            for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add"):
                        v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024))
                        v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024))
                        v2 = T.axis.spatial(T.int64(1), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(1))
                        T.reads(sum54[v0, T.int64(0)], matmul_intermediate_shared[v0, v1], permute_dims54[T.int64(0), v1])
                        T.writes(T_add_intermediate[v0, v1])
                        T_add_intermediate[v0, v1] = sum54[v0, T.int64(0)] - matmul_intermediate_shared[v0, v1] * T.float32(2.0) + permute_dims54[T.int64(0), v1]

    @T.prim_func(private=True)
    def fused_matmul1_multiply31_subtract2_add22(p_divide30: T.handle, permute_dims61: T.Buffer((T.int64(8), T.int64(1024)), "float32"), p_sum62: T.handle, permute_dims62: T.Buffer((T.int64(1), T.int64(1024)), "float32"), p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        divide30 = T.match_buffer(p_divide30, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        sum62 = T.match_buffer(p_sum62, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1024)))
        # with T.block("root"):
        matmul_intermediate_shared = T.alloc_buffer((batch_size * (seq_len // T.int64(512)), T.int64(1024)), scope="shared")
        for ax0_ax1_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("matmul"):
                            v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024) + ax1)
                            v2 = T.axis.reduce(T.int64(8), ax2_fused_0 * T.int64(64) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(64) + ax2_fused_1 < T.int64(8))
                            T.reads(divide30[v0, v2], permute_dims61[v2, v1])
                            T.writes(matmul_intermediate_shared[v0, v1])
                            with T.init():
                                matmul_intermediate_shared[v0, v1] = T.float32(0.0)
                            matmul_intermediate_shared[v0, v1] = matmul_intermediate_shared[v0, v1] + divide30[v0, v2] * permute_dims61[v2, v1]
            for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add"):
                        v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024))
                        v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024))
                        v2 = T.axis.spatial(T.int64(1), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(1))
                        T.reads(sum62[v0, T.int64(0)], matmul_intermediate_shared[v0, v1], permute_dims62[T.int64(0), v1])
                        T.writes(T_add_intermediate[v0, v1])
                        T_add_intermediate[v0, v1] = sum62[v0, T.int64(0)] - matmul_intermediate_shared[v0, v1] * T.float32(2.0) + permute_dims62[T.int64(0), v1]

    @T.prim_func(private=True)
    def fused_matmul1_multiply31_subtract_add20(p_divide20: T.handle, permute_dims41: T.Buffer((T.int64(8), T.int64(1024)), "float32"), p_sum42: T.handle, permute_dims42: T.Buffer((T.int64(1), T.int64(1024)), "float32"), p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        divide20 = T.match_buffer(p_divide20, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        sum42 = T.match_buffer(p_sum42, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1024)))
        # with T.block("root"):
        matmul_intermediate_shared = T.alloc_buffer((batch_size * (seq_len // T.int64(512)), T.int64(1024)), scope="shared")
        for ax0_ax1_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("matmul"):
                            v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024) + ax1)
                            v2 = T.axis.reduce(T.int64(8), ax2_fused_0 * T.int64(64) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(64) + ax2_fused_1 < T.int64(8))
                            T.reads(divide20[v0, v2], permute_dims41[v2, v1])
                            T.writes(matmul_intermediate_shared[v0, v1])
                            with T.init():
                                matmul_intermediate_shared[v0, v1] = T.float32(0.0)
                            matmul_intermediate_shared[v0, v1] = matmul_intermediate_shared[v0, v1] + divide20[v0, v2] * permute_dims41[v2, v1]
            for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add"):
                        v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024))
                        v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024))
                        v2 = T.axis.spatial(T.int64(1), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(1))
                        T.reads(sum42[v0, T.int64(0)], matmul_intermediate_shared[v0, v1], permute_dims42[T.int64(0), v1])
                        T.writes(T_add_intermediate[v0, v1])
                        T_add_intermediate[v0, v1] = sum42[v0, T.int64(0)] - matmul_intermediate_shared[v0, v1] * T.float32(2.0) + permute_dims42[T.int64(0), v1]

    @T.prim_func(private=True)
    def fused_matmul1_multiply31_subtract_add22(p_divide2: T.handle, permute_dims5: T.Buffer((T.int64(8), T.int64(1024)), "float32"), p_sum6: T.handle, permute_dims6: T.Buffer((T.int64(1), T.int64(1024)), "float32"), p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        divide2 = T.match_buffer(p_divide2, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        sum6 = T.match_buffer(p_sum6, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1024)))
        # with T.block("root"):
        matmul_intermediate_shared = T.alloc_buffer((batch_size * (seq_len // T.int64(512)), T.int64(1024)), scope="shared")
        for ax0_ax1_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("matmul"):
                            v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024) + ax1)
                            v2 = T.axis.reduce(T.int64(8), ax2_fused_0 * T.int64(64) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(64) + ax2_fused_1 < T.int64(8))
                            T.reads(divide2[v0, v2], permute_dims5[v2, v1])
                            T.writes(matmul_intermediate_shared[v0, v1])
                            with T.init():
                                matmul_intermediate_shared[v0, v1] = T.float32(0.0)
                            matmul_intermediate_shared[v0, v1] = matmul_intermediate_shared[v0, v1] + divide2[v0, v2] * permute_dims5[v2, v1]
            for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add"):
                        v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024))
                        v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024))
                        v2 = T.axis.spatial(T.int64(1), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(1))
                        T.reads(sum6[v0, T.int64(0)], matmul_intermediate_shared[v0, v1], permute_dims6[T.int64(0), v1])
                        T.writes(T_add_intermediate[v0, v1])
                        T_add_intermediate[v0, v1] = sum6[v0, T.int64(0)] - matmul_intermediate_shared[v0, v1] * T.float32(2.0) + permute_dims6[T.int64(0), v1]

    @T.prim_func(private=True)
    def fused_matmul_multiply29_subtract_add20(p_divide18: T.handle, permute_dims37: T.Buffer((T.int64(8), T.int64(1024)), "float32"), p_sum38: T.handle, permute_dims38: T.Buffer((T.int64(1), T.int64(1024)), "float32"), p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        divide18 = T.match_buffer(p_divide18, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        sum38 = T.match_buffer(p_sum38, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1024)))
        # with T.block("root"):
        matmul_intermediate_shared = T.alloc_buffer((batch_size * (seq_len // T.int64(512)), T.int64(1024)), scope="shared")
        for ax0_ax1_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("matmul"):
                            v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024) + ax1)
                            v2 = T.axis.reduce(T.int64(8), ax2_fused_0 * T.int64(64) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(64) + ax2_fused_1 < T.int64(8))
                            T.reads(divide18[v0, v2], permute_dims37[v2, v1])
                            T.writes(matmul_intermediate_shared[v0, v1])
                            with T.init():
                                matmul_intermediate_shared[v0, v1] = T.float32(0.0)
                            matmul_intermediate_shared[v0, v1] = matmul_intermediate_shared[v0, v1] + divide18[v0, v2] * permute_dims37[v2, v1]
            for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add"):
                        v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024))
                        v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024))
                        v2 = T.axis.spatial(T.int64(1), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(1))
                        T.reads(sum38[v0, T.int64(0)], matmul_intermediate_shared[v0, v1], permute_dims38[T.int64(0), v1])
                        T.writes(T_add_intermediate[v0, v1])
                        T_add_intermediate[v0, v1] = sum38[v0, T.int64(0)] - matmul_intermediate_shared[v0, v1] * T.float32(2.0) + permute_dims38[T.int64(0), v1]

    @T.prim_func(private=True)
    def fused_matmul_multiply31_subtract2_add20(p_divide28: T.handle, permute_dims57: T.Buffer((T.int64(8), T.int64(1024)), "float32"), p_sum58: T.handle, permute_dims58: T.Buffer((T.int64(1), T.int64(1024)), "float32"), p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        divide28 = T.match_buffer(p_divide28, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        sum58 = T.match_buffer(p_sum58, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1024)))
        # with T.block("root"):
        matmul_intermediate_shared = T.alloc_buffer((batch_size * (seq_len // T.int64(512)), T.int64(1024)), scope="shared")
        for ax0_ax1_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("matmul"):
                            v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024) + ax1)
                            v2 = T.axis.reduce(T.int64(8), ax2_fused_0 * T.int64(64) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(64) + ax2_fused_1 < T.int64(8))
                            T.reads(divide28[v0, v2], permute_dims57[v2, v1])
                            T.writes(matmul_intermediate_shared[v0, v1])
                            with T.init():
                                matmul_intermediate_shared[v0, v1] = T.float32(0.0)
                            matmul_intermediate_shared[v0, v1] = matmul_intermediate_shared[v0, v1] + divide28[v0, v2] * permute_dims57[v2, v1]
            for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add"):
                        v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024))
                        v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024))
                        v2 = T.axis.spatial(T.int64(1), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(1))
                        T.reads(sum58[v0, T.int64(0)], matmul_intermediate_shared[v0, v1], permute_dims58[T.int64(0), v1])
                        T.writes(T_add_intermediate[v0, v1])
                        T_add_intermediate[v0, v1] = sum58[v0, T.int64(0)] - matmul_intermediate_shared[v0, v1] * T.float32(2.0) + permute_dims58[T.int64(0), v1]

    @T.prim_func(private=True)
    def fused_matmul_multiply31_subtract2_add22(p_divide34: T.handle, permute_dims69: T.Buffer((T.int64(8), T.int64(1024)), "float32"), p_sum70: T.handle, permute_dims70: T.Buffer((T.int64(1), T.int64(1024)), "float32"), p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        divide34 = T.match_buffer(p_divide34, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        sum70 = T.match_buffer(p_sum70, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1024)))
        # with T.block("root"):
        matmul_intermediate_shared = T.alloc_buffer((batch_size * (seq_len // T.int64(512)), T.int64(1024)), scope="shared")
        for ax0_ax1_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("matmul"):
                            v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024) + ax1)
                            v2 = T.axis.reduce(T.int64(8), ax2_fused_0 * T.int64(64) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(64) + ax2_fused_1 < T.int64(8))
                            T.reads(divide34[v0, v2], permute_dims69[v2, v1])
                            T.writes(matmul_intermediate_shared[v0, v1])
                            with T.init():
                                matmul_intermediate_shared[v0, v1] = T.float32(0.0)
                            matmul_intermediate_shared[v0, v1] = matmul_intermediate_shared[v0, v1] + divide34[v0, v2] * permute_dims69[v2, v1]
            for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add"):
                        v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024))
                        v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024))
                        v2 = T.axis.spatial(T.int64(1), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(1))
                        T.reads(sum70[v0, T.int64(0)], matmul_intermediate_shared[v0, v1], permute_dims70[T.int64(0), v1])
                        T.writes(T_add_intermediate[v0, v1])
                        T_add_intermediate[v0, v1] = sum70[v0, T.int64(0)] - matmul_intermediate_shared[v0, v1] * T.float32(2.0) + permute_dims70[T.int64(0), v1]

    @T.prim_func(private=True)
    def fused_matmul_multiply31_subtract_add20(p_divide: T.handle, permute_dims1: T.Buffer((T.int64(8), T.int64(1024)), "float32"), p_sum2: T.handle, permute_dims2: T.Buffer((T.int64(1), T.int64(1024)), "float32"), p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        divide = T.match_buffer(p_divide, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        sum2 = T.match_buffer(p_sum2, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1024)))
        # with T.block("root"):
        matmul_intermediate_shared = T.alloc_buffer((batch_size * (seq_len // T.int64(512)), T.int64(1024)), scope="shared")
        for ax0_ax1_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                for ax2_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax2_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("matmul"):
                            v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024) + ax1)
                            v2 = T.axis.reduce(T.int64(8), ax2_fused_0 * T.int64(64) + ax2_fused_1)
                            T.where(ax2_fused_0 * T.int64(64) + ax2_fused_1 < T.int64(8))
                            T.reads(divide[v0, v2], permute_dims1[v2, v1])
                            T.writes(matmul_intermediate_shared[v0, v1])
                            with T.init():
                                matmul_intermediate_shared[v0, v1] = T.float32(0.0)
                            matmul_intermediate_shared[v0, v1] = matmul_intermediate_shared[v0, v1] + divide[v0, v2] * permute_dims1[v2, v1]
            for ax2_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax2_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add"):
                        v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_ax1_fused // T.int64(1024))
                        v1 = T.axis.spatial(T.int64(1024), ax0_ax1_fused % T.int64(1024))
                        v2 = T.axis.spatial(T.int64(1), ax2_0 * T.int64(64) + ax2_1)
                        T.where(ax2_0 * T.int64(64) + ax2_1 < T.int64(1))
                        T.reads(sum2[v0, T.int64(0)], matmul_intermediate_shared[v0, v1], permute_dims2[T.int64(0), v1])
                        T.writes(T_add_intermediate[v0, v1])
                        T_add_intermediate[v0, v1] = sum2[v0, T.int64(0)] - matmul_intermediate_shared[v0, v1] * T.float32(2.0) + permute_dims2[T.int64(0), v1]

    @T.prim_func(private=True)
    def fused_tir_sqrt10_divide26_multiply26(lv723: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32"), encoder_block_4_block_4_weight_v1: T.Buffer((T.int64(1024), T.int64(512), T.int64(16)), "float32"), encoder_block_4_block_4_weight_g1: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1024), T.int64(512), T.int64(16)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8192), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(8192))
                    v1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(8192) // T.int64(16))
                    v2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(16))
                    T.reads(encoder_block_4_block_4_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_4_block_4_weight_v1[v0, v1, v2], lv723[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_4_block_4_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_4_block_4_weight_v1[v0, v1, v2] / T.sqrt(lv723[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt10_divide27_multiply27(lv730: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32"), encoder_block_6_weight_v1: T.Buffer((T.int64(1024), T.int64(1024), T.int64(3)), "float32"), encoder_block_6_weight_g1: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1024), T.int64(1024), T.int64(3)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(3072), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(3072))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(3072) // T.int64(3))
                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(3))
                    T.reads(encoder_block_6_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_6_weight_v1[v0, v1, v2], lv730[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_6_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_6_weight_v1[v0, v1, v2] / T.sqrt(lv730[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt10_divide31_multiply30(lv742: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32"), quantizer_quantizers_0_out_proj_weight_v1: T.Buffer((T.int64(1024), T.int64(8), T.int64(1)), "float32"), quantizer_quantizers_0_out_proj_weight_g1: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1024), T.int64(8), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(8), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(1024), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.reads(quantizer_quantizers_0_out_proj_weight_g1[v0, T.int64(0), T.int64(0)], quantizer_quantizers_0_out_proj_weight_v1[v0, v1, T.int64(0)], lv742[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = quantizer_quantizers_0_out_proj_weight_g1[v0, T.int64(0), T.int64(0)] * (quantizer_quantizers_0_out_proj_weight_v1[v0, v1, T.int64(0)] / T.sqrt(lv742[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt11_divide28_multiply28(lv736: T.Buffer((T.int64(8), T.int64(1), T.int64(1)), "float32"), quantizer_quantizers_0_in_proj_weight_v1: T.Buffer((T.int64(8), T.int64(1024), T.int64(1)), "float32"), quantizer_quantizers_0_in_proj_weight_g1: T.Buffer((T.int64(8), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(8), T.int64(1024), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(8), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(1024))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(1024))
                    T.reads(quantizer_quantizers_0_in_proj_weight_g1[v0, T.int64(0), T.int64(0)], quantizer_quantizers_0_in_proj_weight_v1[v0, v1, T.int64(0)], lv736[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = quantizer_quantizers_0_in_proj_weight_g1[v0, T.int64(0), T.int64(0)] * (quantizer_quantizers_0_in_proj_weight_v1[v0, v1, T.int64(0)] / T.sqrt(lv736[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt1_divide2_multiply2(lv858: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32"), decoder_model_1_block_2_block_1_weight_v2: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32"), decoder_model_1_block_2_block_1_weight_g2: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(4032), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(5376))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(5376) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(decoder_model_1_block_2_block_1_weight_g2[v0, T.int64(0), T.int64(0)], decoder_model_1_block_2_block_1_weight_v2[v0, v1, v2], lv858[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_1_block_2_block_1_weight_g2[v0, T.int64(0), T.int64(0)] * (decoder_model_1_block_2_block_1_weight_v2[v0, v1, v2] / T.sqrt(lv858[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt1_divide3_multiply3(lv865: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32"), decoder_model_1_block_2_block_3_weight_v2: T.Buffer((T.int64(768), T.int64(768), T.int64(1)), "float32"), decoder_model_1_block_2_block_3_weight_g2: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(768), T.int64(768), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(576), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(768), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(768))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(768))
                    T.reads(decoder_model_1_block_2_block_3_weight_g2[v0, T.int64(0), T.int64(0)], decoder_model_1_block_2_block_3_weight_v2[v0, v1, T.int64(0)], lv865[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = decoder_model_1_block_2_block_3_weight_g2[v0, T.int64(0), T.int64(0)] * (decoder_model_1_block_2_block_3_weight_v2[v0, v1, T.int64(0)] / T.sqrt(lv865[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt1_divide4_multiply4(lv900: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32"), decoder_model_2_block_1_weight_v2: T.Buffer((T.int64(768), T.int64(384), T.int64(16)), "float32"), decoder_model_2_block_1_weight_g2: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(768), T.int64(384), T.int64(16)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(4608), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(6144))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(6144) // T.int64(16))
                    v2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(16))
                    T.reads(decoder_model_2_block_1_weight_g2[v0, T.int64(0), T.int64(0)], decoder_model_2_block_1_weight_v2[v0, v1, v2], lv900[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_2_block_1_weight_g2[v0, T.int64(0), T.int64(0)] * (decoder_model_2_block_1_weight_v2[v0, v1, v2] / T.sqrt(lv900[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt2_divide5_multiply5(lv907: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32"), decoder_model_2_block_2_block_1_weight_v2: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32"), decoder_model_2_block_2_block_1_weight_g2: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(1008), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(2688))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(2688) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(decoder_model_2_block_2_block_1_weight_g2[v0, T.int64(0), T.int64(0)], decoder_model_2_block_2_block_1_weight_v2[v0, v1, v2], lv907[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_2_block_2_block_1_weight_g2[v0, T.int64(0), T.int64(0)] * (decoder_model_2_block_2_block_1_weight_v2[v0, v1, v2] / T.sqrt(lv907[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt2_divide6_multiply6(lv914: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32"), decoder_model_2_block_2_block_3_weight_v2: T.Buffer((T.int64(384), T.int64(384), T.int64(1)), "float32"), decoder_model_2_block_2_block_3_weight_g2: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(384), T.int64(384), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(144), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(384), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(384))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(384))
                    T.reads(decoder_model_2_block_2_block_3_weight_g2[v0, T.int64(0), T.int64(0)], decoder_model_2_block_2_block_3_weight_v2[v0, v1, T.int64(0)], lv914[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = decoder_model_2_block_2_block_3_weight_g2[v0, T.int64(0), T.int64(0)] * (decoder_model_2_block_2_block_3_weight_v2[v0, v1, T.int64(0)] / T.sqrt(lv914[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt2_divide7_multiply7(lv949: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32"), decoder_model_3_block_1_weight_v2: T.Buffer((T.int64(384), T.int64(192), T.int64(8)), "float32"), decoder_model_3_block_1_weight_g2: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(384), T.int64(192), T.int64(8)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(576), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(1536))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(1536) // T.int64(8))
                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(8))
                    T.reads(decoder_model_3_block_1_weight_g2[v0, T.int64(0), T.int64(0)], decoder_model_3_block_1_weight_v2[v0, v1, v2], lv949[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_3_block_1_weight_g2[v0, T.int64(0), T.int64(0)] * (decoder_model_3_block_1_weight_v2[v0, v1, v2] / T.sqrt(lv949[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt3_divide10_multiply10(lv998: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32"), decoder_model_4_block_1_weight_v2: T.Buffer((T.int64(192), T.int64(96), T.int64(4)), "float32"), decoder_model_4_block_1_weight_g2: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(192), T.int64(96), T.int64(4)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(72), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(384))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(384) // T.int64(4))
                    v2 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(4))
                    T.reads(decoder_model_4_block_1_weight_g2[v0, T.int64(0), T.int64(0)], decoder_model_4_block_1_weight_v2[v0, v1, v2], lv998[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_4_block_1_weight_g2[v0, T.int64(0), T.int64(0)] * (decoder_model_4_block_1_weight_v2[v0, v1, v2] / T.sqrt(lv998[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt3_divide8_multiply8(lv956: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32"), decoder_model_3_block_2_block_1_weight_v2: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32"), decoder_model_3_block_2_block_1_weight_g2: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(252), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(1344))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(1344) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(decoder_model_3_block_2_block_1_weight_g2[v0, T.int64(0), T.int64(0)], decoder_model_3_block_2_block_1_weight_v2[v0, v1, v2], lv956[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_3_block_2_block_1_weight_g2[v0, T.int64(0), T.int64(0)] * (decoder_model_3_block_2_block_1_weight_v2[v0, v1, v2] / T.sqrt(lv956[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt3_divide9_multiply9(lv963: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32"), decoder_model_3_block_2_block_3_weight_v2: T.Buffer((T.int64(192), T.int64(192), T.int64(1)), "float32"), decoder_model_3_block_2_block_3_weight_g2: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(192), T.int64(192), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(36), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(192), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(192))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(192))
                    T.reads(decoder_model_3_block_2_block_3_weight_g2[v0, T.int64(0), T.int64(0)], decoder_model_3_block_2_block_3_weight_v2[v0, v1, T.int64(0)], lv963[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = decoder_model_3_block_2_block_3_weight_g2[v0, T.int64(0), T.int64(0)] * (decoder_model_3_block_2_block_3_weight_v2[v0, v1, T.int64(0)] / T.sqrt(lv963[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt4_divide11_multiply11(lv1005: T.Buffer((T.int64(96), T.int64(1), T.int64(1)), "float32"), decoder_model_4_block_2_block_1_weight_v2: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32"), decoder_model_4_block_2_block_1_weight_g2: T.Buffer((T.int64(96), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(63), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(672))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(672) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(decoder_model_4_block_2_block_1_weight_g2[v0, T.int64(0), T.int64(0)], decoder_model_4_block_2_block_1_weight_v2[v0, v1, v2], lv1005[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_4_block_2_block_1_weight_g2[v0, T.int64(0), T.int64(0)] * (decoder_model_4_block_2_block_1_weight_v2[v0, v1, v2] / T.sqrt(lv1005[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt4_divide12_multiply12(lv1012: T.Buffer((T.int64(96), T.int64(1), T.int64(1)), "float32"), decoder_model_4_block_2_block_3_weight_v2: T.Buffer((T.int64(96), T.int64(96), T.int64(1)), "float32"), decoder_model_4_block_2_block_3_weight_g2: T.Buffer((T.int64(96), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(96), T.int64(96), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(9), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(96), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(96))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(96))
                    T.reads(decoder_model_4_block_2_block_3_weight_g2[v0, T.int64(0), T.int64(0)], decoder_model_4_block_2_block_3_weight_v2[v0, v1, T.int64(0)], lv1012[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = decoder_model_4_block_2_block_3_weight_g2[v0, T.int64(0), T.int64(0)] * (decoder_model_4_block_2_block_3_weight_v2[v0, v1, T.int64(0)] / T.sqrt(lv1012[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt5_divide13_multiply13(lv1047: T.Buffer((T.int64(1), T.int64(1), T.int64(1)), "float32"), decoder_model_6_weight_v2: T.Buffer((T.int64(1), T.int64(96), T.int64(7)), "float32"), decoder_model_6_weight_g2: T.Buffer((T.int64(1), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(96), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(96), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(7))
                    v1 = T.axis.spatial(T.int64(7), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(7))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(672))
                    T.reads(decoder_model_6_weight_g2[T.int64(0), T.int64(0), T.int64(0)], decoder_model_6_weight_v2[T.int64(0), v0, v1], lv1047[T.int64(0), T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[T.int64(0), v0, v1])
                    T_multiply_intermediate[T.int64(0), v0, v1] = decoder_model_6_weight_g2[T.int64(0), T.int64(0), T.int64(0)] * (decoder_model_6_weight_v2[T.int64(0), v0, v1] / T.sqrt(lv1047[T.int64(0), T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt6_divide14_multiply14(lv527: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32"), encoder_block_0_weight_v1: T.Buffer((T.int64(64), T.int64(1), T.int64(7)), "float32"), encoder_block_0_weight_g1: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(64), T.int64(1), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(64), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(7))
                    v1 = T.axis.spatial(T.int64(7), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(7))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(448))
                    T.reads(encoder_block_0_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_0_weight_v1[v0, T.int64(0), v1], lv527[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, T.int64(0), v1])
                    T_multiply_intermediate[v0, T.int64(0), v1] = encoder_block_0_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_0_weight_v1[v0, T.int64(0), v1] / T.sqrt(lv527[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt6_divide15_multiply15(lv534: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32"), encoder_block_1_block_0_block_1_weight_v1: T.Buffer((T.int64(64), T.int64(64), T.int64(7)), "float32"), encoder_block_1_block_0_block_1_weight_g1: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(64), T.int64(64), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(28), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(448))
                    v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(448) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(encoder_block_1_block_0_block_1_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_1_block_0_block_1_weight_v1[v0, v1, v2], lv534[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_1_block_0_block_1_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_1_block_0_block_1_weight_v1[v0, v1, v2] / T.sqrt(lv534[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt6_divide16_multiply16(lv541: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32"), encoder_block_1_block_0_block_3_weight_v1: T.Buffer((T.int64(64), T.int64(64), T.int64(1)), "float32"), encoder_block_1_block_0_block_3_weight_g1: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(64), T.int64(64), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(4), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(64), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(64))
                    v1 = T.axis.spatial(T.int64(64), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(64))
                    T.reads(encoder_block_1_block_0_block_3_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_1_block_0_block_3_weight_v1[v0, v1, T.int64(0)], lv541[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = encoder_block_1_block_0_block_3_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_1_block_0_block_3_weight_v1[v0, v1, T.int64(0)] / T.sqrt(lv541[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt7_divide17_multiply17(lv576: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32"), encoder_block_1_block_4_weight_v1: T.Buffer((T.int64(128), T.int64(64), T.int64(4)), "float32"), encoder_block_1_block_4_weight_g1: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(128), T.int64(64), T.int64(4)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(32), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(256))
                    v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(256) // T.int64(4))
                    v2 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(4))
                    T.reads(encoder_block_1_block_4_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_1_block_4_weight_v1[v0, v1, v2], lv576[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_1_block_4_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_1_block_4_weight_v1[v0, v1, v2] / T.sqrt(lv576[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt7_divide18_multiply18(lv583: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32"), encoder_block_2_block_0_block_1_weight_v1: T.Buffer((T.int64(128), T.int64(128), T.int64(7)), "float32"), encoder_block_2_block_0_block_1_weight_g1: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(128), T.int64(128), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(112), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(896))
                    v1 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(896) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(encoder_block_2_block_0_block_1_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_2_block_0_block_1_weight_v1[v0, v1, v2], lv583[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_2_block_0_block_1_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_2_block_0_block_1_weight_v1[v0, v1, v2] / T.sqrt(lv583[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt7_divide19_multiply19(lv590: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32"), encoder_block_2_block_0_block_3_weight_v1: T.Buffer((T.int64(128), T.int64(128), T.int64(1)), "float32"), encoder_block_2_block_0_block_3_weight_g1: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(128), T.int64(128), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(16), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(128), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(128))
                    v1 = T.axis.spatial(T.int64(128), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(128))
                    T.reads(encoder_block_2_block_0_block_3_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_2_block_0_block_3_weight_v1[v0, v1, T.int64(0)], lv590[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = encoder_block_2_block_0_block_3_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_2_block_0_block_3_weight_v1[v0, v1, T.int64(0)] / T.sqrt(lv590[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt8_divide20_multiply20(lv625: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32"), encoder_block_2_block_4_weight_v1: T.Buffer((T.int64(256), T.int64(128), T.int64(8)), "float32"), encoder_block_2_block_4_weight_g1: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(256), T.int64(128), T.int64(8)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(1024))
                    v1 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(1024) // T.int64(8))
                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(8))
                    T.reads(encoder_block_2_block_4_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_2_block_4_weight_v1[v0, v1, v2], lv625[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_2_block_4_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_2_block_4_weight_v1[v0, v1, v2] / T.sqrt(lv625[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt8_divide21_multiply21(lv632: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32"), encoder_block_3_block_0_block_1_weight_v1: T.Buffer((T.int64(256), T.int64(256), T.int64(7)), "float32"), encoder_block_3_block_0_block_1_weight_g1: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(256), T.int64(256), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(448), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(1792))
                    v1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(1792) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(encoder_block_3_block_0_block_1_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_3_block_0_block_1_weight_v1[v0, v1, v2], lv632[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_3_block_0_block_1_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_3_block_0_block_1_weight_v1[v0, v1, v2] / T.sqrt(lv632[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt8_divide22_multiply22(lv639: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32"), encoder_block_3_block_0_block_3_weight_v1: T.Buffer((T.int64(256), T.int64(256), T.int64(1)), "float32"), encoder_block_3_block_0_block_3_weight_g1: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(256), T.int64(256), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(256), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(256))
                    v1 = T.axis.spatial(T.int64(256), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(256))
                    T.reads(encoder_block_3_block_0_block_3_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_3_block_0_block_3_weight_v1[v0, v1, T.int64(0)], lv639[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = encoder_block_3_block_0_block_3_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_3_block_0_block_3_weight_v1[v0, v1, T.int64(0)] / T.sqrt(lv639[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt9_divide23_multiply23(lv674: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32"), encoder_block_3_block_4_weight_v1: T.Buffer((T.int64(512), T.int64(256), T.int64(16)), "float32"), encoder_block_3_block_4_weight_g1: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(512), T.int64(256), T.int64(16)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(2048), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(4096))
                    v1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(4096) // T.int64(16))
                    v2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(16))
                    T.reads(encoder_block_3_block_4_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_3_block_4_weight_v1[v0, v1, v2], lv674[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_3_block_4_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_3_block_4_weight_v1[v0, v1, v2] / T.sqrt(lv674[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt9_divide24_multiply24(lv681: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32"), encoder_block_4_block_0_block_1_weight_v1: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), "float32"), encoder_block_4_block_0_block_1_weight_g1: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(1792), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(3584))
                    v1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(3584) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(encoder_block_4_block_0_block_1_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_4_block_0_block_1_weight_v1[v0, v1, v2], lv681[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_4_block_0_block_1_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_4_block_0_block_1_weight_v1[v0, v1, v2] / T.sqrt(lv681[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt9_divide25_multiply25(lv688: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32"), encoder_block_4_block_0_block_3_weight_v1: T.Buffer((T.int64(512), T.int64(512), T.int64(1)), "float32"), encoder_block_4_block_0_block_3_weight_g1: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(512), T.int64(512), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(512), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(512))
                    v1 = T.axis.spatial(T.int64(512), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(512))
                    T.reads(encoder_block_4_block_0_block_3_weight_g1[v0, T.int64(0), T.int64(0)], encoder_block_4_block_0_block_3_weight_v1[v0, v1, T.int64(0)], lv688[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = encoder_block_4_block_0_block_3_weight_g1[v0, T.int64(0), T.int64(0)] * (encoder_block_4_block_0_block_3_weight_v1[v0, v1, T.int64(0)] / T.sqrt(lv688[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt_divide1_multiply1(lv851: T.Buffer((T.int64(1536), T.int64(1), T.int64(1)), "float32"), decoder_model_1_block_1_weight_v2: T.Buffer((T.int64(1536), T.int64(768), T.int64(16)), "float32"), decoder_model_1_block_1_weight_g2: T.Buffer((T.int64(1536), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1536), T.int64(768), T.int64(16)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(18432), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(1536), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(12288))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(12288) // T.int64(16))
                    v2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(16))
                    T.reads(decoder_model_1_block_1_weight_g2[v0, T.int64(0), T.int64(0)], decoder_model_1_block_1_weight_v2[v0, v1, v2], lv851[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_1_block_1_weight_g2[v0, T.int64(0), T.int64(0)] * (decoder_model_1_block_1_weight_v2[v0, v1, v2] / T.sqrt(lv851[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt_divide_multiply(lv844: T.Buffer((T.int64(1536), T.int64(1), T.int64(1)), "float32"), decoder_model_0_weight_v2: T.Buffer((T.int64(1536), T.int64(1024), T.int64(7)), "float32"), decoder_model_0_weight_g2: T.Buffer((T.int64(1536), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1536), T.int64(1024), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(10752), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(1536), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(7168))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7168) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(decoder_model_0_weight_g2[v0, T.int64(0), T.int64(0)], decoder_model_0_weight_v2[v0, v1, v2], lv844[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_0_weight_g2[v0, T.int64(0), T.int64(0)] * (decoder_model_0_weight_v2[v0, v1, v2] / T.sqrt(lv844[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_square10_sum10(decoder_model_4_block_1_weight_v2: T.Buffer((T.int64(192), T.int64(96), T.int64(4)), "float32"), lv997_red_intermediate: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv997_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(192), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(192), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv997_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv997_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv997_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv997_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(384))
                        T.reads(lv997_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_4_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(4), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(4)])
                        T.writes(lv997_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv997_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv997_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_4_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(4), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(4)] * decoder_model_4_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(4), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(4)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv997_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv997_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv997_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv997_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv997_red_intermediate[v0, T.int64(0), T.int64(0)] = lv997_red_intermediate[v0, T.int64(0), T.int64(0)] + lv997_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square11_sum11(decoder_model_4_block_2_block_1_weight_v2: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32"), lv1004_red_intermediate: T.Buffer((T.int64(96), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv1004_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(96), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(96), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv1004_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv1004_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv1004_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(3), 1):
                    with T.block("lv1004_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(672))
                        T.reads(lv1004_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_4_block_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv1004_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv1004_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv1004_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_4_block_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * decoder_model_4_block_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv1004_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv1004_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv1004_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv1004_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv1004_red_intermediate[v0, T.int64(0), T.int64(0)] = lv1004_red_intermediate[v0, T.int64(0), T.int64(0)] + lv1004_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square12_sum12(decoder_model_4_block_2_block_3_weight_v2: T.Buffer((T.int64(96), T.int64(96), T.int64(1)), "float32"), lv1011_red_intermediate: T.Buffer((T.int64(96), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv1011_red_intermediate_rf_local = T.alloc_buffer((T.int64(64), T.int64(96), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(96), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv1011_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv1011_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv1011_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv1011_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.where(ax1_fused_0 * T.int64(64) + ax1_fused_1 < T.int64(96))
                        T.reads(lv1011_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_4_block_2_block_3_weight_v2[v0, vax1_fused_0 * T.int64(64) + vax1_fused_1, T.int64(0)])
                        T.writes(lv1011_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv1011_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv1011_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_4_block_2_block_3_weight_v2[v0, vax1_fused_0 * T.int64(64) + vax1_fused_1, T.int64(0)] * decoder_model_4_block_2_block_3_weight_v2[v0, vax1_fused_0 * T.int64(64) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("lv1011_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv1011_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv1011_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv1011_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv1011_red_intermediate[v0, T.int64(0), T.int64(0)] = lv1011_red_intermediate[v0, T.int64(0), T.int64(0)] + lv1011_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square13_sum13(decoder_model_6_weight_v2: T.Buffer((T.int64(1), T.int64(96), T.int64(7)), "float32"), lv1046_red_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv1046_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(1), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv1046_red_rf_init"):
                    vax1_ax2_fused_1 = T.axis.spatial(T.int64(256), ax1_ax2_fused_1)
                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                    T.reads()
                    T.writes(lv1046_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)])
                    lv1046_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(3), 1):
                    with T.block("lv1046_red_rf_update"):
                        vax1_ax2_fused_1 = T.axis.spatial(T.int64(256), ax1_ax2_fused_1)
                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                        vax1_ax2_fused_0 = T.axis.reduce(T.int64(3), ax1_ax2_fused_0)
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(672))
                        T.reads(lv1046_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)], decoder_model_6_weight_v2[T.int64(0), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv1046_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)])
                        lv1046_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)] = lv1046_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)] + decoder_model_6_weight_v2[T.int64(0), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * decoder_model_6_weight_v2[T.int64(0), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv1046_red"):
                        vax1_ax2_fused_1 = T.axis.reduce(T.int64(256), ax0)
                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                        T.reads(lv1046_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)])
                        T.writes(lv1046_red_intermediate[T.int64(0), T.int64(0), T.int64(0)])
                        with T.init():
                            lv1046_red_intermediate[T.int64(0), T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv1046_red_intermediate[T.int64(0), T.int64(0), T.int64(0)] = lv1046_red_intermediate[T.int64(0), T.int64(0), T.int64(0)] + lv1046_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square14_sum14(encoder_block_0_weight_v1: T.Buffer((T.int64(64), T.int64(1), T.int64(7)), "float32"), lv526_red_intermediate: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv526_red_intermediate_rf_local = T.alloc_buffer((T.int64(4), T.int64(64), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv526_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv526_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv526_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv526_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.where(ax1_fused_0 * T.int64(4) + ax1_fused_1 < T.int64(7))
                        T.reads(lv526_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_0_weight_v1[v0, T.int64(0), vax1_fused_0 * T.int64(4) + vax1_fused_1])
                        T.writes(lv526_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv526_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv526_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_0_weight_v1[v0, T.int64(0), vax1_fused_0 * T.int64(4) + vax1_fused_1] * encoder_block_0_weight_v1[v0, T.int64(0), vax1_fused_0 * T.int64(4) + vax1_fused_1]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("lv526_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv526_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv526_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv526_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv526_red_intermediate[v0, T.int64(0), T.int64(0)] = lv526_red_intermediate[v0, T.int64(0), T.int64(0)] + lv526_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square15_sum15(encoder_block_1_block_0_block_1_weight_v1: T.Buffer((T.int64(64), T.int64(64), T.int64(7)), "float32"), lv533_red_intermediate: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv533_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(64), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv533_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv533_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv533_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv533_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(448))
                        T.reads(lv533_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_1_block_0_block_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv533_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv533_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv533_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_1_block_0_block_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * encoder_block_1_block_0_block_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv533_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv533_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv533_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv533_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv533_red_intermediate[v0, T.int64(0), T.int64(0)] = lv533_red_intermediate[v0, T.int64(0), T.int64(0)] + lv533_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square16_sum16(encoder_block_1_block_0_block_3_weight_v1: T.Buffer((T.int64(64), T.int64(64), T.int64(1)), "float32"), lv540_red_intermediate: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv540_red_intermediate_rf_local = T.alloc_buffer((T.int64(64), T.int64(64), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv540_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv540_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv540_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("lv540_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv540_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_1_block_0_block_3_weight_v1[v0, vax1_fused_0 * T.int64(64) + vax1_fused_1, T.int64(0)])
                        T.writes(lv540_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv540_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv540_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_1_block_0_block_3_weight_v1[v0, vax1_fused_0 * T.int64(64) + vax1_fused_1, T.int64(0)] * encoder_block_1_block_0_block_3_weight_v1[v0, vax1_fused_0 * T.int64(64) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("lv540_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv540_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv540_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv540_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv540_red_intermediate[v0, T.int64(0), T.int64(0)] = lv540_red_intermediate[v0, T.int64(0), T.int64(0)] + lv540_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square17_sum17(encoder_block_1_block_4_weight_v1: T.Buffer((T.int64(128), T.int64(64), T.int64(4)), "float32"), lv575_red_intermediate: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv575_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(128), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv575_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv575_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv575_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("lv575_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv575_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_1_block_4_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(4), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(4)])
                        T.writes(lv575_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv575_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv575_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_1_block_4_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(4), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(4)] * encoder_block_1_block_4_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(4), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(4)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv575_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv575_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv575_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv575_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv575_red_intermediate[v0, T.int64(0), T.int64(0)] = lv575_red_intermediate[v0, T.int64(0), T.int64(0)] + lv575_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square18_sum18(encoder_block_2_block_0_block_1_weight_v1: T.Buffer((T.int64(128), T.int64(128), T.int64(7)), "float32"), lv582_red_intermediate: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv582_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(128), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv582_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv582_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv582_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(4), 1):
                    with T.block("lv582_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(896))
                        T.reads(lv582_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_2_block_0_block_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv582_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv582_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv582_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_2_block_0_block_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * encoder_block_2_block_0_block_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv582_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv582_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv582_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv582_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv582_red_intermediate[v0, T.int64(0), T.int64(0)] = lv582_red_intermediate[v0, T.int64(0), T.int64(0)] + lv582_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square19_sum19(encoder_block_2_block_0_block_3_weight_v1: T.Buffer((T.int64(128), T.int64(128), T.int64(1)), "float32"), lv589_red_intermediate: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv589_red_intermediate_rf_local = T.alloc_buffer((T.int64(128), T.int64(128), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv589_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv589_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv589_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("lv589_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv589_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_2_block_0_block_3_weight_v1[v0, vax1_fused_0 * T.int64(128) + vax1_fused_1, T.int64(0)])
                        T.writes(lv589_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv589_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv589_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_2_block_0_block_3_weight_v1[v0, vax1_fused_0 * T.int64(128) + vax1_fused_1, T.int64(0)] * encoder_block_2_block_0_block_3_weight_v1[v0, vax1_fused_0 * T.int64(128) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("lv589_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv589_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv589_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv589_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv589_red_intermediate[v0, T.int64(0), T.int64(0)] = lv589_red_intermediate[v0, T.int64(0), T.int64(0)] + lv589_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square1_sum1(decoder_model_1_block_1_weight_v2: T.Buffer((T.int64(1536), T.int64(768), T.int64(16)), "float32"), lv850_red_intermediate: T.Buffer((T.int64(1536), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv850_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(1536), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1536), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv850_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv850_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv850_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(48), 1):
                    with T.block("lv850_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv850_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_1_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)])
                        T.writes(lv850_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv850_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv850_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_1_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)] * decoder_model_1_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv850_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv850_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv850_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv850_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv850_red_intermediate[v0, T.int64(0), T.int64(0)] = lv850_red_intermediate[v0, T.int64(0), T.int64(0)] + lv850_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square20_sum20(encoder_block_2_block_4_weight_v1: T.Buffer((T.int64(256), T.int64(128), T.int64(8)), "float32"), lv624_red_intermediate: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv624_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv624_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv624_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv624_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(4), 1):
                    with T.block("lv624_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv624_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_2_block_4_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(8), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(8)])
                        T.writes(lv624_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv624_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv624_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_2_block_4_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(8), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(8)] * encoder_block_2_block_4_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(8), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(8)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv624_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv624_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv624_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv624_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv624_red_intermediate[v0, T.int64(0), T.int64(0)] = lv624_red_intermediate[v0, T.int64(0), T.int64(0)] + lv624_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square21_sum21(encoder_block_3_block_0_block_1_weight_v1: T.Buffer((T.int64(256), T.int64(256), T.int64(7)), "float32"), lv631_red_intermediate: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv631_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv631_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv631_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv631_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(7), 1):
                    with T.block("lv631_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv631_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_3_block_0_block_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv631_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv631_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv631_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_3_block_0_block_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * encoder_block_3_block_0_block_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv631_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv631_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv631_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv631_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv631_red_intermediate[v0, T.int64(0), T.int64(0)] = lv631_red_intermediate[v0, T.int64(0), T.int64(0)] + lv631_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square22_sum22(encoder_block_3_block_0_block_3_weight_v1: T.Buffer((T.int64(256), T.int64(256), T.int64(1)), "float32"), lv638_red_intermediate: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv638_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv638_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv638_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv638_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("lv638_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv638_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_3_block_0_block_3_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)])
                        T.writes(lv638_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv638_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv638_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_3_block_0_block_3_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)] * encoder_block_3_block_0_block_3_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv638_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv638_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv638_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv638_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv638_red_intermediate[v0, T.int64(0), T.int64(0)] = lv638_red_intermediate[v0, T.int64(0), T.int64(0)] + lv638_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square23_sum23(encoder_block_3_block_4_weight_v1: T.Buffer((T.int64(512), T.int64(256), T.int64(16)), "float32"), lv673_red_intermediate: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv673_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(512), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv673_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv673_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv673_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(16), 1):
                    with T.block("lv673_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv673_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_3_block_4_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)])
                        T.writes(lv673_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv673_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv673_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_3_block_4_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)] * encoder_block_3_block_4_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv673_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv673_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv673_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv673_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv673_red_intermediate[v0, T.int64(0), T.int64(0)] = lv673_red_intermediate[v0, T.int64(0), T.int64(0)] + lv673_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square24_sum24(encoder_block_4_block_0_block_1_weight_v1: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), "float32"), lv680_red_intermediate: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv680_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(512), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv680_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv680_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv680_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(14), 1):
                    with T.block("lv680_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv680_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_4_block_0_block_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv680_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv680_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv680_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_4_block_0_block_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * encoder_block_4_block_0_block_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv680_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv680_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv680_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv680_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv680_red_intermediate[v0, T.int64(0), T.int64(0)] = lv680_red_intermediate[v0, T.int64(0), T.int64(0)] + lv680_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square25_sum25(encoder_block_4_block_0_block_3_weight_v1: T.Buffer((T.int64(512), T.int64(512), T.int64(1)), "float32"), lv687_red_intermediate: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv687_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(512), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv687_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv687_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv687_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv687_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv687_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_4_block_0_block_3_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)])
                        T.writes(lv687_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv687_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv687_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_4_block_0_block_3_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)] * encoder_block_4_block_0_block_3_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv687_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv687_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv687_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv687_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv687_red_intermediate[v0, T.int64(0), T.int64(0)] = lv687_red_intermediate[v0, T.int64(0), T.int64(0)] + lv687_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square26_sum26(encoder_block_4_block_4_weight_v1: T.Buffer((T.int64(1024), T.int64(512), T.int64(16)), "float32"), lv722_red_intermediate: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv722_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv722_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv722_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv722_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(32), 1):
                    with T.block("lv722_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv722_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_4_block_4_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)])
                        T.writes(lv722_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv722_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv722_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_4_block_4_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)] * encoder_block_4_block_4_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv722_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv722_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv722_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv722_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv722_red_intermediate[v0, T.int64(0), T.int64(0)] = lv722_red_intermediate[v0, T.int64(0), T.int64(0)] + lv722_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square27_sum27(encoder_block_6_weight_v1: T.Buffer((T.int64(1024), T.int64(1024), T.int64(3)), "float32"), lv729_red_intermediate: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv729_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv729_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv729_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv729_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(12), 1):
                    with T.block("lv729_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv729_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_6_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(3), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(3)])
                        T.writes(lv729_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv729_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv729_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_6_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(3), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(3)] * encoder_block_6_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(3), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(3)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv729_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv729_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv729_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv729_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv729_red_intermediate[v0, T.int64(0), T.int64(0)] = lv729_red_intermediate[v0, T.int64(0), T.int64(0)] + lv729_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square28_sum28(quantizer_quantizers_0_in_proj_weight_v1: T.Buffer((T.int64(8), T.int64(1024), T.int64(1)), "float32"), lv735_red_intermediate: T.Buffer((T.int64(8), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv735_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(8), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv735_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv735_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv735_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(4), 1):
                    with T.block("lv735_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv735_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], quantizer_quantizers_0_in_proj_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)])
                        T.writes(lv735_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv735_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv735_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + quantizer_quantizers_0_in_proj_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)] * quantizer_quantizers_0_in_proj_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv735_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv735_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv735_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv735_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv735_red_intermediate[v0, T.int64(0), T.int64(0)] = lv735_red_intermediate[v0, T.int64(0), T.int64(0)] + lv735_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square29_sum29(p_reshape210: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        reshape210 = T.match_buffer(p_reshape210, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        square36_red_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        # with T.block("root"):
        square36_red_intermediate_rf_local = T.alloc_buffer((T.int64(8), batch_size * (seq_len // T.int64(512)), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("square36_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                    square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("square36_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)], reshape210[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1])
                        T.writes(square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] + reshape210[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1] * reshape210[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("square36_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        T.writes(square36_red_intermediate[v0, T.int64(0)])
                        with T.init():
                            square36_red_intermediate[v0, T.int64(0)] = T.float32(0.0)
                        square36_red_intermediate[v0, T.int64(0)] = square36_red_intermediate[v0, T.int64(0)] + square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square29_sum32(p_divide26: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        divide26 = T.match_buffer(p_divide26, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        square40_red_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        # with T.block("root"):
        square40_red_intermediate_rf_local = T.alloc_buffer((T.int64(8), batch_size * (seq_len // T.int64(512)), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("square40_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                    square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("square40_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)], divide26[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1])
                        T.writes(square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] + divide26[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1] * divide26[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("square40_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        T.writes(square40_red_intermediate[v0, T.int64(0)])
                        with T.init():
                            square40_red_intermediate[v0, T.int64(0)] = T.float32(0.0)
                        square40_red_intermediate[v0, T.int64(0)] = square40_red_intermediate[v0, T.int64(0)] + square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square2_sum2(decoder_model_1_block_2_block_1_weight_v2: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32"), lv857_red_intermediate: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv857_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(768), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(768), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv857_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv857_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv857_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(21), 1):
                    with T.block("lv857_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv857_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_1_block_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv857_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv857_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv857_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_1_block_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * decoder_model_1_block_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv857_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv857_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv857_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv857_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv857_red_intermediate[v0, T.int64(0), T.int64(0)] = lv857_red_intermediate[v0, T.int64(0), T.int64(0)] + lv857_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square30_sum30(quantizer_quantizers_0_codebook_weight1: T.Buffer((T.int64(1024), T.int64(8)), "float32"), square37_red_intermediate: T.Buffer((T.int64(1024), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        square37_red_intermediate_rf_local = T.alloc_buffer((T.int64(8), T.int64(1024), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("square37_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(square37_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                    square37_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("square37_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(square37_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)], quantizer_quantizers_0_codebook_weight1[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1])
                        T.writes(square37_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        square37_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = square37_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] + quantizer_quantizers_0_codebook_weight1[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1] * quantizer_quantizers_0_codebook_weight1[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("square37_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(square37_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        T.writes(square37_red_intermediate[v0, T.int64(0)])
                        with T.init():
                            square37_red_intermediate[v0, T.int64(0)] = T.float32(0.0)
                        square37_red_intermediate[v0, T.int64(0)] = square37_red_intermediate[v0, T.int64(0)] + square37_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square31_sum31(quantizer_quantizers_0_out_proj_weight_v1: T.Buffer((T.int64(1024), T.int64(8), T.int64(1)), "float32"), lv741_red_intermediate: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv741_red_intermediate_rf_local = T.alloc_buffer((T.int64(8), T.int64(1024), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv741_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv741_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv741_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("lv741_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv741_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], quantizer_quantizers_0_out_proj_weight_v1[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1, T.int64(0)])
                        T.writes(lv741_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv741_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv741_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + quantizer_quantizers_0_out_proj_weight_v1[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1, T.int64(0)] * quantizer_quantizers_0_out_proj_weight_v1[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("lv741_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv741_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv741_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv741_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv741_red_intermediate[v0, T.int64(0), T.int64(0)] = lv741_red_intermediate[v0, T.int64(0), T.int64(0)] + lv741_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square32_sum29(p_divide20: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        divide20 = T.match_buffer(p_divide20, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        square36_red_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        # with T.block("root"):
        square36_red_intermediate_rf_local = T.alloc_buffer((T.int64(8), batch_size * (seq_len // T.int64(512)), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("square36_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                    square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("square36_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)], divide20[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1])
                        T.writes(square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] + divide20[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1] * divide20[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("square36_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        T.writes(square36_red_intermediate[v0, T.int64(0)])
                        with T.init():
                            square36_red_intermediate[v0, T.int64(0)] = T.float32(0.0)
                        square36_red_intermediate[v0, T.int64(0)] = square36_red_intermediate[v0, T.int64(0)] + square36_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square32_sum32(p_reshape214: T.handle, p_output0: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        reshape214 = T.match_buffer(p_reshape214, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        square40_red_intermediate = T.match_buffer(p_output0, (batch_size * (seq_len // T.int64(512)), T.int64(1)))
        # with T.block("root"):
        square40_red_intermediate_rf_local = T.alloc_buffer((T.int64(8), batch_size * (seq_len // T.int64(512)), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("square40_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                    square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("square40_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)], reshape214[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1])
                        T.writes(square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] + reshape214[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1] * reshape214[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("square40_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        T.writes(square40_red_intermediate[v0, T.int64(0)])
                        with T.init():
                            square40_red_intermediate[v0, T.int64(0)] = T.float32(0.0)
                        square40_red_intermediate[v0, T.int64(0)] = square40_red_intermediate[v0, T.int64(0)] + square40_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square3_sum3(decoder_model_1_block_2_block_3_weight_v2: T.Buffer((T.int64(768), T.int64(768), T.int64(1)), "float32"), lv864_red_intermediate: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv864_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(768), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(768), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv864_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv864_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv864_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(3), 1):
                    with T.block("lv864_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv864_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_1_block_2_block_3_weight_v2[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)])
                        T.writes(lv864_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv864_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv864_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_1_block_2_block_3_weight_v2[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)] * decoder_model_1_block_2_block_3_weight_v2[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv864_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv864_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv864_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv864_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv864_red_intermediate[v0, T.int64(0), T.int64(0)] = lv864_red_intermediate[v0, T.int64(0), T.int64(0)] + lv864_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square4_sum4(decoder_model_2_block_1_weight_v2: T.Buffer((T.int64(768), T.int64(384), T.int64(16)), "float32"), lv899_red_intermediate: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv899_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(768), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(768), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv899_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv899_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv899_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(24), 1):
                    with T.block("lv899_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv899_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)])
                        T.writes(lv899_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv899_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv899_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)] * decoder_model_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv899_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv899_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv899_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv899_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv899_red_intermediate[v0, T.int64(0), T.int64(0)] = lv899_red_intermediate[v0, T.int64(0), T.int64(0)] + lv899_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square5_sum5(decoder_model_2_block_2_block_1_weight_v2: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32"), lv906_red_intermediate: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv906_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(384), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(384), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv906_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv906_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv906_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(11), 1):
                    with T.block("lv906_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(2688))
                        T.reads(lv906_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_2_block_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv906_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv906_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv906_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_2_block_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * decoder_model_2_block_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv906_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv906_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv906_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv906_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv906_red_intermediate[v0, T.int64(0), T.int64(0)] = lv906_red_intermediate[v0, T.int64(0), T.int64(0)] + lv906_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square6_sum6(decoder_model_2_block_2_block_3_weight_v2: T.Buffer((T.int64(384), T.int64(384), T.int64(1)), "float32"), lv913_red_intermediate: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv913_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(384), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(384), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv913_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv913_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv913_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv913_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.where(ax1_fused_0 * T.int64(256) + ax1_fused_1 < T.int64(384))
                        T.reads(lv913_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_2_block_2_block_3_weight_v2[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)])
                        T.writes(lv913_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv913_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv913_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_2_block_2_block_3_weight_v2[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)] * decoder_model_2_block_2_block_3_weight_v2[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv913_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv913_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv913_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv913_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv913_red_intermediate[v0, T.int64(0), T.int64(0)] = lv913_red_intermediate[v0, T.int64(0), T.int64(0)] + lv913_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square7_sum7(decoder_model_3_block_1_weight_v2: T.Buffer((T.int64(384), T.int64(192), T.int64(8)), "float32"), lv948_red_intermediate: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv948_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(384), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(384), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv948_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv948_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv948_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(6), 1):
                    with T.block("lv948_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv948_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_3_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(8), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(8)])
                        T.writes(lv948_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv948_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv948_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_3_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(8), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(8)] * decoder_model_3_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(8), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(8)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv948_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv948_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv948_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv948_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv948_red_intermediate[v0, T.int64(0), T.int64(0)] = lv948_red_intermediate[v0, T.int64(0), T.int64(0)] + lv948_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square8_sum8(decoder_model_3_block_2_block_1_weight_v2: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32"), lv955_red_intermediate: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv955_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(192), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(192), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv955_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv955_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv955_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(6), 1):
                    with T.block("lv955_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(1344))
                        T.reads(lv955_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_3_block_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv955_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv955_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv955_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_3_block_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * decoder_model_3_block_2_block_1_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv955_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv955_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv955_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv955_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv955_red_intermediate[v0, T.int64(0), T.int64(0)] = lv955_red_intermediate[v0, T.int64(0), T.int64(0)] + lv955_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square9_sum9(decoder_model_3_block_2_block_3_weight_v2: T.Buffer((T.int64(192), T.int64(192), T.int64(1)), "float32"), lv962_red_intermediate: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv962_red_intermediate_rf_local = T.alloc_buffer((T.int64(128), T.int64(192), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(192), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv962_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv962_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv962_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv962_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.where(ax1_fused_0 * T.int64(128) + ax1_fused_1 < T.int64(192))
                        T.reads(lv962_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_3_block_2_block_3_weight_v2[v0, vax1_fused_0 * T.int64(128) + vax1_fused_1, T.int64(0)])
                        T.writes(lv962_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv962_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv962_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_3_block_2_block_3_weight_v2[v0, vax1_fused_0 * T.int64(128) + vax1_fused_1, T.int64(0)] * decoder_model_3_block_2_block_3_weight_v2[v0, vax1_fused_0 * T.int64(128) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("lv962_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv962_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv962_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv962_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv962_red_intermediate[v0, T.int64(0), T.int64(0)] = lv962_red_intermediate[v0, T.int64(0), T.int64(0)] + lv962_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square_sum(decoder_model_0_weight_v2: T.Buffer((T.int64(1536), T.int64(1024), T.int64(7)), "float32"), lv843_red_intermediate: T.Buffer((T.int64(1536), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv843_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(1536), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1536), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv843_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv843_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv843_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(28), 1):
                    with T.block("lv843_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv843_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_0_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv843_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv843_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv843_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_0_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * decoder_model_0_weight_v2[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv843_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv843_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv843_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv843_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv843_red_intermediate[v0, T.int64(0), T.int64(0)] = lv843_red_intermediate[v0, T.int64(0), T.int64(0)] + lv843_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_zeros_add21_add21_add21_add21_add21_add21_add21_add21_conv1d41_add18_add21(p_conv1d105: T.handle, p_conv1d107: T.handle, p_conv1d109: T.handle, p_conv1d111: T.handle, p_conv1d113: T.handle, p_conv1d115: T.handle, p_conv1d117: T.handle, p_conv1d119: T.handle, p_permute_dims71: T.handle, wnconv1d121: T.Buffer((T.int64(1024), T.int64(8), T.int64(1)), "float32"), lv842: T.Buffer((T.int64(1), T.int64(1024), T.int64(1)), "float32"), p_output0: T.handle, seq_len: T.int64):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d105 = T.match_buffer(p_conv1d105, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        conv1d107 = T.match_buffer(p_conv1d107, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        conv1d109 = T.match_buffer(p_conv1d109, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        conv1d111 = T.match_buffer(p_conv1d111, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        conv1d113 = T.match_buffer(p_conv1d113, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        conv1d115 = T.match_buffer(p_conv1d115, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        conv1d117 = T.match_buffer(p_conv1d117, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        conv1d119 = T.match_buffer(p_conv1d119, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        permute_dims71 = T.match_buffer(p_permute_dims71, (batch_size, T.int64(8), seq_len // T.int64(512)))
        T_add_intermediate_1_2_3_4_5_6_7_8_9 = T.match_buffer(p_output0, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_shared = T.alloc_buffer((batch_size, T.int64(1024), seq_len // T.int64(512)), scope="shared")
        for ax0_ax1_ax2_fused in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(1024), thread="blockIdx.x"):
            for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(1)):
                for ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for ax3_fused_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                        with T.block("conv1d_ncw"):
                            v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024) * batch_size) // (seq_len // T.int64(512) * T.int64(1024)) + ax0)
                            v1 = T.axis.spatial(T.int64(1024), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)) + ax1)
                            v2 = T.axis.spatial(seq_len // T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512)) + ax2)
                            v3 = T.axis.reduce(T.int64(8), ax3_fused_0 * T.int64(64) + ax3_fused_1)
                            T.where(T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(1024)) // (seq_len // T.int64(512) * T.int64(1024)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * batch_size * T.int64(1024)) // (seq_len // T.int64(512) * T.int64(1024)) < batch_size and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)) < T.int64(1024) and T.int64(0) <= ax0_ax1_ax2_fused % (seq_len // T.int64(512)) and ax0_ax1_ax2_fused % (seq_len // T.int64(512)) < seq_len // T.int64(512) and ax3_fused_0 * T.int64(64) + ax3_fused_1 < T.int64(8))
                            T.reads(permute_dims71[v0, v3, v2], wnconv1d121[v1, v3, T.int64(0)])
                            T.writes(conv1d_ncw_intermediate_shared[v0, v1, v2])
                            with T.init():
                                conv1d_ncw_intermediate_shared[v0, v1, v2] = T.float32(0.0)
                            conv1d_ncw_intermediate_shared[v0, v1, v2] = conv1d_ncw_intermediate_shared[v0, v1, v2] + permute_dims71[v0, v3, v2] * wnconv1d121[v1, v3, T.int64(0)]
            for ax3_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                for ax3_0 in T.serial(T.int64(1), annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
                    with T.block("T_add_9"):
                        v0 = T.axis.spatial(batch_size, ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024) * batch_size) // (seq_len // T.int64(512) * T.int64(1024)))
                        v1 = T.axis.spatial(T.int64(1024), ax0_ax1_ax2_fused % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)))
                        v2 = T.axis.spatial(seq_len // T.int64(512), ax0_ax1_ax2_fused % (seq_len // T.int64(512)))
                        v3 = T.axis.spatial(T.int64(1), ax3_0 * T.int64(64) + ax3_1)
                        T.where(ax3_0 * T.int64(64) + ax3_1 < T.int64(1))
                        T.reads(conv1d105[v0, v1, v2], conv1d107[v0, v1, v2], conv1d109[v0, v1, v2], conv1d111[v0, v1, v2], conv1d113[v0, v1, v2], conv1d115[v0, v1, v2], conv1d117[v0, v1, v2], conv1d119[v0, v1, v2], conv1d_ncw_intermediate_shared[v0, v1, v2], lv842[T.int64(0), v1, T.int64(0)])
                        T.writes(T_add_intermediate_1_2_3_4_5_6_7_8_9[v0, v1, v2])
                        T_add_intermediate_1_2_3_4_5_6_7_8_9[v0, v1, v2] = T.Add(T.float32(0.0), conv1d105[v0, v1, v2]) + conv1d107[v0, v1, v2] + conv1d109[v0, v1, v2] + conv1d111[v0, v1, v2] + conv1d113[v0, v1, v2] + conv1d115[v0, v1, v2] + conv1d117[v0, v1, v2] + conv1d119[v0, v1, v2] + (conv1d_ncw_intermediate_shared[v0, v1, v2] + lv842[T.int64(0), v1, T.int64(0)])

    @T.prim_func(private=True)
    def reshape(decoder_model_0_bias2: T.Buffer((T.int64(1536),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(1536), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(1536), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(1536))
                    T.reads(decoder_model_0_bias2[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = decoder_model_0_bias2[v0]

    @T.prim_func(private=True)
    def reshape1(var_conv1d122: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, seq_len = T.int64(), T.int64()
        conv1d122 = T.match_buffer(var_conv1d122, (batch_size, T.int64(1536), seq_len))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(1536), seq_len))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size * seq_len * T.int64(1536) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(1536) * batch_size) // (seq_len * T.int64(1536)))
                    v1 = T.axis.spatial(T.int64(1536), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(1536)) // seq_len)
                    v2 = T.axis.spatial(seq_len, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % seq_len)
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size * seq_len * T.int64(1536))
                    T.reads(conv1d122[(v0 * (seq_len * T.int64(1536)) + v1 * seq_len + v2) % (seq_len * T.int64(1536) * batch_size) // (seq_len * T.int64(1536)), (v0 * (seq_len * T.int64(1536)) + v1 * seq_len + v2) % (seq_len * T.int64(1536)) // seq_len, (v0 * (seq_len * T.int64(1536)) + v1 * seq_len + v2) % seq_len])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d122[(v0 * (seq_len * T.int64(1536)) + v1 * seq_len + v2) % (seq_len * T.int64(1536) * batch_size) // (seq_len * T.int64(1536)), (v0 * (seq_len * T.int64(1536)) + v1 * seq_len + v2) % (seq_len * T.int64(1536)) // seq_len, (v0 * (seq_len * T.int64(1536)) + v1 * seq_len + v2) % seq_len]

    @T.prim_func(private=True)
    def reshape11(encoder_block_0_bias1: T.Buffer((T.int64(64),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(64), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(64))
                    T.reads(encoder_block_0_bias1[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = encoder_block_0_bias1[v0]

    @T.prim_func(private=True)
    def reshape12(var_conv1d74: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, seq_len = T.int64(), T.int64()
        conv1d74 = T.match_buffer(var_conv1d74, (batch_size, T.int64(64), seq_len))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(64), seq_len))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size * seq_len * T.int64(64) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(64) * batch_size) // (seq_len * T.int64(64)))
                    v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(64)) // seq_len)
                    v2 = T.axis.spatial(seq_len, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % seq_len)
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size * seq_len * T.int64(64))
                    T.reads(conv1d74[(v0 * (seq_len * T.int64(64)) + v1 * seq_len + v2) % (seq_len * T.int64(64) * batch_size) // (seq_len * T.int64(64)), (v0 * (seq_len * T.int64(64)) + v1 * seq_len + v2) % (seq_len * T.int64(64)) // seq_len, (v0 * (seq_len * T.int64(64)) + v1 * seq_len + v2) % seq_len])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d74[(v0 * (seq_len * T.int64(64)) + v1 * seq_len + v2) % (seq_len * T.int64(64) * batch_size) // (seq_len * T.int64(64)), (v0 * (seq_len * T.int64(64)) + v1 * seq_len + v2) % (seq_len * T.int64(64)) // seq_len, (v0 * (seq_len * T.int64(64)) + v1 * seq_len + v2) % seq_len]

    @T.prim_func(private=True)
    def reshape13(encoder_block_1_block_4_bias1: T.Buffer((T.int64(128),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(128), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(128))
                    T.reads(encoder_block_1_block_4_bias1[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = encoder_block_1_block_4_bias1[v0]

    @T.prim_func(private=True)
    def reshape14(var_conv1d81: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d81 = T.match_buffer(var_conv1d81, (batch_size, T.int64(128), seq_len // T.int64(2)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(128), seq_len // T.int64(2)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(2)) * T.int64(128) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(2) * T.int64(128) * batch_size) // (seq_len // T.int64(2) * T.int64(128)))
                    v1 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)))
                    v2 = T.axis.spatial(seq_len // T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(2)))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size * (seq_len // T.int64(2)) * T.int64(128))
                    T.reads(conv1d81[(v0 * (seq_len // T.int64(2) * T.int64(128)) + v1 * (seq_len // T.int64(2)) + v2) % (seq_len // T.int64(2) * T.int64(128) * batch_size) // (seq_len // T.int64(2) * T.int64(128)), (v0 * (seq_len // T.int64(2) * T.int64(128)) + v1 * (seq_len // T.int64(2)) + v2) % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)), (v0 * (seq_len // T.int64(2) * T.int64(128)) + v1 * (seq_len // T.int64(2)) + v2) % (seq_len // T.int64(2))])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d81[(v0 * (seq_len // T.int64(2) * T.int64(128)) + v1 * (seq_len // T.int64(2)) + v2) % (seq_len // T.int64(2) * T.int64(128) * batch_size) // (seq_len // T.int64(2) * T.int64(128)), (v0 * (seq_len // T.int64(2) * T.int64(128)) + v1 * (seq_len // T.int64(2)) + v2) % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)), (v0 * (seq_len // T.int64(2) * T.int64(128)) + v1 * (seq_len // T.int64(2)) + v2) % (seq_len // T.int64(2))]

    @T.prim_func(private=True)
    def reshape15(encoder_block_2_block_4_bias1: T.Buffer((T.int64(256),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(256), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(256))
                    T.reads(encoder_block_2_block_4_bias1[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = encoder_block_2_block_4_bias1[v0]

    @T.prim_func(private=True)
    def reshape16(var_conv1d88: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d88 = T.match_buffer(var_conv1d88, (batch_size, T.int64(256), seq_len // T.int64(8)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(256), seq_len // T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(8)) * T.int64(256) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(8) * T.int64(256) * batch_size) // (seq_len // T.int64(8) * T.int64(256)))
                    v1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)))
                    v2 = T.axis.spatial(seq_len // T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(8)))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size * (seq_len // T.int64(8)) * T.int64(256))
                    T.reads(conv1d88[(v0 * (seq_len // T.int64(8) * T.int64(256)) + v1 * (seq_len // T.int64(8)) + v2) % (seq_len // T.int64(8) * T.int64(256) * batch_size) // (seq_len // T.int64(8) * T.int64(256)), (v0 * (seq_len // T.int64(8) * T.int64(256)) + v1 * (seq_len // T.int64(8)) + v2) % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)), (v0 * (seq_len // T.int64(8) * T.int64(256)) + v1 * (seq_len // T.int64(8)) + v2) % (seq_len // T.int64(8))])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d88[(v0 * (seq_len // T.int64(8) * T.int64(256)) + v1 * (seq_len // T.int64(8)) + v2) % (seq_len // T.int64(8) * T.int64(256) * batch_size) // (seq_len // T.int64(8) * T.int64(256)), (v0 * (seq_len // T.int64(8) * T.int64(256)) + v1 * (seq_len // T.int64(8)) + v2) % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)), (v0 * (seq_len // T.int64(8) * T.int64(256)) + v1 * (seq_len // T.int64(8)) + v2) % (seq_len // T.int64(8))]

    @T.prim_func(private=True)
    def reshape17(encoder_block_3_block_4_bias1: T.Buffer((T.int64(512),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(512), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(512))
                    T.reads(encoder_block_3_block_4_bias1[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = encoder_block_3_block_4_bias1[v0]

    @T.prim_func(private=True)
    def reshape18(var_conv1d95: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d95 = T.match_buffer(var_conv1d95, (batch_size, T.int64(512), seq_len // T.int64(64)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(512), seq_len // T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(64)) * T.int64(512) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(64) * T.int64(512) * batch_size) // (seq_len // T.int64(64) * T.int64(512)))
                    v1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)))
                    v2 = T.axis.spatial(seq_len // T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(64)))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size * (seq_len // T.int64(64)) * T.int64(512))
                    T.reads(conv1d95[(v0 * (seq_len // T.int64(64) * T.int64(512)) + v1 * (seq_len // T.int64(64)) + v2) % (seq_len // T.int64(64) * T.int64(512) * batch_size) // (seq_len // T.int64(64) * T.int64(512)), (v0 * (seq_len // T.int64(64) * T.int64(512)) + v1 * (seq_len // T.int64(64)) + v2) % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)), (v0 * (seq_len // T.int64(64) * T.int64(512)) + v1 * (seq_len // T.int64(64)) + v2) % (seq_len // T.int64(64))])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d95[(v0 * (seq_len // T.int64(64) * T.int64(512)) + v1 * (seq_len // T.int64(64)) + v2) % (seq_len // T.int64(64) * T.int64(512) * batch_size) // (seq_len // T.int64(64) * T.int64(512)), (v0 * (seq_len // T.int64(64) * T.int64(512)) + v1 * (seq_len // T.int64(64)) + v2) % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)), (v0 * (seq_len // T.int64(64) * T.int64(512)) + v1 * (seq_len // T.int64(64)) + v2) % (seq_len // T.int64(64))]

    @T.prim_func(private=True)
    def reshape19(encoder_block_4_block_4_bias1: T.Buffer((T.int64(1024),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(1024), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(1024), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.reads(encoder_block_4_block_4_bias1[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = encoder_block_4_block_4_bias1[v0]

    @T.prim_func(private=True)
    def reshape2(decoder_model_1_block_1_bias2: T.Buffer((T.int64(768),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(768), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(768))
                    T.reads(decoder_model_1_block_1_bias2[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = decoder_model_1_block_1_bias2[v0]

    @T.prim_func(private=True)
    def reshape20(var_conv1d102: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d102 = T.match_buffer(var_conv1d102, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * (seq_len // T.int64(512)), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(1024) * batch_size) // (seq_len // T.int64(512) * T.int64(1024)))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)))
                    v2 = T.axis.spatial(seq_len // T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512)))
                    T.reads(conv1d102[(v0 * (seq_len // T.int64(512) * T.int64(1024)) + v1 * (seq_len // T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(1024) * batch_size) // (seq_len // T.int64(512) * T.int64(1024)), (v0 * (seq_len // T.int64(512) * T.int64(1024)) + v1 * (seq_len // T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)), (v0 * (seq_len // T.int64(512) * T.int64(1024)) + v1 * (seq_len // T.int64(512)) + v2) % (seq_len // T.int64(512))])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d102[(v0 * (seq_len // T.int64(512) * T.int64(1024)) + v1 * (seq_len // T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(1024) * batch_size) // (seq_len // T.int64(512) * T.int64(1024)), (v0 * (seq_len // T.int64(512) * T.int64(1024)) + v1 * (seq_len // T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)), (v0 * (seq_len // T.int64(512) * T.int64(1024)) + v1 * (seq_len // T.int64(512)) + v2) % (seq_len // T.int64(512))]

    @T.prim_func(private=True)
    def reshape21(quantizer_quantizers_0_in_proj_bias1: T.Buffer((T.int64(8),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(8), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(8), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(8))
                    T.reads(quantizer_quantizers_0_in_proj_bias1[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = quantizer_quantizers_0_in_proj_bias1[v0]

    @T.prim_func(private=True)
    def reshape22(var_permute_dims36: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        permute_dims36 = T.match_buffer(var_permute_dims36, (batch_size, seq_len // T.int64(512), T.int64(8)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(permute_dims36[v0 % (seq_len // T.int64(512) * batch_size) // (seq_len // T.int64(512)), v0 % (seq_len // T.int64(512)), v1])
                    T.writes(T_reshape[v0, v1])
                    T_reshape[v0, v1] = permute_dims36[v0 % (seq_len // T.int64(512) * batch_size) // (seq_len // T.int64(512)), v0 % (seq_len // T.int64(512)), v1]

    @T.prim_func(private=True)
    def reshape23(var_take18: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        take18 = T.match_buffer(var_take18, (batch_size * (seq_len // T.int64(512)), T.int64(1)), "int32")
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, seq_len // T.int64(512)), "int32")
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % (seq_len // T.int64(512) * batch_size) // (seq_len // T.int64(512)))
                    v1 = T.axis.spatial(seq_len // T.int64(512), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % (seq_len // T.int64(512)))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)))
                    T.reads(take18[v0 * (seq_len // T.int64(512)) + v1, T.int64(0)])
                    T.writes(T_reshape[v0, v1])
                    T_reshape[v0, v1] = take18[v0 * (seq_len // T.int64(512)) + v1, T.int64(0)]

    @T.prim_func(private=True)
    def reshape24(var_reshape211: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape211 = T.match_buffer(var_reshape211, (batch_size, seq_len // T.int64(512)), "int32")
        T_reshape = T.match_buffer(var_T_reshape, (batch_size * (seq_len // T.int64(512)),), "int32")
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < batch_size * (seq_len // T.int64(512)))
                    T.reads(reshape211[v0 % (seq_len // T.int64(512) * batch_size) // (seq_len // T.int64(512)), v0 % (seq_len // T.int64(512))])
                    T.writes(T_reshape[v0])
                    T_reshape[v0] = reshape211[v0 % (seq_len // T.int64(512) * batch_size) // (seq_len // T.int64(512)), v0 % (seq_len // T.int64(512))]

    @T.prim_func(private=True)
    def reshape25(var_take19: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        take19 = T.match_buffer(var_take19, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, seq_len // T.int64(512), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (T.int64(8) * (seq_len // T.int64(512)) * batch_size) // (T.int64(8) * (seq_len // T.int64(512))))
                    v1 = T.axis.spatial(seq_len // T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (T.int64(8) * (seq_len // T.int64(512))) // T.int64(8))
                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(8))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(take19[v0 * (seq_len // T.int64(512)) + v1, v2])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = take19[v0 * (seq_len // T.int64(512)) + v1, v2]

    @T.prim_func(private=True)
    def reshape26(var_conv1d48: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d48 = T.match_buffer(var_conv1d48, (batch_size, T.int64(1536), seq_len // T.int64(512)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(1536), seq_len // T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(1536) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(1536) * batch_size) // (seq_len // T.int64(512) * T.int64(1536)))
                    v1 = T.axis.spatial(T.int64(1536), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(1536)) // (seq_len // T.int64(512)))
                    v2 = T.axis.spatial(seq_len // T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512)))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(1536))
                    T.reads(conv1d48[(v0 * (seq_len // T.int64(512) * T.int64(1536)) + v1 * (seq_len // T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(1536) * batch_size) // (seq_len // T.int64(512) * T.int64(1536)), (v0 * (seq_len // T.int64(512) * T.int64(1536)) + v1 * (seq_len // T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(1536)) // (seq_len // T.int64(512)), (v0 * (seq_len // T.int64(512) * T.int64(1536)) + v1 * (seq_len // T.int64(512)) + v2) % (seq_len // T.int64(512))])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d48[(v0 * (seq_len // T.int64(512) * T.int64(1536)) + v1 * (seq_len // T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(1536) * batch_size) // (seq_len // T.int64(512) * T.int64(1536)), (v0 * (seq_len // T.int64(512) * T.int64(1536)) + v1 * (seq_len // T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(1536)) // (seq_len // T.int64(512)), (v0 * (seq_len // T.int64(512) * T.int64(1536)) + v1 * (seq_len // T.int64(512)) + v2) % (seq_len // T.int64(512))]

    @T.prim_func(private=True)
    def reshape27(var_conv1d_transpose: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d_transpose = T.match_buffer(var_conv1d_transpose, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(6), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(8) * T.int64(768) * batch_size) // (seq_len // T.int64(512) * T.int64(8) * T.int64(768)))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)))
                    v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(8)))
                    T.reads(conv1d_transpose[(v0 * (seq_len // T.int64(512) * T.int64(6144)) + v1 * (seq_len // T.int64(512) * T.int64(8)) + v2) % (seq_len // T.int64(512) * T.int64(8) * T.int64(768) * batch_size) // (seq_len // T.int64(512) * T.int64(8) * T.int64(768)), (v0 * (seq_len // T.int64(512) * T.int64(6144)) + v1 * (seq_len // T.int64(512) * T.int64(8)) + v2) % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)), (v0 * (seq_len // T.int64(512) * T.int64(6144)) + v1 * (seq_len // T.int64(512) * T.int64(8)) + v2) % (seq_len // T.int64(512) * T.int64(8))])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d_transpose[(v0 * (seq_len // T.int64(512) * T.int64(6144)) + v1 * (seq_len // T.int64(512) * T.int64(8)) + v2) % (seq_len // T.int64(512) * T.int64(8) * T.int64(768) * batch_size) // (seq_len // T.int64(512) * T.int64(8) * T.int64(768)), (v0 * (seq_len // T.int64(512) * T.int64(6144)) + v1 * (seq_len // T.int64(512) * T.int64(8)) + v2) % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)), (v0 * (seq_len // T.int64(512) * T.int64(6144)) + v1 * (seq_len // T.int64(512) * T.int64(8)) + v2) % (seq_len // T.int64(512) * T.int64(8))]

    @T.prim_func(private=True)
    def reshape28(var_conv1d_transpose1: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d_transpose1 = T.match_buffer(var_conv1d_transpose1, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(24), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(64) * T.int64(384) * batch_size) // (seq_len // T.int64(512) * T.int64(64) * T.int64(384)))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)))
                    v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(64)))
                    T.reads(conv1d_transpose1[(v0 * (seq_len // T.int64(512) * T.int64(24576)) + v1 * (seq_len // T.int64(512) * T.int64(64)) + v2) % (seq_len // T.int64(512) * T.int64(64) * T.int64(384) * batch_size) // (seq_len // T.int64(512) * T.int64(64) * T.int64(384)), (v0 * (seq_len // T.int64(512) * T.int64(24576)) + v1 * (seq_len // T.int64(512) * T.int64(64)) + v2) % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)), (v0 * (seq_len // T.int64(512) * T.int64(24576)) + v1 * (seq_len // T.int64(512) * T.int64(64)) + v2) % (seq_len // T.int64(512) * T.int64(64))])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d_transpose1[(v0 * (seq_len // T.int64(512) * T.int64(24576)) + v1 * (seq_len // T.int64(512) * T.int64(64)) + v2) % (seq_len // T.int64(512) * T.int64(64) * T.int64(384) * batch_size) // (seq_len // T.int64(512) * T.int64(64) * T.int64(384)), (v0 * (seq_len // T.int64(512) * T.int64(24576)) + v1 * (seq_len // T.int64(512) * T.int64(64)) + v2) % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)), (v0 * (seq_len // T.int64(512) * T.int64(24576)) + v1 * (seq_len // T.int64(512) * T.int64(64)) + v2) % (seq_len // T.int64(512) * T.int64(64))]

    @T.prim_func(private=True)
    def reshape29(var_conv1d_transpose2: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d_transpose2 = T.match_buffer(var_conv1d_transpose2, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(256) * T.int64(192) * batch_size) // (seq_len // T.int64(512) * T.int64(256) * T.int64(192)))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)))
                    v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(256)))
                    T.reads(conv1d_transpose2[(v0 * (seq_len // T.int64(512) * T.int64(49152)) + v1 * (seq_len // T.int64(512) * T.int64(256)) + v2) % (seq_len // T.int64(512) * T.int64(256) * T.int64(192) * batch_size) // (seq_len // T.int64(512) * T.int64(256) * T.int64(192)), (v0 * (seq_len // T.int64(512) * T.int64(49152)) + v1 * (seq_len // T.int64(512) * T.int64(256)) + v2) % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)), (v0 * (seq_len // T.int64(512) * T.int64(49152)) + v1 * (seq_len // T.int64(512) * T.int64(256)) + v2) % (seq_len // T.int64(512) * T.int64(256))])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d_transpose2[(v0 * (seq_len // T.int64(512) * T.int64(49152)) + v1 * (seq_len // T.int64(512) * T.int64(256)) + v2) % (seq_len // T.int64(512) * T.int64(256) * T.int64(192) * batch_size) // (seq_len // T.int64(512) * T.int64(256) * T.int64(192)), (v0 * (seq_len // T.int64(512) * T.int64(49152)) + v1 * (seq_len // T.int64(512) * T.int64(256)) + v2) % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)), (v0 * (seq_len // T.int64(512) * T.int64(49152)) + v1 * (seq_len // T.int64(512) * T.int64(256)) + v2) % (seq_len // T.int64(512) * T.int64(256))]

    @T.prim_func(private=True)
    def reshape3(var_conv1d_transpose4: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d_transpose4 = T.match_buffer(var_conv1d_transpose4, (batch_size, T.int64(768), seq_len * T.int64(8)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(768), seq_len * T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * seq_len * T.int64(6), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(8) * T.int64(768) * batch_size) // (seq_len * T.int64(8) * T.int64(768)))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)))
                    v2 = T.axis.spatial(seq_len * T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(8)))
                    T.reads(conv1d_transpose4[(v0 * (seq_len * T.int64(6144)) + v1 * (seq_len * T.int64(8)) + v2) % (seq_len * T.int64(8) * T.int64(768) * batch_size) // (seq_len * T.int64(8) * T.int64(768)), (v0 * (seq_len * T.int64(6144)) + v1 * (seq_len * T.int64(8)) + v2) % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)), (v0 * (seq_len * T.int64(6144)) + v1 * (seq_len * T.int64(8)) + v2) % (seq_len * T.int64(8))])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d_transpose4[(v0 * (seq_len * T.int64(6144)) + v1 * (seq_len * T.int64(8)) + v2) % (seq_len * T.int64(8) * T.int64(768) * batch_size) // (seq_len * T.int64(8) * T.int64(768)), (v0 * (seq_len * T.int64(6144)) + v1 * (seq_len * T.int64(8)) + v2) % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)), (v0 * (seq_len * T.int64(6144)) + v1 * (seq_len * T.int64(8)) + v2) % (seq_len * T.int64(8))]

    @T.prim_func(private=True)
    def reshape30(var_conv1d_transpose3: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d_transpose3 = T.match_buffer(var_conv1d_transpose3, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * (seq_len // T.int64(512)) * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(512) * T.int64(96) * batch_size) // (seq_len // T.int64(512) * T.int64(512) * T.int64(96)))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)))
                    v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(512)))
                    T.reads(conv1d_transpose3[(v0 * (seq_len // T.int64(512) * T.int64(49152)) + v1 * (seq_len // T.int64(512) * T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(512) * T.int64(96) * batch_size) // (seq_len // T.int64(512) * T.int64(512) * T.int64(96)), (v0 * (seq_len // T.int64(512) * T.int64(49152)) + v1 * (seq_len // T.int64(512) * T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)), (v0 * (seq_len // T.int64(512) * T.int64(49152)) + v1 * (seq_len // T.int64(512) * T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(512))])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d_transpose3[(v0 * (seq_len // T.int64(512) * T.int64(49152)) + v1 * (seq_len // T.int64(512) * T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(512) * T.int64(96) * batch_size) // (seq_len // T.int64(512) * T.int64(512) * T.int64(96)), (v0 * (seq_len // T.int64(512) * T.int64(49152)) + v1 * (seq_len // T.int64(512) * T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)), (v0 * (seq_len // T.int64(512) * T.int64(49152)) + v1 * (seq_len // T.int64(512) * T.int64(512)) + v2) % (seq_len // T.int64(512) * T.int64(512))]

    @T.prim_func(private=True)
    def reshape4(decoder_model_2_block_1_bias2: T.Buffer((T.int64(384),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(384), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(384))
                    T.reads(decoder_model_2_block_1_bias2[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = decoder_model_2_block_1_bias2[v0]

    @T.prim_func(private=True)
    def reshape5(var_conv1d_transpose5: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d_transpose5 = T.match_buffer(var_conv1d_transpose5, (batch_size, T.int64(384), seq_len * T.int64(64)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(384), seq_len * T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * seq_len * T.int64(24), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(64) * T.int64(384) * batch_size) // (seq_len * T.int64(64) * T.int64(384)))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)))
                    v2 = T.axis.spatial(seq_len * T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(64)))
                    T.reads(conv1d_transpose5[(v0 * (seq_len * T.int64(24576)) + v1 * (seq_len * T.int64(64)) + v2) % (seq_len * T.int64(64) * T.int64(384) * batch_size) // (seq_len * T.int64(64) * T.int64(384)), (v0 * (seq_len * T.int64(24576)) + v1 * (seq_len * T.int64(64)) + v2) % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)), (v0 * (seq_len * T.int64(24576)) + v1 * (seq_len * T.int64(64)) + v2) % (seq_len * T.int64(64))])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d_transpose5[(v0 * (seq_len * T.int64(24576)) + v1 * (seq_len * T.int64(64)) + v2) % (seq_len * T.int64(64) * T.int64(384) * batch_size) // (seq_len * T.int64(64) * T.int64(384)), (v0 * (seq_len * T.int64(24576)) + v1 * (seq_len * T.int64(64)) + v2) % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)), (v0 * (seq_len * T.int64(24576)) + v1 * (seq_len * T.int64(64)) + v2) % (seq_len * T.int64(64))]

    @T.prim_func(private=True)
    def reshape6(decoder_model_3_block_1_bias2: T.Buffer((T.int64(192),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(192), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(192))
                    T.reads(decoder_model_3_block_1_bias2[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = decoder_model_3_block_1_bias2[v0]

    @T.prim_func(private=True)
    def reshape7(var_conv1d_transpose6: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d_transpose6 = T.match_buffer(var_conv1d_transpose6, (batch_size, T.int64(192), seq_len * T.int64(256)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(192), seq_len * T.int64(256)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * seq_len * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(256) * T.int64(192) * batch_size) // (seq_len * T.int64(256) * T.int64(192)))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)))
                    v2 = T.axis.spatial(seq_len * T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(256)))
                    T.reads(conv1d_transpose6[(v0 * (seq_len * T.int64(49152)) + v1 * (seq_len * T.int64(256)) + v2) % (seq_len * T.int64(256) * T.int64(192) * batch_size) // (seq_len * T.int64(256) * T.int64(192)), (v0 * (seq_len * T.int64(49152)) + v1 * (seq_len * T.int64(256)) + v2) % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)), (v0 * (seq_len * T.int64(49152)) + v1 * (seq_len * T.int64(256)) + v2) % (seq_len * T.int64(256))])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d_transpose6[(v0 * (seq_len * T.int64(49152)) + v1 * (seq_len * T.int64(256)) + v2) % (seq_len * T.int64(256) * T.int64(192) * batch_size) // (seq_len * T.int64(256) * T.int64(192)), (v0 * (seq_len * T.int64(49152)) + v1 * (seq_len * T.int64(256)) + v2) % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)), (v0 * (seq_len * T.int64(49152)) + v1 * (seq_len * T.int64(256)) + v2) % (seq_len * T.int64(256))]

    @T.prim_func(private=True)
    def reshape8(decoder_model_4_block_1_bias2: T.Buffer((T.int64(96),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(96), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(96))
                    T.reads(decoder_model_4_block_1_bias2[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = decoder_model_4_block_1_bias2[v0]

    @T.prim_func(private=True)
    def reshape9(var_conv1d_transpose7: T.handle, var_T_reshape: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d_transpose7 = T.match_buffer(var_conv1d_transpose7, (batch_size, T.int64(96), seq_len * T.int64(512)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(96), seq_len * T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * seq_len * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(512) * T.int64(96) * batch_size) // (seq_len * T.int64(512) * T.int64(96)))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)))
                    v2 = T.axis.spatial(seq_len * T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(512)))
                    T.reads(conv1d_transpose7[(v0 * (seq_len * T.int64(49152)) + v1 * (seq_len * T.int64(512)) + v2) % (seq_len * T.int64(512) * T.int64(96) * batch_size) // (seq_len * T.int64(512) * T.int64(96)), (v0 * (seq_len * T.int64(49152)) + v1 * (seq_len * T.int64(512)) + v2) % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)), (v0 * (seq_len * T.int64(49152)) + v1 * (seq_len * T.int64(512)) + v2) % (seq_len * T.int64(512))])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = conv1d_transpose7[(v0 * (seq_len * T.int64(49152)) + v1 * (seq_len * T.int64(512)) + v2) % (seq_len * T.int64(512) * T.int64(96) * batch_size) // (seq_len * T.int64(512) * T.int64(96)), (v0 * (seq_len * T.int64(49152)) + v1 * (seq_len * T.int64(512)) + v2) % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)), (v0 * (seq_len * T.int64(49152)) + v1 * (seq_len * T.int64(512)) + v2) % (seq_len * T.int64(512))]

    @T.prim_func(private=True)
    def snake(var_reshape: T.handle, encoder_block_1_block_0_block_0_alpha: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32"), var_snake_compute: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, seq_len = T.int64(), T.int64()
        reshape = T.match_buffer(var_reshape, (batch_size, T.int64(64), seq_len))
        batch_size_1, seq_len_1 = T.int64(), T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(64), seq_len_1))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size_1 * seq_len_1 * T.int64(64) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 * T.int64(64) * batch_size_1) // (seq_len_1 * T.int64(64)))
                    v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 * T.int64(64)) // seq_len_1)
                    v2 = T.axis.spatial(seq_len_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % seq_len_1)
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size_1 * seq_len_1 * T.int64(64))
                    T.reads(reshape[v0, v1, v2], encoder_block_1_block_0_block_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape[v0, v1, v2] + T.float32(1.0) / (encoder_block_1_block_0_block_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(encoder_block_1_block_0_block_0_alpha[T.int64(0), v1, T.int64(0)] * reshape[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake1(var_reshape14: T.handle, encoder_block_2_block_0_block_0_alpha: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape14 = T.match_buffer(var_reshape14, (batch_size, T.int64(128), seq_len_1 // T.int64(2)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(128), seq_len // T.int64(2)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size_1 * (seq_len // T.int64(2)) * T.int64(128) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(2) * T.int64(128) * batch_size_1) // (seq_len // T.int64(2) * T.int64(128)))
                    v1 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(2) * T.int64(128)) // (seq_len // T.int64(2)))
                    v2 = T.axis.spatial(seq_len // T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(2)))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size_1 * (seq_len // T.int64(2)) * T.int64(128))
                    T.reads(reshape14[v0, v1, v2], encoder_block_2_block_0_block_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape14[v0, v1, v2] + T.float32(1.0) / (encoder_block_2_block_0_block_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(encoder_block_2_block_0_block_0_alpha[T.int64(0), v1, T.int64(0)] * reshape14[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake10(var_reshape110: T.handle, decoder_model_2_block_2_block_0_alpha: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape110 = T.match_buffer(var_reshape110, (batch_size, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(384), seq_len_1 // T.int64(512) * T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * (seq_len_1 // T.int64(512)) * T.int64(24), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(64) * T.int64(384) * batch_size_1) // (seq_len_1 // T.int64(512) * T.int64(64) * T.int64(384)))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len_1 // T.int64(512) * T.int64(64)))
                    v2 = T.axis.spatial(seq_len_1 // T.int64(512) * T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(64)))
                    T.reads(reshape110[v0, v1, v2], decoder_model_2_block_2_block_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape110[v0, v1, v2] + T.float32(1.0) / (decoder_model_2_block_2_block_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_2_block_2_block_0_alpha[T.int64(0), v1, T.int64(0)] * reshape110[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake11(var_reshape116: T.handle, decoder_model_2_block_3_block_2_alpha: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape116 = T.match_buffer(var_reshape116, (batch_size, T.int64(384), seq_len_1 // T.int64(512) * T.int64(64)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(384), seq_len // T.int64(512) * T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * (seq_len // T.int64(512)) * T.int64(24), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(64) * T.int64(384) * batch_size_1) // (seq_len // T.int64(512) * T.int64(64) * T.int64(384)))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(64) * T.int64(384)) // (seq_len // T.int64(512) * T.int64(64)))
                    v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(64)))
                    T.reads(reshape116[v0, v1, v2], decoder_model_2_block_3_block_2_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape116[v0, v1, v2] + T.float32(1.0) / (decoder_model_2_block_3_block_2_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_2_block_3_block_2_alpha[T.int64(0), v1, T.int64(0)] * reshape116[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake12(var_reshape124: T.handle, decoder_model_3_block_2_block_0_alpha: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape124 = T.match_buffer(var_reshape124, (batch_size, T.int64(192), seq_len_1 // T.int64(512) * T.int64(256)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * (seq_len // T.int64(512)) * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(256) * T.int64(192) * batch_size_1) // (seq_len // T.int64(512) * T.int64(256) * T.int64(192)))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len // T.int64(512) * T.int64(256)))
                    v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(256)))
                    T.reads(reshape124[v0, v1, v2], decoder_model_3_block_2_block_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape124[v0, v1, v2] + T.float32(1.0) / (decoder_model_3_block_2_block_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_3_block_2_block_0_alpha[T.int64(0), v1, T.int64(0)] * reshape124[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake13(var_reshape126: T.handle, decoder_model_3_block_2_block_2_alpha: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape126 = T.match_buffer(var_reshape126, (batch_size, T.int64(192), seq_len // T.int64(512) * T.int64(256)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(192), seq_len_1 // T.int64(512) * T.int64(256)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * (seq_len_1 // T.int64(512)) * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(256) * T.int64(192) * batch_size_1) // (seq_len_1 // T.int64(512) * T.int64(256) * T.int64(192)))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(256) * T.int64(192)) // (seq_len_1 // T.int64(512) * T.int64(256)))
                    v2 = T.axis.spatial(seq_len_1 // T.int64(512) * T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(256)))
                    T.reads(reshape126[v0, v1, v2], decoder_model_3_block_2_block_2_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape126[v0, v1, v2] + T.float32(1.0) / (decoder_model_3_block_2_block_2_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_3_block_2_block_2_alpha[T.int64(0), v1, T.int64(0)] * reshape126[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake14(var_reshape138: T.handle, decoder_model_4_block_2_block_0_alpha: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape138 = T.match_buffer(var_reshape138, (batch_size, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(96), seq_len_1 // T.int64(512) * T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * (seq_len_1 // T.int64(512)) * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(512) * T.int64(96) * batch_size_1) // (seq_len_1 // T.int64(512) * T.int64(512) * T.int64(96)))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len_1 // T.int64(512) * T.int64(512)))
                    v2 = T.axis.spatial(seq_len_1 // T.int64(512) * T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(512)))
                    T.reads(reshape138[v0, v1, v2], decoder_model_4_block_2_block_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape138[v0, v1, v2] + T.float32(1.0) / (decoder_model_4_block_2_block_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_4_block_2_block_0_alpha[T.int64(0), v1, T.int64(0)] * reshape138[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake15(var_reshape142: T.handle, decoder_model_4_block_3_block_0_alpha: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape142 = T.match_buffer(var_reshape142, (batch_size, T.int64(96), seq_len_1 // T.int64(512) * T.int64(512)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(96), seq_len // T.int64(512) * T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * (seq_len // T.int64(512)) * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(512) * T.int64(96) * batch_size_1) // (seq_len // T.int64(512) * T.int64(512) * T.int64(96)))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(512) * T.int64(96)) // (seq_len // T.int64(512) * T.int64(512)))
                    v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(512)))
                    T.reads(reshape142[v0, v1, v2], decoder_model_4_block_3_block_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape142[v0, v1, v2] + T.float32(1.0) / (decoder_model_4_block_3_block_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_4_block_3_block_0_alpha[T.int64(0), v1, T.int64(0)] * reshape142[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake16(var_reshape200: T.handle, encoder_block_4_block_1_block_2_alpha1: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape200 = T.match_buffer(var_reshape200, (batch_size, T.int64(512), seq_len_1 // T.int64(64)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(512), seq_len // T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size_1 * (seq_len // T.int64(64)) * T.int64(512) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(64) * T.int64(512) * batch_size_1) // (seq_len // T.int64(64) * T.int64(512)))
                    v1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(64) * T.int64(512)) // (seq_len // T.int64(64)))
                    v2 = T.axis.spatial(seq_len // T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(64)))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size_1 * (seq_len // T.int64(64)) * T.int64(512))
                    T.reads(reshape200[v0, v1, v2], encoder_block_4_block_1_block_2_alpha1[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape200[v0, v1, v2] + T.float32(1.0) / (encoder_block_4_block_1_block_2_alpha1[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(encoder_block_4_block_1_block_2_alpha1[T.int64(0), v1, T.int64(0)] * reshape200[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake17(var_reshape246: T.handle, decoder_model_1_block_0_alpha2: T.Buffer((T.int64(1), T.int64(1536), T.int64(1)), "float32"), var_snake_compute: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size, seq_len = T.int64(), T.int64()
        reshape246 = T.match_buffer(var_reshape246, (batch_size, T.int64(1536), seq_len))
        batch_size_1, seq_len_1 = T.int64(), T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(1536), seq_len_1))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size_1 * seq_len_1 * T.int64(1536) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 * T.int64(1536) * batch_size_1) // (seq_len_1 * T.int64(1536)))
                    v1 = T.axis.spatial(T.int64(1536), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 * T.int64(1536)) // seq_len_1)
                    v2 = T.axis.spatial(seq_len_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % seq_len_1)
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size_1 * seq_len_1 * T.int64(1536))
                    T.reads(reshape246[v0, v1, v2], decoder_model_1_block_0_alpha2[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape246[v0, v1, v2] + T.float32(1.0) / (decoder_model_1_block_0_alpha2[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_1_block_0_alpha2[T.int64(0), v1, T.int64(0)] * reshape246[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake18(var_reshape248: T.handle, decoder_model_1_block_2_block_0_alpha2: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape248 = T.match_buffer(var_reshape248, (batch_size, T.int64(768), seq_len_1 * T.int64(8)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(768), seq_len * T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * seq_len * T.int64(6), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(8) * T.int64(768) * batch_size_1) // (seq_len * T.int64(8) * T.int64(768)))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(8) * T.int64(768)) // (seq_len * T.int64(8)))
                    v2 = T.axis.spatial(seq_len * T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(8)))
                    T.reads(reshape248[v0, v1, v2], decoder_model_1_block_2_block_0_alpha2[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape248[v0, v1, v2] + T.float32(1.0) / (decoder_model_1_block_2_block_0_alpha2[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_1_block_2_block_0_alpha2[T.int64(0), v1, T.int64(0)] * reshape248[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake19(var_reshape250: T.handle, decoder_model_1_block_2_block_2_alpha2: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape250 = T.match_buffer(var_reshape250, (batch_size, T.int64(768), seq_len * T.int64(8)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(768), seq_len_1 * T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * seq_len_1 * T.int64(6), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 * T.int64(8) * T.int64(768) * batch_size_1) // (seq_len_1 * T.int64(8) * T.int64(768)))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 * T.int64(8) * T.int64(768)) // (seq_len_1 * T.int64(8)))
                    v2 = T.axis.spatial(seq_len_1 * T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 * T.int64(8)))
                    T.reads(reshape250[v0, v1, v2], decoder_model_1_block_2_block_2_alpha2[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape250[v0, v1, v2] + T.float32(1.0) / (decoder_model_1_block_2_block_2_alpha2[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_1_block_2_block_2_alpha2[T.int64(0), v1, T.int64(0)] * reshape250[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake2(var_reshape18: T.handle, encoder_block_2_block_1_block_0_alpha: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape18 = T.match_buffer(var_reshape18, (batch_size, T.int64(128), seq_len // T.int64(2)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(128), seq_len_1 // T.int64(2)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size_1 * (seq_len_1 // T.int64(2)) * T.int64(128) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(2) * T.int64(128) * batch_size_1) // (seq_len_1 // T.int64(2) * T.int64(128)))
                    v1 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(2) * T.int64(128)) // (seq_len_1 // T.int64(2)))
                    v2 = T.axis.spatial(seq_len_1 // T.int64(2), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(2)))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size_1 * (seq_len_1 // T.int64(2)) * T.int64(128))
                    T.reads(reshape18[v0, v1, v2], encoder_block_2_block_1_block_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape18[v0, v1, v2] + T.float32(1.0) / (encoder_block_2_block_1_block_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(encoder_block_2_block_1_block_0_alpha[T.int64(0), v1, T.int64(0)] * reshape18[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake20(var_reshape262: T.handle, decoder_model_2_block_2_block_0_alpha2: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape262 = T.match_buffer(var_reshape262, (batch_size, T.int64(384), seq_len * T.int64(64)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(384), seq_len_1 * T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * seq_len_1 * T.int64(24), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 * T.int64(64) * T.int64(384) * batch_size_1) // (seq_len_1 * T.int64(64) * T.int64(384)))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 * T.int64(64) * T.int64(384)) // (seq_len_1 * T.int64(64)))
                    v2 = T.axis.spatial(seq_len_1 * T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 * T.int64(64)))
                    T.reads(reshape262[v0, v1, v2], decoder_model_2_block_2_block_0_alpha2[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape262[v0, v1, v2] + T.float32(1.0) / (decoder_model_2_block_2_block_0_alpha2[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_2_block_2_block_0_alpha2[T.int64(0), v1, T.int64(0)] * reshape262[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake21(var_reshape268: T.handle, decoder_model_2_block_3_block_2_alpha2: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape268 = T.match_buffer(var_reshape268, (batch_size, T.int64(384), seq_len_1 * T.int64(64)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(384), seq_len * T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * seq_len * T.int64(24), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(64) * T.int64(384) * batch_size_1) // (seq_len * T.int64(64) * T.int64(384)))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(64) * T.int64(384)) // (seq_len * T.int64(64)))
                    v2 = T.axis.spatial(seq_len * T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(64)))
                    T.reads(reshape268[v0, v1, v2], decoder_model_2_block_3_block_2_alpha2[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape268[v0, v1, v2] + T.float32(1.0) / (decoder_model_2_block_3_block_2_alpha2[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_2_block_3_block_2_alpha2[T.int64(0), v1, T.int64(0)] * reshape268[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake22(var_reshape276: T.handle, decoder_model_3_block_2_block_0_alpha2: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape276 = T.match_buffer(var_reshape276, (batch_size, T.int64(192), seq_len * T.int64(256)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(192), seq_len_1 * T.int64(256)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * seq_len_1 * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 * T.int64(256) * T.int64(192) * batch_size_1) // (seq_len_1 * T.int64(256) * T.int64(192)))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 * T.int64(256) * T.int64(192)) // (seq_len_1 * T.int64(256)))
                    v2 = T.axis.spatial(seq_len_1 * T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 * T.int64(256)))
                    T.reads(reshape276[v0, v1, v2], decoder_model_3_block_2_block_0_alpha2[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape276[v0, v1, v2] + T.float32(1.0) / (decoder_model_3_block_2_block_0_alpha2[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_3_block_2_block_0_alpha2[T.int64(0), v1, T.int64(0)] * reshape276[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake23(var_reshape280: T.handle, decoder_model_3_block_3_block_0_alpha2: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape280 = T.match_buffer(var_reshape280, (batch_size, T.int64(192), seq_len_1 * T.int64(256)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(192), seq_len * T.int64(256)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * seq_len * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(256) * T.int64(192) * batch_size_1) // (seq_len * T.int64(256) * T.int64(192)))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(256) * T.int64(192)) // (seq_len * T.int64(256)))
                    v2 = T.axis.spatial(seq_len * T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(256)))
                    T.reads(reshape280[v0, v1, v2], decoder_model_3_block_3_block_0_alpha2[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape280[v0, v1, v2] + T.float32(1.0) / (decoder_model_3_block_3_block_0_alpha2[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_3_block_3_block_0_alpha2[T.int64(0), v1, T.int64(0)] * reshape280[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake24(var_reshape290: T.handle, decoder_model_4_block_2_block_0_alpha2: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape290 = T.match_buffer(var_reshape290, (batch_size, T.int64(96), seq_len_1 * T.int64(512)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(96), seq_len * T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * seq_len * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(512) * T.int64(96) * batch_size_1) // (seq_len * T.int64(512) * T.int64(96)))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(512) * T.int64(96)) // (seq_len * T.int64(512)))
                    v2 = T.axis.spatial(seq_len * T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len * T.int64(512)))
                    T.reads(reshape290[v0, v1, v2], decoder_model_4_block_2_block_0_alpha2[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape290[v0, v1, v2] + T.float32(1.0) / (decoder_model_4_block_2_block_0_alpha2[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_4_block_2_block_0_alpha2[T.int64(0), v1, T.int64(0)] * reshape290[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake3(var_reshape28: T.handle, encoder_block_3_block_0_block_0_alpha: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape28 = T.match_buffer(var_reshape28, (batch_size, T.int64(256), seq_len // T.int64(8)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(256), seq_len_1 // T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size_1 * (seq_len_1 // T.int64(8)) * T.int64(256) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(8) * T.int64(256) * batch_size_1) // (seq_len_1 // T.int64(8) * T.int64(256)))
                    v1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(8) * T.int64(256)) // (seq_len_1 // T.int64(8)))
                    v2 = T.axis.spatial(seq_len_1 // T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(8)))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size_1 * (seq_len_1 // T.int64(8)) * T.int64(256))
                    T.reads(reshape28[v0, v1, v2], encoder_block_3_block_0_block_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape28[v0, v1, v2] + T.float32(1.0) / (encoder_block_3_block_0_block_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(encoder_block_3_block_0_block_0_alpha[T.int64(0), v1, T.int64(0)] * reshape28[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake4(var_reshape30: T.handle, encoder_block_3_block_0_block_2_alpha: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape30 = T.match_buffer(var_reshape30, (batch_size, T.int64(256), seq_len_1 // T.int64(8)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(256), seq_len // T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size_1 * (seq_len // T.int64(8)) * T.int64(256) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(8) * T.int64(256) * batch_size_1) // (seq_len // T.int64(8) * T.int64(256)))
                    v1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(8) * T.int64(256)) // (seq_len // T.int64(8)))
                    v2 = T.axis.spatial(seq_len // T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(8)))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size_1 * (seq_len // T.int64(8)) * T.int64(256))
                    T.reads(reshape30[v0, v1, v2], encoder_block_3_block_0_block_2_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape30[v0, v1, v2] + T.float32(1.0) / (encoder_block_3_block_0_block_2_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(encoder_block_3_block_0_block_2_alpha[T.int64(0), v1, T.int64(0)] * reshape30[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake5(var_reshape42: T.handle, encoder_block_4_block_0_block_0_alpha: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape42 = T.match_buffer(var_reshape42, (batch_size, T.int64(512), seq_len // T.int64(64)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(512), seq_len_1 // T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size_1 * (seq_len_1 // T.int64(64)) * T.int64(512) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(64) * T.int64(512) * batch_size_1) // (seq_len_1 // T.int64(64) * T.int64(512)))
                    v1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(64) * T.int64(512)) // (seq_len_1 // T.int64(64)))
                    v2 = T.axis.spatial(seq_len_1 // T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(64)))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size_1 * (seq_len_1 // T.int64(64)) * T.int64(512))
                    T.reads(reshape42[v0, v1, v2], encoder_block_4_block_0_block_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape42[v0, v1, v2] + T.float32(1.0) / (encoder_block_4_block_0_block_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(encoder_block_4_block_0_block_0_alpha[T.int64(0), v1, T.int64(0)] * reshape42[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake6(var_reshape56: T.handle, encoder_block_5_alpha: T.Buffer((T.int64(1), T.int64(1024), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape56 = T.match_buffer(var_reshape56, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(1024), seq_len_1 // T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * (seq_len_1 // T.int64(512)), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(1024) * batch_size_1) // (seq_len_1 // T.int64(512) * T.int64(1024)))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(1024)) // (seq_len_1 // T.int64(512)))
                    v2 = T.axis.spatial(seq_len_1 // T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512)))
                    T.reads(reshape56[v0, v1, v2], encoder_block_5_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape56[v0, v1, v2] + T.float32(1.0) / (encoder_block_5_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(encoder_block_5_alpha[T.int64(0), v1, T.int64(0)] * reshape56[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake7(var_reshape94: T.handle, decoder_model_1_block_0_alpha: T.Buffer((T.int64(1), T.int64(1536), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape94 = T.match_buffer(var_reshape94, (batch_size, T.int64(1536), seq_len // T.int64(512)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(1536), seq_len_1 // T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size_1 * (seq_len_1 // T.int64(512)) * T.int64(1536) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(1536) * batch_size_1) // (seq_len_1 // T.int64(512) * T.int64(1536)))
                    v1 = T.axis.spatial(T.int64(1536), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(1536)) // (seq_len_1 // T.int64(512)))
                    v2 = T.axis.spatial(seq_len_1 // T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512)))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size_1 * (seq_len_1 // T.int64(512)) * T.int64(1536))
                    T.reads(reshape94[v0, v1, v2], decoder_model_1_block_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape94[v0, v1, v2] + T.float32(1.0) / (decoder_model_1_block_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_1_block_0_alpha[T.int64(0), v1, T.int64(0)] * reshape94[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake8(var_reshape96: T.handle, decoder_model_1_block_2_block_0_alpha: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape96 = T.match_buffer(var_reshape96, (batch_size, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(768), seq_len_1 // T.int64(512) * T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * (seq_len_1 // T.int64(512)) * T.int64(6), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(8) * T.int64(768) * batch_size_1) // (seq_len_1 // T.int64(512) * T.int64(8) * T.int64(768)))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len_1 // T.int64(512) * T.int64(8)))
                    v2 = T.axis.spatial(seq_len_1 // T.int64(512) * T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len_1 // T.int64(512) * T.int64(8)))
                    T.reads(reshape96[v0, v1, v2], decoder_model_1_block_2_block_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape96[v0, v1, v2] + T.float32(1.0) / (decoder_model_1_block_2_block_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_1_block_2_block_0_alpha[T.int64(0), v1, T.int64(0)] * reshape96[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake9(var_reshape98: T.handle, decoder_model_1_block_2_block_2_alpha: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), var_snake_compute: T.handle, seq_len: T.int64, seq_len_1: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape98 = T.match_buffer(var_reshape98, (batch_size, T.int64(768), seq_len_1 // T.int64(512) * T.int64(8)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(768), seq_len // T.int64(512) * T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * (seq_len // T.int64(512)) * T.int64(6), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(8) * T.int64(768) * batch_size_1) // (seq_len // T.int64(512) * T.int64(8) * T.int64(768)))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(8) * T.int64(768)) // (seq_len // T.int64(512) * T.int64(8)))
                    v2 = T.axis.spatial(seq_len // T.int64(512) * T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(8)))
                    T.reads(reshape98[v0, v1, v2], decoder_model_1_block_2_block_2_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape98[v0, v1, v2] + T.float32(1.0) / (decoder_model_1_block_2_block_2_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_1_block_2_block_2_alpha[T.int64(0), v1, T.int64(0)] * reshape98[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def subtract1(var_conv1d103: T.handle, var_conv1d105: T.handle, var_T_subtract: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d103 = T.match_buffer(var_conv1d103, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        conv1d105 = T.match_buffer(var_conv1d105, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        T_subtract = T.match_buffer(var_T_subtract, (batch_size, T.int64(1024), seq_len // T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * (seq_len // T.int64(512)), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_subtract"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(1024) * batch_size) // (seq_len // T.int64(512) * T.int64(1024)))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(1024)) // (seq_len // T.int64(512)))
                    v2 = T.axis.spatial(seq_len // T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512)))
                    T.reads(conv1d103[v0, v1, v2], conv1d105[v0, v1, v2])
                    T.writes(T_subtract[v0, v1, v2])
                    T_subtract[v0, v1, v2] = conv1d103[v0, v1, v2] - conv1d105[v0, v1, v2]

    @T.prim_func(private=True)
    def take(var_argsort9: T.handle, B: T.Buffer((T.int64(1),), "int32"), var_T_take: T.handle, batch_size: T.int64, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        argsort9 = T.match_buffer(var_argsort9, (batch_size * (seq_len // T.int64(512)), T.int64(1024)), "int32")
        T_take = T.match_buffer(var_T_take, (batch_size * (seq_len // T.int64(512)), T.int64(1)), "int32")
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_take"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < batch_size * (seq_len // T.int64(512)))
                    T.reads(argsort9[v0, B[T.int64(0)]], B[T.int64(0)])
                    T.writes(T_take[v0, T.int64(0)])
                    T_take[v0, T.int64(0)] = argsort9[v0, B[T.int64(0)]]

    @T.prim_func(private=True)
    def take1(quantizer_quantizers_0_codebook_weight1: T.Buffer((T.int64(1024), T.int64(8)), "float32"), var_reshape212: T.handle, var_T_take: T.handle, batch_size: T.int64, seq_len: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        reshape212 = T.match_buffer(var_reshape212, (batch_size * (seq_len // T.int64(512)),), "int32")
        T_take = T.match_buffer(var_T_take, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_take"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(quantizer_quantizers_0_codebook_weight1[reshape212[v0], v1], reshape212[v0])
                    T.writes(T_take[v0, v1])
                    T_take[v0, v1] = quantizer_quantizers_0_codebook_weight1[reshape212[v0], v1]

    @T.prim_func(private=True)
    def take2(quantizer_quantizers_2_codebook_weight1: T.Buffer((T.int64(1024), T.int64(8)), "float32"), var_reshape220: T.handle, var_T_take: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        reshape220 = T.match_buffer(var_reshape220, (batch_size * (seq_len // T.int64(512)),), "int32")
        T_take = T.match_buffer(var_T_take, (batch_size * (seq_len // T.int64(512)), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_take"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(quantizer_quantizers_2_codebook_weight1[reshape220[v0], v1], reshape220[v0])
                    T.writes(T_take[v0, v1])
                    T_take[v0, v1] = quantizer_quantizers_2_codebook_weight1[reshape220[v0], v1]

    @T.prim_func(private=True)
    def take3(var_argsort12: T.handle, B: T.Buffer((T.int64(1),), "int32"), var_T_take: T.handle, seq_len: T.int64, batch_size: T.int64):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        argsort12 = T.match_buffer(var_argsort12, (batch_size * (seq_len // T.int64(512)), T.int64(1024)), "int32")
        T_take = T.match_buffer(var_T_take, (batch_size * (seq_len // T.int64(512)), T.int64(1)), "int32")
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_take"):
                    v0 = T.axis.spatial(batch_size * (seq_len // T.int64(512)), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < batch_size * (seq_len // T.int64(512)))
                    T.reads(argsort12[v0, B[T.int64(0)]], B[T.int64(0)])
                    T.writes(T_take[v0, T.int64(0)])
                    T_take[v0, T.int64(0)] = argsort12[v0, B[T.int64(0)]]

    @T.prim_func(private=True)
    def transpose(var_conv1d104: T.handle, var_T_transpose: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d104 = T.match_buffer(var_conv1d104, (batch_size, T.int64(8), seq_len // T.int64(512)))
        T_transpose = T.match_buffer(var_T_transpose, (batch_size, seq_len // T.int64(512), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_transpose"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (T.int64(8) * (seq_len // T.int64(512)) * batch_size) // (T.int64(8) * (seq_len // T.int64(512))))
                    v1 = T.axis.spatial(seq_len // T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (T.int64(8) * (seq_len // T.int64(512))) // T.int64(8))
                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(8))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(conv1d104[v0, v2, v1])
                    T.writes(T_transpose[v0, v1, v2])
                    T_transpose[v0, v1, v2] = conv1d104[v0, v2, v1]

    @T.prim_func(private=True)
    def transpose1(divide19: T.Buffer((T.int64(1024), T.int64(8)), "float32"), T_transpose: T.Buffer((T.int64(8), T.int64(1024)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(8), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_transpose"):
                    v0 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(1024))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(1024))
                    T.reads(divide19[v1, v0])
                    T.writes(T_transpose[v0, v1])
                    T_transpose[v0, v1] = divide19[v1, v0]

    @T.prim_func(private=True)
    def transpose2(sum39: T.Buffer((T.int64(1024), T.int64(1)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(1024)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_transpose"):
                    v0 = T.axis.spatial(T.int64(1024), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.reads(sum39[v0, T.int64(0)])
                    T.writes(T_transpose[T.int64(0), v0])
                    T_transpose[T.int64(0), v0] = sum39[v0, T.int64(0)]

    @T.prim_func(private=True)
    def transpose3(var_reshape213: T.handle, var_T_transpose: T.handle, seq_len: T.int64):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape213 = T.match_buffer(var_reshape213, (batch_size, seq_len // T.int64(512), T.int64(8)))
        T_transpose = T.match_buffer(var_T_transpose, (batch_size, T.int64(8), seq_len // T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((batch_size * (seq_len // T.int64(512)) * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_transpose"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(8) * batch_size) // (seq_len // T.int64(512) * T.int64(8)))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512) * T.int64(8)) // (seq_len // T.int64(512)))
                    v2 = T.axis.spatial(seq_len // T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % (seq_len // T.int64(512)))
                    T.where(ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1 < batch_size * (seq_len // T.int64(512)) * T.int64(8))
                    T.reads(reshape213[v0, v2, v1])
                    T.writes(T_transpose[v0, v1, v2])
                    T_transpose[v0, v1, v2] = reshape213[v0, v2, v1]

    @R.function
    def decode(z: R.Tensor(("batch_size", 1024, "seq_len"), dtype="float32"), packed_params: R.Tuple(R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 1, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 64, 4), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 128, 8), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 256, 16), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 512, 16), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1, 1024, 1), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 1024, 3), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((1536, 1, 1), dtype="float32"), R.Tensor((1536, 1024, 7), dtype="float32"), R.Tensor((1536,), dtype="float32"), R.Tensor((1, 1536, 1), dtype="float32"), R.Tensor((1536, 1, 1), dtype="float32"), R.Tensor((1536, 768, 16), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 384, 16), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 192, 8), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 96, 4), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((1, 1, 1), dtype="float32"), R.Tensor((1, 96, 7), dtype="float32"), R.Tensor((1,), dtype="float32"))) -> R.Tensor(("batch_size", 1, "seq_len * 512"), dtype="float32"):
        batch_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 1})
        cls = Module
        with R.dataflow():
            decoder_model_0_weight_g2: R.Tensor((1536, 1, 1), dtype="float32") = packed_params[182]
            decoder_model_0_weight_v2: R.Tensor((1536, 1024, 7), dtype="float32") = packed_params[183]
            decoder_model_0_bias2: R.Tensor((1536,), dtype="float32") = packed_params[184]
            decoder_model_1_block_0_alpha2: R.Tensor((1, 1536, 1), dtype="float32") = packed_params[185]
            decoder_model_1_block_1_weight_g2: R.Tensor((1536, 1, 1), dtype="float32") = packed_params[186]
            decoder_model_1_block_1_weight_v2: R.Tensor((1536, 768, 16), dtype="float32") = packed_params[187]
            decoder_model_1_block_1_bias2: R.Tensor((768,), dtype="float32") = packed_params[188]
            decoder_model_1_block_2_block_0_alpha2: R.Tensor((1, 768, 1), dtype="float32") = packed_params[189]
            decoder_model_1_block_2_block_1_weight_g2: R.Tensor((768, 1, 1), dtype="float32") = packed_params[190]
            decoder_model_1_block_2_block_1_weight_v2: R.Tensor((768, 768, 7), dtype="float32") = packed_params[191]
            decoder_model_1_block_2_block_1_bias2: R.Tensor((768,), dtype="float32") = packed_params[192]
            decoder_model_1_block_2_block_2_alpha2: R.Tensor((1, 768, 1), dtype="float32") = packed_params[193]
            decoder_model_1_block_2_block_3_weight_g2: R.Tensor((768, 1, 1), dtype="float32") = packed_params[194]
            decoder_model_1_block_2_block_3_weight_v2: R.Tensor((768, 768, 1), dtype="float32") = packed_params[195]
            decoder_model_1_block_2_block_3_bias2: R.Tensor((768,), dtype="float32") = packed_params[196]
            decoder_model_1_block_3_block_0_alpha2: R.Tensor((1, 768, 1), dtype="float32") = packed_params[197]
            decoder_model_1_block_3_block_1_weight_g2: R.Tensor((768, 1, 1), dtype="float32") = packed_params[198]
            decoder_model_1_block_3_block_1_weight_v2: R.Tensor((768, 768, 7), dtype="float32") = packed_params[199]
            decoder_model_1_block_3_block_1_bias2: R.Tensor((768,), dtype="float32") = packed_params[200]
            decoder_model_1_block_3_block_2_alpha2: R.Tensor((1, 768, 1), dtype="float32") = packed_params[201]
            decoder_model_1_block_3_block_3_weight_g2: R.Tensor((768, 1, 1), dtype="float32") = packed_params[202]
            decoder_model_1_block_3_block_3_weight_v2: R.Tensor((768, 768, 1), dtype="float32") = packed_params[203]
            decoder_model_1_block_3_block_3_bias2: R.Tensor((768,), dtype="float32") = packed_params[204]
            decoder_model_1_block_4_block_0_alpha2: R.Tensor((1, 768, 1), dtype="float32") = packed_params[205]
            decoder_model_1_block_4_block_1_weight_g2: R.Tensor((768, 1, 1), dtype="float32") = packed_params[206]
            decoder_model_1_block_4_block_1_weight_v2: R.Tensor((768, 768, 7), dtype="float32") = packed_params[207]
            decoder_model_1_block_4_block_1_bias2: R.Tensor((768,), dtype="float32") = packed_params[208]
            decoder_model_1_block_4_block_2_alpha2: R.Tensor((1, 768, 1), dtype="float32") = packed_params[209]
            decoder_model_1_block_4_block_3_weight_g2: R.Tensor((768, 1, 1), dtype="float32") = packed_params[210]
            decoder_model_1_block_4_block_3_weight_v2: R.Tensor((768, 768, 1), dtype="float32") = packed_params[211]
            decoder_model_1_block_4_block_3_bias2: R.Tensor((768,), dtype="float32") = packed_params[212]
            decoder_model_2_block_0_alpha2: R.Tensor((1, 768, 1), dtype="float32") = packed_params[213]
            decoder_model_2_block_1_weight_g2: R.Tensor((768, 1, 1), dtype="float32") = packed_params[214]
            decoder_model_2_block_1_weight_v2: R.Tensor((768, 384, 16), dtype="float32") = packed_params[215]
            decoder_model_2_block_1_bias2: R.Tensor((384,), dtype="float32") = packed_params[216]
            decoder_model_2_block_2_block_0_alpha2: R.Tensor((1, 384, 1), dtype="float32") = packed_params[217]
            decoder_model_2_block_2_block_1_weight_g2: R.Tensor((384, 1, 1), dtype="float32") = packed_params[218]
            decoder_model_2_block_2_block_1_weight_v2: R.Tensor((384, 384, 7), dtype="float32") = packed_params[219]
            decoder_model_2_block_2_block_1_bias2: R.Tensor((384,), dtype="float32") = packed_params[220]
            decoder_model_2_block_2_block_2_alpha2: R.Tensor((1, 384, 1), dtype="float32") = packed_params[221]
            decoder_model_2_block_2_block_3_weight_g2: R.Tensor((384, 1, 1), dtype="float32") = packed_params[222]
            decoder_model_2_block_2_block_3_weight_v2: R.Tensor((384, 384, 1), dtype="float32") = packed_params[223]
            decoder_model_2_block_2_block_3_bias2: R.Tensor((384,), dtype="float32") = packed_params[224]
            decoder_model_2_block_3_block_0_alpha2: R.Tensor((1, 384, 1), dtype="float32") = packed_params[225]
            decoder_model_2_block_3_block_1_weight_g2: R.Tensor((384, 1, 1), dtype="float32") = packed_params[226]
            decoder_model_2_block_3_block_1_weight_v2: R.Tensor((384, 384, 7), dtype="float32") = packed_params[227]
            decoder_model_2_block_3_block_1_bias2: R.Tensor((384,), dtype="float32") = packed_params[228]
            decoder_model_2_block_3_block_2_alpha2: R.Tensor((1, 384, 1), dtype="float32") = packed_params[229]
            decoder_model_2_block_3_block_3_weight_g2: R.Tensor((384, 1, 1), dtype="float32") = packed_params[230]
            decoder_model_2_block_3_block_3_weight_v2: R.Tensor((384, 384, 1), dtype="float32") = packed_params[231]
            decoder_model_2_block_3_block_3_bias2: R.Tensor((384,), dtype="float32") = packed_params[232]
            decoder_model_2_block_4_block_0_alpha2: R.Tensor((1, 384, 1), dtype="float32") = packed_params[233]
            decoder_model_2_block_4_block_1_weight_g2: R.Tensor((384, 1, 1), dtype="float32") = packed_params[234]
            decoder_model_2_block_4_block_1_weight_v2: R.Tensor((384, 384, 7), dtype="float32") = packed_params[235]
            decoder_model_2_block_4_block_1_bias2: R.Tensor((384,), dtype="float32") = packed_params[236]
            decoder_model_2_block_4_block_2_alpha2: R.Tensor((1, 384, 1), dtype="float32") = packed_params[237]
            decoder_model_2_block_4_block_3_weight_g2: R.Tensor((384, 1, 1), dtype="float32") = packed_params[238]
            decoder_model_2_block_4_block_3_weight_v2: R.Tensor((384, 384, 1), dtype="float32") = packed_params[239]
            decoder_model_2_block_4_block_3_bias2: R.Tensor((384,), dtype="float32") = packed_params[240]
            decoder_model_3_block_0_alpha2: R.Tensor((1, 384, 1), dtype="float32") = packed_params[241]
            decoder_model_3_block_1_weight_g2: R.Tensor((384, 1, 1), dtype="float32") = packed_params[242]
            decoder_model_3_block_1_weight_v2: R.Tensor((384, 192, 8), dtype="float32") = packed_params[243]
            decoder_model_3_block_1_bias2: R.Tensor((192,), dtype="float32") = packed_params[244]
            decoder_model_3_block_2_block_0_alpha2: R.Tensor((1, 192, 1), dtype="float32") = packed_params[245]
            decoder_model_3_block_2_block_1_weight_g2: R.Tensor((192, 1, 1), dtype="float32") = packed_params[246]
            decoder_model_3_block_2_block_1_weight_v2: R.Tensor((192, 192, 7), dtype="float32") = packed_params[247]
            decoder_model_3_block_2_block_1_bias2: R.Tensor((192,), dtype="float32") = packed_params[248]
            decoder_model_3_block_2_block_2_alpha2: R.Tensor((1, 192, 1), dtype="float32") = packed_params[249]
            decoder_model_3_block_2_block_3_weight_g2: R.Tensor((192, 1, 1), dtype="float32") = packed_params[250]
            decoder_model_3_block_2_block_3_weight_v2: R.Tensor((192, 192, 1), dtype="float32") = packed_params[251]
            decoder_model_3_block_2_block_3_bias2: R.Tensor((192,), dtype="float32") = packed_params[252]
            decoder_model_3_block_3_block_0_alpha2: R.Tensor((1, 192, 1), dtype="float32") = packed_params[253]
            decoder_model_3_block_3_block_1_weight_g2: R.Tensor((192, 1, 1), dtype="float32") = packed_params[254]
            decoder_model_3_block_3_block_1_weight_v2: R.Tensor((192, 192, 7), dtype="float32") = packed_params[255]
            decoder_model_3_block_3_block_1_bias2: R.Tensor((192,), dtype="float32") = packed_params[256]
            decoder_model_3_block_3_block_2_alpha2: R.Tensor((1, 192, 1), dtype="float32") = packed_params[257]
            decoder_model_3_block_3_block_3_weight_g2: R.Tensor((192, 1, 1), dtype="float32") = packed_params[258]
            decoder_model_3_block_3_block_3_weight_v2: R.Tensor((192, 192, 1), dtype="float32") = packed_params[259]
            decoder_model_3_block_3_block_3_bias2: R.Tensor((192,), dtype="float32") = packed_params[260]
            decoder_model_3_block_4_block_0_alpha2: R.Tensor((1, 192, 1), dtype="float32") = packed_params[261]
            decoder_model_3_block_4_block_1_weight_g2: R.Tensor((192, 1, 1), dtype="float32") = packed_params[262]
            decoder_model_3_block_4_block_1_weight_v2: R.Tensor((192, 192, 7), dtype="float32") = packed_params[263]
            decoder_model_3_block_4_block_1_bias2: R.Tensor((192,), dtype="float32") = packed_params[264]
            decoder_model_3_block_4_block_2_alpha2: R.Tensor((1, 192, 1), dtype="float32") = packed_params[265]
            decoder_model_3_block_4_block_3_weight_g2: R.Tensor((192, 1, 1), dtype="float32") = packed_params[266]
            decoder_model_3_block_4_block_3_weight_v2: R.Tensor((192, 192, 1), dtype="float32") = packed_params[267]
            decoder_model_3_block_4_block_3_bias2: R.Tensor((192,), dtype="float32") = packed_params[268]
            decoder_model_4_block_0_alpha2: R.Tensor((1, 192, 1), dtype="float32") = packed_params[269]
            decoder_model_4_block_1_weight_g2: R.Tensor((192, 1, 1), dtype="float32") = packed_params[270]
            decoder_model_4_block_1_weight_v2: R.Tensor((192, 96, 4), dtype="float32") = packed_params[271]
            decoder_model_4_block_1_bias2: R.Tensor((96,), dtype="float32") = packed_params[272]
            decoder_model_4_block_2_block_0_alpha2: R.Tensor((1, 96, 1), dtype="float32") = packed_params[273]
            decoder_model_4_block_2_block_1_weight_g2: R.Tensor((96, 1, 1), dtype="float32") = packed_params[274]
            decoder_model_4_block_2_block_1_weight_v2: R.Tensor((96, 96, 7), dtype="float32") = packed_params[275]
            decoder_model_4_block_2_block_1_bias2: R.Tensor((96,), dtype="float32") = packed_params[276]
            decoder_model_4_block_2_block_2_alpha2: R.Tensor((1, 96, 1), dtype="float32") = packed_params[277]
            decoder_model_4_block_2_block_3_weight_g2: R.Tensor((96, 1, 1), dtype="float32") = packed_params[278]
            decoder_model_4_block_2_block_3_weight_v2: R.Tensor((96, 96, 1), dtype="float32") = packed_params[279]
            decoder_model_4_block_2_block_3_bias2: R.Tensor((96,), dtype="float32") = packed_params[280]
            decoder_model_4_block_3_block_0_alpha2: R.Tensor((1, 96, 1), dtype="float32") = packed_params[281]
            decoder_model_4_block_3_block_1_weight_g2: R.Tensor((96, 1, 1), dtype="float32") = packed_params[282]
            decoder_model_4_block_3_block_1_weight_v2: R.Tensor((96, 96, 7), dtype="float32") = packed_params[283]
            decoder_model_4_block_3_block_1_bias2: R.Tensor((96,), dtype="float32") = packed_params[284]
            decoder_model_4_block_3_block_2_alpha2: R.Tensor((1, 96, 1), dtype="float32") = packed_params[285]
            decoder_model_4_block_3_block_3_weight_g2: R.Tensor((96, 1, 1), dtype="float32") = packed_params[286]
            decoder_model_4_block_3_block_3_weight_v2: R.Tensor((96, 96, 1), dtype="float32") = packed_params[287]
            decoder_model_4_block_3_block_3_bias2: R.Tensor((96,), dtype="float32") = packed_params[288]
            decoder_model_4_block_4_block_0_alpha2: R.Tensor((1, 96, 1), dtype="float32") = packed_params[289]
            decoder_model_4_block_4_block_1_weight_g2: R.Tensor((96, 1, 1), dtype="float32") = packed_params[290]
            decoder_model_4_block_4_block_1_weight_v2: R.Tensor((96, 96, 7), dtype="float32") = packed_params[291]
            decoder_model_4_block_4_block_1_bias2: R.Tensor((96,), dtype="float32") = packed_params[292]
            decoder_model_4_block_4_block_2_alpha2: R.Tensor((1, 96, 1), dtype="float32") = packed_params[293]
            decoder_model_4_block_4_block_3_weight_g2: R.Tensor((96, 1, 1), dtype="float32") = packed_params[294]
            decoder_model_4_block_4_block_3_weight_v2: R.Tensor((96, 96, 1), dtype="float32") = packed_params[295]
            decoder_model_4_block_4_block_3_bias2: R.Tensor((96,), dtype="float32") = packed_params[296]
            decoder_model_5_alpha2: R.Tensor((1, 96, 1), dtype="float32") = packed_params[297]
            decoder_model_6_weight_g2: R.Tensor((1, 1, 1), dtype="float32") = packed_params[298]
            decoder_model_6_weight_v2: R.Tensor((1, 96, 7), dtype="float32") = packed_params[299]
            decoder_model_6_bias2: R.Tensor((1,), dtype="float32") = packed_params[300]
            lv = R.call_tir(cls.fused_tir_square_sum, (decoder_model_0_weight_v2,), out_sinfo=R.Tensor((1536, 1, 1), dtype="float32"))
            lv1 = R.call_tir(cls.fused_tir_sqrt_divide_multiply, (lv, decoder_model_0_weight_v2, decoder_model_0_weight_g2), out_sinfo=R.Tensor((1536, 1024, 7), dtype="float32"))
            lv848 = R.call_tir(cls.reshape, (decoder_model_0_bias2,), out_sinfo=R.Tensor((1, 1536, 1), dtype="float32"))
            lv2 = R.call_tir(cls.fused_conv1d_add, (z, lv1, lv848), out_sinfo=R.Tensor((batch_size, 1536, seq_len), dtype="float32"))
            reshape246 = R.call_tir(cls.reshape1, (lv2,), out_sinfo=R.Tensor((batch_size, 1536, seq_len), dtype="float32"))
            lv849 = R.call_tir(cls.snake17, (reshape246, decoder_model_1_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 1536, seq_len), dtype="float32"))
            reshape247 = R.call_tir(cls.reshape1, (lv849,), out_sinfo=R.Tensor((batch_size, 1536, seq_len), dtype="float32"))
            lv3 = R.call_tir(cls.fused_tir_square1_sum1, (decoder_model_1_block_1_weight_v2,), out_sinfo=R.Tensor((1536, 1, 1), dtype="float32"))
            lv4 = R.call_tir(cls.fused_tir_sqrt_divide1_multiply1, (lv3, decoder_model_1_block_1_weight_v2, decoder_model_1_block_1_weight_g2), out_sinfo=R.Tensor((1536, 768, 16), dtype="float32"))
            lv855 = R.call_tir(cls.reshape2, (decoder_model_1_block_1_bias2,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv5 = R.call_tir(cls.fused_conv1d_transpose_add1, (reshape247, lv4, lv855), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"))
            reshape248 = R.call_tir(cls.reshape3, (lv5,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv856 = R.call_tir(cls.snake18, (reshape248, decoder_model_1_block_2_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape249 = R.call_tir(cls.reshape3, (lv856,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv6 = R.call_tir(cls.fused_tir_square2_sum2, (decoder_model_1_block_2_block_1_weight_v2,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv7 = R.call_tir(cls.fused_tir_sqrt1_divide2_multiply2, (lv6, decoder_model_1_block_2_block_1_weight_v2, decoder_model_1_block_2_block_1_weight_g2), out_sinfo=R.Tensor((768, 768, 7), dtype="float32"))
            lv862 = R.call_tir(cls.reshape2, (decoder_model_1_block_2_block_1_bias2,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv8 = R.call_tir(cls.fused_conv1d1_add1, (reshape249, lv7, lv862), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape250 = R.call_tir(cls.reshape3, (lv8,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv863 = R.call_tir(cls.snake19, (reshape250, decoder_model_1_block_2_block_2_alpha2), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape251 = R.call_tir(cls.reshape3, (lv863,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv9 = R.call_tir(cls.fused_tir_square3_sum3, (decoder_model_1_block_2_block_3_weight_v2,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv10 = R.call_tir(cls.fused_tir_sqrt1_divide3_multiply3, (lv9, decoder_model_1_block_2_block_3_weight_v2, decoder_model_1_block_2_block_3_weight_g2), out_sinfo=R.Tensor((768, 768, 1), dtype="float32"))
            lv869 = R.call_tir(cls.reshape2, (decoder_model_1_block_2_block_3_bias2,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv11 = R.call_tir(cls.fused_conv1d2_add1_add2, (reshape251, lv10, lv869, lv5), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape252 = R.call_tir(cls.reshape3, (lv11,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv870 = R.call_tir(cls.snake19, (reshape252, decoder_model_1_block_3_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape253 = R.call_tir(cls.reshape3, (lv870,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv12 = R.call_tir(cls.fused_tir_square2_sum2, (decoder_model_1_block_3_block_1_weight_v2,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv13 = R.call_tir(cls.fused_tir_sqrt1_divide2_multiply2, (lv12, decoder_model_1_block_3_block_1_weight_v2, decoder_model_1_block_3_block_1_weight_g2), out_sinfo=R.Tensor((768, 768, 7), dtype="float32"))
            lv876 = R.call_tir(cls.reshape2, (decoder_model_1_block_3_block_1_bias2,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv14 = R.call_tir(cls.fused_conv1d3_add1, (reshape253, lv13, lv876), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape254 = R.call_tir(cls.reshape3, (lv14,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv877 = R.call_tir(cls.snake19, (reshape254, decoder_model_1_block_3_block_2_alpha2), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape255 = R.call_tir(cls.reshape3, (lv877,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv15 = R.call_tir(cls.fused_tir_square3_sum3, (decoder_model_1_block_3_block_3_weight_v2,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv16 = R.call_tir(cls.fused_tir_sqrt1_divide3_multiply3, (lv15, decoder_model_1_block_3_block_3_weight_v2, decoder_model_1_block_3_block_3_weight_g2), out_sinfo=R.Tensor((768, 768, 1), dtype="float32"))
            lv883 = R.call_tir(cls.reshape2, (decoder_model_1_block_3_block_3_bias2,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv17 = R.call_tir(cls.fused_conv1d2_add1_add2, (reshape255, lv16, lv883, lv11), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape256 = R.call_tir(cls.reshape3, (lv17,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv884 = R.call_tir(cls.snake18, (reshape256, decoder_model_1_block_4_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape257 = R.call_tir(cls.reshape3, (lv884,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv18 = R.call_tir(cls.fused_tir_square2_sum2, (decoder_model_1_block_4_block_1_weight_v2,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv19 = R.call_tir(cls.fused_tir_sqrt1_divide2_multiply2, (lv18, decoder_model_1_block_4_block_1_weight_v2, decoder_model_1_block_4_block_1_weight_g2), out_sinfo=R.Tensor((768, 768, 7), dtype="float32"))
            lv890 = R.call_tir(cls.reshape2, (decoder_model_1_block_4_block_1_bias2,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv20 = R.call_tir(cls.fused_conv1d4_add1, (reshape257, lv19, lv890), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape258 = R.call_tir(cls.reshape3, (lv20,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv891 = R.call_tir(cls.snake19, (reshape258, decoder_model_1_block_4_block_2_alpha2), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape259 = R.call_tir(cls.reshape3, (lv891,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv21 = R.call_tir(cls.fused_tir_square3_sum3, (decoder_model_1_block_4_block_3_weight_v2,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv22 = R.call_tir(cls.fused_tir_sqrt1_divide3_multiply3, (lv21, decoder_model_1_block_4_block_3_weight_v2, decoder_model_1_block_4_block_3_weight_g2), out_sinfo=R.Tensor((768, 768, 1), dtype="float32"))
            lv897 = R.call_tir(cls.reshape2, (decoder_model_1_block_4_block_3_bias2,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv23 = R.call_tir(cls.fused_conv1d2_add1_add2, (reshape259, lv22, lv897, lv17), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape260 = R.call_tir(cls.reshape3, (lv23,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv898 = R.call_tir(cls.snake18, (reshape260, decoder_model_2_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape261 = R.call_tir(cls.reshape3, (lv898,), out_sinfo=R.Tensor((batch_size, 768, seq_len * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv24 = R.call_tir(cls.fused_tir_square4_sum4, (decoder_model_2_block_1_weight_v2,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv25 = R.call_tir(cls.fused_tir_sqrt1_divide4_multiply4, (lv24, decoder_model_2_block_1_weight_v2, decoder_model_2_block_1_weight_g2), out_sinfo=R.Tensor((768, 384, 16), dtype="float32"))
            lv904 = R.call_tir(cls.reshape4, (decoder_model_2_block_1_bias2,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv26 = R.call_tir(cls.fused_conv1d_transpose1_add3, (reshape261, lv25, lv904), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape262 = R.call_tir(cls.reshape5, (lv26,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv905 = R.call_tir(cls.snake20, (reshape262, decoder_model_2_block_2_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape263 = R.call_tir(cls.reshape5, (lv905,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv27 = R.call_tir(cls.fused_tir_square5_sum5, (decoder_model_2_block_2_block_1_weight_v2,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv28 = R.call_tir(cls.fused_tir_sqrt2_divide5_multiply5, (lv27, decoder_model_2_block_2_block_1_weight_v2, decoder_model_2_block_2_block_1_weight_g2), out_sinfo=R.Tensor((384, 384, 7), dtype="float32"))
            lv911 = R.call_tir(cls.reshape4, (decoder_model_2_block_2_block_1_bias2,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv29 = R.call_tir(cls.fused_conv1d5_add3, (reshape263, lv28, lv911), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape264 = R.call_tir(cls.reshape5, (lv29,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv912 = R.call_tir(cls.snake20, (reshape264, decoder_model_2_block_2_block_2_alpha2), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape265 = R.call_tir(cls.reshape5, (lv912,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv30 = R.call_tir(cls.fused_tir_square6_sum6, (decoder_model_2_block_2_block_3_weight_v2,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv31 = R.call_tir(cls.fused_tir_sqrt2_divide6_multiply6, (lv30, decoder_model_2_block_2_block_3_weight_v2, decoder_model_2_block_2_block_3_weight_g2), out_sinfo=R.Tensor((384, 384, 1), dtype="float32"))
            lv918 = R.call_tir(cls.reshape4, (decoder_model_2_block_2_block_3_bias2,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv32 = R.call_tir(cls.fused_conv1d6_add3_add4, (reshape265, lv31, lv918, lv26), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape266 = R.call_tir(cls.reshape5, (lv32,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv919 = R.call_tir(cls.snake20, (reshape266, decoder_model_2_block_3_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape267 = R.call_tir(cls.reshape5, (lv919,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv33 = R.call_tir(cls.fused_tir_square5_sum5, (decoder_model_2_block_3_block_1_weight_v2,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv34 = R.call_tir(cls.fused_tir_sqrt2_divide5_multiply5, (lv33, decoder_model_2_block_3_block_1_weight_v2, decoder_model_2_block_3_block_1_weight_g2), out_sinfo=R.Tensor((384, 384, 7), dtype="float32"))
            lv925 = R.call_tir(cls.reshape4, (decoder_model_2_block_3_block_1_bias2,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv35 = R.call_tir(cls.fused_conv1d7_add3, (reshape267, lv34, lv925), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape268 = R.call_tir(cls.reshape5, (lv35,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv926 = R.call_tir(cls.snake21, (reshape268, decoder_model_2_block_3_block_2_alpha2), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape269 = R.call_tir(cls.reshape5, (lv926,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv36 = R.call_tir(cls.fused_tir_square6_sum6, (decoder_model_2_block_3_block_3_weight_v2,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv37 = R.call_tir(cls.fused_tir_sqrt2_divide6_multiply6, (lv36, decoder_model_2_block_3_block_3_weight_v2, decoder_model_2_block_3_block_3_weight_g2), out_sinfo=R.Tensor((384, 384, 1), dtype="float32"))
            lv932 = R.call_tir(cls.reshape4, (decoder_model_2_block_3_block_3_bias2,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv38 = R.call_tir(cls.fused_conv1d6_add3_add4, (reshape269, lv37, lv932, lv32), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape270 = R.call_tir(cls.reshape5, (lv38,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv933 = R.call_tir(cls.snake21, (reshape270, decoder_model_2_block_4_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape271 = R.call_tir(cls.reshape5, (lv933,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv39 = R.call_tir(cls.fused_tir_square5_sum5, (decoder_model_2_block_4_block_1_weight_v2,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv40 = R.call_tir(cls.fused_tir_sqrt2_divide5_multiply5, (lv39, decoder_model_2_block_4_block_1_weight_v2, decoder_model_2_block_4_block_1_weight_g2), out_sinfo=R.Tensor((384, 384, 7), dtype="float32"))
            lv939 = R.call_tir(cls.reshape4, (decoder_model_2_block_4_block_1_bias2,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv41 = R.call_tir(cls.fused_conv1d8_add3, (reshape271, lv40, lv939), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape272 = R.call_tir(cls.reshape5, (lv41,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv940 = R.call_tir(cls.snake21, (reshape272, decoder_model_2_block_4_block_2_alpha2), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape273 = R.call_tir(cls.reshape5, (lv940,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv42 = R.call_tir(cls.fused_tir_square6_sum6, (decoder_model_2_block_4_block_3_weight_v2,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv43 = R.call_tir(cls.fused_tir_sqrt2_divide6_multiply6, (lv42, decoder_model_2_block_4_block_3_weight_v2, decoder_model_2_block_4_block_3_weight_g2), out_sinfo=R.Tensor((384, 384, 1), dtype="float32"))
            lv946 = R.call_tir(cls.reshape4, (decoder_model_2_block_4_block_3_bias2,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv44 = R.call_tir(cls.fused_conv1d6_add3_add4, (reshape273, lv43, lv946, lv38), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape274 = R.call_tir(cls.reshape5, (lv44,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv947 = R.call_tir(cls.snake21, (reshape274, decoder_model_3_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape275 = R.call_tir(cls.reshape5, (lv947,), out_sinfo=R.Tensor((batch_size, 384, seq_len * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv45 = R.call_tir(cls.fused_tir_square7_sum7, (decoder_model_3_block_1_weight_v2,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv46 = R.call_tir(cls.fused_tir_sqrt2_divide7_multiply7, (lv45, decoder_model_3_block_1_weight_v2, decoder_model_3_block_1_weight_g2), out_sinfo=R.Tensor((384, 192, 8), dtype="float32"))
            lv953 = R.call_tir(cls.reshape6, (decoder_model_3_block_1_bias2,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv47 = R.call_tir(cls.fused_conv1d_transpose2_add5, (reshape275, lv46, lv953), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape276 = R.call_tir(cls.reshape7, (lv47,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv954 = R.call_tir(cls.snake22, (reshape276, decoder_model_3_block_2_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape277 = R.call_tir(cls.reshape7, (lv954,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv48 = R.call_tir(cls.fused_tir_square8_sum8, (decoder_model_3_block_2_block_1_weight_v2,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv49 = R.call_tir(cls.fused_tir_sqrt3_divide8_multiply8, (lv48, decoder_model_3_block_2_block_1_weight_v2, decoder_model_3_block_2_block_1_weight_g2), out_sinfo=R.Tensor((192, 192, 7), dtype="float32"))
            lv960 = R.call_tir(cls.reshape6, (decoder_model_3_block_2_block_1_bias2,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv50 = R.call_tir(cls.fused_conv1d9_add5, (reshape277, lv49, lv960), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape278 = R.call_tir(cls.reshape7, (lv50,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv961 = R.call_tir(cls.snake22, (reshape278, decoder_model_3_block_2_block_2_alpha2), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape279 = R.call_tir(cls.reshape7, (lv961,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv51 = R.call_tir(cls.fused_tir_square9_sum9, (decoder_model_3_block_2_block_3_weight_v2,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv52 = R.call_tir(cls.fused_tir_sqrt3_divide9_multiply9, (lv51, decoder_model_3_block_2_block_3_weight_v2, decoder_model_3_block_2_block_3_weight_g2), out_sinfo=R.Tensor((192, 192, 1), dtype="float32"))
            lv967 = R.call_tir(cls.reshape6, (decoder_model_3_block_2_block_3_bias2,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv53 = R.call_tir(cls.fused_conv1d10_add5_add6, (reshape279, lv52, lv967, lv47), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape280 = R.call_tir(cls.reshape7, (lv53,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv968 = R.call_tir(cls.snake23, (reshape280, decoder_model_3_block_3_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape281 = R.call_tir(cls.reshape7, (lv968,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv54 = R.call_tir(cls.fused_tir_square8_sum8, (decoder_model_3_block_3_block_1_weight_v2,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv55 = R.call_tir(cls.fused_tir_sqrt3_divide8_multiply8, (lv54, decoder_model_3_block_3_block_1_weight_v2, decoder_model_3_block_3_block_1_weight_g2), out_sinfo=R.Tensor((192, 192, 7), dtype="float32"))
            lv974 = R.call_tir(cls.reshape6, (decoder_model_3_block_3_block_1_bias2,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv56 = R.call_tir(cls.fused_conv1d11_add5, (reshape281, lv55, lv974), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape282 = R.call_tir(cls.reshape7, (lv56,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv975 = R.call_tir(cls.snake23, (reshape282, decoder_model_3_block_3_block_2_alpha2), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape283 = R.call_tir(cls.reshape7, (lv975,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv57 = R.call_tir(cls.fused_tir_square9_sum9, (decoder_model_3_block_3_block_3_weight_v2,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv58 = R.call_tir(cls.fused_tir_sqrt3_divide9_multiply9, (lv57, decoder_model_3_block_3_block_3_weight_v2, decoder_model_3_block_3_block_3_weight_g2), out_sinfo=R.Tensor((192, 192, 1), dtype="float32"))
            lv981 = R.call_tir(cls.reshape6, (decoder_model_3_block_3_block_3_bias2,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv59 = R.call_tir(cls.fused_conv1d10_add5_add6, (reshape283, lv58, lv981, lv53), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape284 = R.call_tir(cls.reshape7, (lv59,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv982 = R.call_tir(cls.snake22, (reshape284, decoder_model_3_block_4_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape285 = R.call_tir(cls.reshape7, (lv982,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv60 = R.call_tir(cls.fused_tir_square8_sum8, (decoder_model_3_block_4_block_1_weight_v2,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv61 = R.call_tir(cls.fused_tir_sqrt3_divide8_multiply8, (lv60, decoder_model_3_block_4_block_1_weight_v2, decoder_model_3_block_4_block_1_weight_g2), out_sinfo=R.Tensor((192, 192, 7), dtype="float32"))
            lv988 = R.call_tir(cls.reshape6, (decoder_model_3_block_4_block_1_bias2,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv62 = R.call_tir(cls.fused_conv1d12_add5, (reshape285, lv61, lv988), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape286 = R.call_tir(cls.reshape7, (lv62,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv989 = R.call_tir(cls.snake23, (reshape286, decoder_model_3_block_4_block_2_alpha2), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape287 = R.call_tir(cls.reshape7, (lv989,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv63 = R.call_tir(cls.fused_tir_square9_sum9, (decoder_model_3_block_4_block_3_weight_v2,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv64 = R.call_tir(cls.fused_tir_sqrt3_divide9_multiply9, (lv63, decoder_model_3_block_4_block_3_weight_v2, decoder_model_3_block_4_block_3_weight_g2), out_sinfo=R.Tensor((192, 192, 1), dtype="float32"))
            lv995 = R.call_tir(cls.reshape6, (decoder_model_3_block_4_block_3_bias2,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv65 = R.call_tir(cls.fused_conv1d10_add5_add6, (reshape287, lv64, lv995, lv59), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape288 = R.call_tir(cls.reshape7, (lv65,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv996 = R.call_tir(cls.snake23, (reshape288, decoder_model_4_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape289 = R.call_tir(cls.reshape7, (lv996,), out_sinfo=R.Tensor((batch_size, 192, seq_len * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv66 = R.call_tir(cls.fused_tir_square10_sum10, (decoder_model_4_block_1_weight_v2,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv67 = R.call_tir(cls.fused_tir_sqrt3_divide10_multiply10, (lv66, decoder_model_4_block_1_weight_v2, decoder_model_4_block_1_weight_g2), out_sinfo=R.Tensor((192, 96, 4), dtype="float32"))
            lv1002 = R.call_tir(cls.reshape8, (decoder_model_4_block_1_bias2,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv68 = R.call_tir(cls.fused_conv1d_transpose3_add7, (reshape289, lv67, lv1002), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape290 = R.call_tir(cls.reshape9, (lv68,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv1003 = R.call_tir(cls.snake24, (reshape290, decoder_model_4_block_2_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape291 = R.call_tir(cls.reshape9, (lv1003,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv69 = R.call_tir(cls.fused_tir_square11_sum11, (decoder_model_4_block_2_block_1_weight_v2,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv70 = R.call_tir(cls.fused_tir_sqrt4_divide11_multiply11, (lv69, decoder_model_4_block_2_block_1_weight_v2, decoder_model_4_block_2_block_1_weight_g2), out_sinfo=R.Tensor((96, 96, 7), dtype="float32"))
            lv1009 = R.call_tir(cls.reshape8, (decoder_model_4_block_2_block_1_bias2,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv71 = R.call_tir(cls.fused_conv1d13_add7, (reshape291, lv70, lv1009), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape292 = R.call_tir(cls.reshape9, (lv71,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv1010 = R.call_tir(cls.snake24, (reshape292, decoder_model_4_block_2_block_2_alpha2), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape293 = R.call_tir(cls.reshape9, (lv1010,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv72 = R.call_tir(cls.fused_tir_square12_sum12, (decoder_model_4_block_2_block_3_weight_v2,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv73 = R.call_tir(cls.fused_tir_sqrt4_divide12_multiply12, (lv72, decoder_model_4_block_2_block_3_weight_v2, decoder_model_4_block_2_block_3_weight_g2), out_sinfo=R.Tensor((96, 96, 1), dtype="float32"))
            lv1016 = R.call_tir(cls.reshape8, (decoder_model_4_block_2_block_3_bias2,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv74 = R.call_tir(cls.fused_conv1d14_add7_add8, (reshape293, lv73, lv1016, lv68), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape294 = R.call_tir(cls.reshape9, (lv74,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv1017 = R.call_tir(cls.snake24, (reshape294, decoder_model_4_block_3_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape295 = R.call_tir(cls.reshape9, (lv1017,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv75 = R.call_tir(cls.fused_tir_square11_sum11, (decoder_model_4_block_3_block_1_weight_v2,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv76 = R.call_tir(cls.fused_tir_sqrt4_divide11_multiply11, (lv75, decoder_model_4_block_3_block_1_weight_v2, decoder_model_4_block_3_block_1_weight_g2), out_sinfo=R.Tensor((96, 96, 7), dtype="float32"))
            lv1023 = R.call_tir(cls.reshape8, (decoder_model_4_block_3_block_1_bias2,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv77 = R.call_tir(cls.fused_conv1d15_add7, (reshape295, lv76, lv1023), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape296 = R.call_tir(cls.reshape9, (lv77,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv1024 = R.call_tir(cls.snake24, (reshape296, decoder_model_4_block_3_block_2_alpha2), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape297 = R.call_tir(cls.reshape9, (lv1024,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv78 = R.call_tir(cls.fused_tir_square12_sum12, (decoder_model_4_block_3_block_3_weight_v2,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv79 = R.call_tir(cls.fused_tir_sqrt4_divide12_multiply12, (lv78, decoder_model_4_block_3_block_3_weight_v2, decoder_model_4_block_3_block_3_weight_g2), out_sinfo=R.Tensor((96, 96, 1), dtype="float32"))
            lv1030 = R.call_tir(cls.reshape8, (decoder_model_4_block_3_block_3_bias2,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv80 = R.call_tir(cls.fused_conv1d14_add7_add8, (reshape297, lv79, lv1030, lv74), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape298 = R.call_tir(cls.reshape9, (lv80,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv1031 = R.call_tir(cls.snake24, (reshape298, decoder_model_4_block_4_block_0_alpha2), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape299 = R.call_tir(cls.reshape9, (lv1031,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv81 = R.call_tir(cls.fused_tir_square11_sum11, (decoder_model_4_block_4_block_1_weight_v2,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv82 = R.call_tir(cls.fused_tir_sqrt4_divide11_multiply11, (lv81, decoder_model_4_block_4_block_1_weight_v2, decoder_model_4_block_4_block_1_weight_g2), out_sinfo=R.Tensor((96, 96, 7), dtype="float32"))
            lv1037 = R.call_tir(cls.reshape8, (decoder_model_4_block_4_block_1_bias2,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv83 = R.call_tir(cls.fused_conv1d16_add7, (reshape299, lv82, lv1037), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape300 = R.call_tir(cls.reshape9, (lv83,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv1038 = R.call_tir(cls.snake24, (reshape300, decoder_model_4_block_4_block_2_alpha2), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape301 = R.call_tir(cls.reshape9, (lv1038,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv84 = R.call_tir(cls.fused_tir_square12_sum12, (decoder_model_4_block_4_block_3_weight_v2,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv85 = R.call_tir(cls.fused_tir_sqrt4_divide12_multiply12, (lv84, decoder_model_4_block_4_block_3_weight_v2, decoder_model_4_block_4_block_3_weight_g2), out_sinfo=R.Tensor((96, 96, 1), dtype="float32"))
            lv1044 = R.call_tir(cls.reshape8, (decoder_model_4_block_4_block_3_bias2,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv86 = R.call_tir(cls.fused_conv1d14_add7_add8, (reshape301, lv85, lv1044, lv80), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape302 = R.call_tir(cls.reshape9, (lv86,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv1045 = R.call_tir(cls.snake24, (reshape302, decoder_model_5_alpha2), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape303 = R.call_tir(cls.reshape9, (lv1045,), out_sinfo=R.Tensor((batch_size, 96, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv87 = R.call_tir(cls.fused_tir_square13_sum13, (decoder_model_6_weight_v2,), out_sinfo=R.Tensor((1, 1, 1), dtype="float32"))
            lv88 = R.call_tir(cls.fused_tir_sqrt5_divide13_multiply13, (lv87, decoder_model_6_weight_v2, decoder_model_6_weight_g2), out_sinfo=R.Tensor((1, 96, 7), dtype="float32"))
            gv2 = R.call_tir(cls.fused_conv1d17_reshape10_add9_tir_tanh, (reshape303, lv88, decoder_model_6_bias2), out_sinfo=R.Tensor((batch_size, 1, seq_len * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            R.output(gv2)
        return gv2

    @R.function
    def encode(audio_data: R.Tensor(("batch_size", 1, "seq_len"), dtype="float32"), packed_params: R.Tuple(R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 1, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 64, 4), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 128, 8), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 256, 16), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 512, 16), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1, 1024, 1), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 1024, 3), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((1536, 1, 1), dtype="float32"), R.Tensor((1536, 1024, 7), dtype="float32"), R.Tensor((1536,), dtype="float32"), R.Tensor((1, 1536, 1), dtype="float32"), R.Tensor((1536, 1, 1), dtype="float32"), R.Tensor((1536, 768, 16), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 384, 16), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 192, 8), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 96, 4), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((1, 1, 1), dtype="float32"), R.Tensor((1, 96, 7), dtype="float32"), R.Tensor((1,), dtype="float32"))) -> R.Tuple(R.Tensor(("batch_size", 1024, "seq_len // 512"), dtype="float32"), R.Tuple(R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"))):
        batch_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 1})
        cls = Module
        with R.dataflow():
            encoder_block_0_weight_g1: R.Tensor((64, 1, 1), dtype="float32") = packed_params[0]
            encoder_block_0_weight_v1: R.Tensor((64, 1, 7), dtype="float32") = packed_params[1]
            encoder_block_0_bias1: R.Tensor((64,), dtype="float32") = packed_params[2]
            encoder_block_1_block_0_block_0_alpha1: R.Tensor((1, 64, 1), dtype="float32") = packed_params[3]
            encoder_block_1_block_0_block_1_weight_g1: R.Tensor((64, 1, 1), dtype="float32") = packed_params[4]
            encoder_block_1_block_0_block_1_weight_v1: R.Tensor((64, 64, 7), dtype="float32") = packed_params[5]
            encoder_block_1_block_0_block_1_bias1: R.Tensor((64,), dtype="float32") = packed_params[6]
            encoder_block_1_block_0_block_2_alpha1: R.Tensor((1, 64, 1), dtype="float32") = packed_params[7]
            encoder_block_1_block_0_block_3_weight_g1: R.Tensor((64, 1, 1), dtype="float32") = packed_params[8]
            encoder_block_1_block_0_block_3_weight_v1: R.Tensor((64, 64, 1), dtype="float32") = packed_params[9]
            encoder_block_1_block_0_block_3_bias1: R.Tensor((64,), dtype="float32") = packed_params[10]
            encoder_block_1_block_1_block_0_alpha1: R.Tensor((1, 64, 1), dtype="float32") = packed_params[11]
            encoder_block_1_block_1_block_1_weight_g1: R.Tensor((64, 1, 1), dtype="float32") = packed_params[12]
            encoder_block_1_block_1_block_1_weight_v1: R.Tensor((64, 64, 7), dtype="float32") = packed_params[13]
            encoder_block_1_block_1_block_1_bias1: R.Tensor((64,), dtype="float32") = packed_params[14]
            encoder_block_1_block_1_block_2_alpha1: R.Tensor((1, 64, 1), dtype="float32") = packed_params[15]
            encoder_block_1_block_1_block_3_weight_g1: R.Tensor((64, 1, 1), dtype="float32") = packed_params[16]
            encoder_block_1_block_1_block_3_weight_v1: R.Tensor((64, 64, 1), dtype="float32") = packed_params[17]
            encoder_block_1_block_1_block_3_bias1: R.Tensor((64,), dtype="float32") = packed_params[18]
            encoder_block_1_block_2_block_0_alpha1: R.Tensor((1, 64, 1), dtype="float32") = packed_params[19]
            encoder_block_1_block_2_block_1_weight_g1: R.Tensor((64, 1, 1), dtype="float32") = packed_params[20]
            encoder_block_1_block_2_block_1_weight_v1: R.Tensor((64, 64, 7), dtype="float32") = packed_params[21]
            encoder_block_1_block_2_block_1_bias1: R.Tensor((64,), dtype="float32") = packed_params[22]
            encoder_block_1_block_2_block_2_alpha1: R.Tensor((1, 64, 1), dtype="float32") = packed_params[23]
            encoder_block_1_block_2_block_3_weight_g1: R.Tensor((64, 1, 1), dtype="float32") = packed_params[24]
            encoder_block_1_block_2_block_3_weight_v1: R.Tensor((64, 64, 1), dtype="float32") = packed_params[25]
            encoder_block_1_block_2_block_3_bias1: R.Tensor((64,), dtype="float32") = packed_params[26]
            encoder_block_1_block_3_alpha1: R.Tensor((1, 64, 1), dtype="float32") = packed_params[27]
            encoder_block_1_block_4_weight_g1: R.Tensor((128, 1, 1), dtype="float32") = packed_params[28]
            encoder_block_1_block_4_weight_v1: R.Tensor((128, 64, 4), dtype="float32") = packed_params[29]
            encoder_block_1_block_4_bias1: R.Tensor((128,), dtype="float32") = packed_params[30]
            encoder_block_2_block_0_block_0_alpha1: R.Tensor((1, 128, 1), dtype="float32") = packed_params[31]
            encoder_block_2_block_0_block_1_weight_g1: R.Tensor((128, 1, 1), dtype="float32") = packed_params[32]
            encoder_block_2_block_0_block_1_weight_v1: R.Tensor((128, 128, 7), dtype="float32") = packed_params[33]
            encoder_block_2_block_0_block_1_bias1: R.Tensor((128,), dtype="float32") = packed_params[34]
            encoder_block_2_block_0_block_2_alpha1: R.Tensor((1, 128, 1), dtype="float32") = packed_params[35]
            encoder_block_2_block_0_block_3_weight_g1: R.Tensor((128, 1, 1), dtype="float32") = packed_params[36]
            encoder_block_2_block_0_block_3_weight_v1: R.Tensor((128, 128, 1), dtype="float32") = packed_params[37]
            encoder_block_2_block_0_block_3_bias1: R.Tensor((128,), dtype="float32") = packed_params[38]
            encoder_block_2_block_1_block_0_alpha1: R.Tensor((1, 128, 1), dtype="float32") = packed_params[39]
            encoder_block_2_block_1_block_1_weight_g1: R.Tensor((128, 1, 1), dtype="float32") = packed_params[40]
            encoder_block_2_block_1_block_1_weight_v1: R.Tensor((128, 128, 7), dtype="float32") = packed_params[41]
            encoder_block_2_block_1_block_1_bias1: R.Tensor((128,), dtype="float32") = packed_params[42]
            encoder_block_2_block_1_block_2_alpha1: R.Tensor((1, 128, 1), dtype="float32") = packed_params[43]
            encoder_block_2_block_1_block_3_weight_g1: R.Tensor((128, 1, 1), dtype="float32") = packed_params[44]
            encoder_block_2_block_1_block_3_weight_v1: R.Tensor((128, 128, 1), dtype="float32") = packed_params[45]
            encoder_block_2_block_1_block_3_bias1: R.Tensor((128,), dtype="float32") = packed_params[46]
            encoder_block_2_block_2_block_0_alpha1: R.Tensor((1, 128, 1), dtype="float32") = packed_params[47]
            encoder_block_2_block_2_block_1_weight_g1: R.Tensor((128, 1, 1), dtype="float32") = packed_params[48]
            encoder_block_2_block_2_block_1_weight_v1: R.Tensor((128, 128, 7), dtype="float32") = packed_params[49]
            encoder_block_2_block_2_block_1_bias1: R.Tensor((128,), dtype="float32") = packed_params[50]
            encoder_block_2_block_2_block_2_alpha1: R.Tensor((1, 128, 1), dtype="float32") = packed_params[51]
            encoder_block_2_block_2_block_3_weight_g1: R.Tensor((128, 1, 1), dtype="float32") = packed_params[52]
            encoder_block_2_block_2_block_3_weight_v1: R.Tensor((128, 128, 1), dtype="float32") = packed_params[53]
            encoder_block_2_block_2_block_3_bias1: R.Tensor((128,), dtype="float32") = packed_params[54]
            encoder_block_2_block_3_alpha1: R.Tensor((1, 128, 1), dtype="float32") = packed_params[55]
            encoder_block_2_block_4_weight_g1: R.Tensor((256, 1, 1), dtype="float32") = packed_params[56]
            encoder_block_2_block_4_weight_v1: R.Tensor((256, 128, 8), dtype="float32") = packed_params[57]
            encoder_block_2_block_4_bias1: R.Tensor((256,), dtype="float32") = packed_params[58]
            encoder_block_3_block_0_block_0_alpha1: R.Tensor((1, 256, 1), dtype="float32") = packed_params[59]
            encoder_block_3_block_0_block_1_weight_g1: R.Tensor((256, 1, 1), dtype="float32") = packed_params[60]
            encoder_block_3_block_0_block_1_weight_v1: R.Tensor((256, 256, 7), dtype="float32") = packed_params[61]
            encoder_block_3_block_0_block_1_bias1: R.Tensor((256,), dtype="float32") = packed_params[62]
            encoder_block_3_block_0_block_2_alpha1: R.Tensor((1, 256, 1), dtype="float32") = packed_params[63]
            encoder_block_3_block_0_block_3_weight_g1: R.Tensor((256, 1, 1), dtype="float32") = packed_params[64]
            encoder_block_3_block_0_block_3_weight_v1: R.Tensor((256, 256, 1), dtype="float32") = packed_params[65]
            encoder_block_3_block_0_block_3_bias1: R.Tensor((256,), dtype="float32") = packed_params[66]
            encoder_block_3_block_1_block_0_alpha1: R.Tensor((1, 256, 1), dtype="float32") = packed_params[67]
            encoder_block_3_block_1_block_1_weight_g1: R.Tensor((256, 1, 1), dtype="float32") = packed_params[68]
            encoder_block_3_block_1_block_1_weight_v1: R.Tensor((256, 256, 7), dtype="float32") = packed_params[69]
            encoder_block_3_block_1_block_1_bias1: R.Tensor((256,), dtype="float32") = packed_params[70]
            encoder_block_3_block_1_block_2_alpha1: R.Tensor((1, 256, 1), dtype="float32") = packed_params[71]
            encoder_block_3_block_1_block_3_weight_g1: R.Tensor((256, 1, 1), dtype="float32") = packed_params[72]
            encoder_block_3_block_1_block_3_weight_v1: R.Tensor((256, 256, 1), dtype="float32") = packed_params[73]
            encoder_block_3_block_1_block_3_bias1: R.Tensor((256,), dtype="float32") = packed_params[74]
            encoder_block_3_block_2_block_0_alpha1: R.Tensor((1, 256, 1), dtype="float32") = packed_params[75]
            encoder_block_3_block_2_block_1_weight_g1: R.Tensor((256, 1, 1), dtype="float32") = packed_params[76]
            encoder_block_3_block_2_block_1_weight_v1: R.Tensor((256, 256, 7), dtype="float32") = packed_params[77]
            encoder_block_3_block_2_block_1_bias1: R.Tensor((256,), dtype="float32") = packed_params[78]
            encoder_block_3_block_2_block_2_alpha1: R.Tensor((1, 256, 1), dtype="float32") = packed_params[79]
            encoder_block_3_block_2_block_3_weight_g1: R.Tensor((256, 1, 1), dtype="float32") = packed_params[80]
            encoder_block_3_block_2_block_3_weight_v1: R.Tensor((256, 256, 1), dtype="float32") = packed_params[81]
            encoder_block_3_block_2_block_3_bias1: R.Tensor((256,), dtype="float32") = packed_params[82]
            encoder_block_3_block_3_alpha1: R.Tensor((1, 256, 1), dtype="float32") = packed_params[83]
            encoder_block_3_block_4_weight_g1: R.Tensor((512, 1, 1), dtype="float32") = packed_params[84]
            encoder_block_3_block_4_weight_v1: R.Tensor((512, 256, 16), dtype="float32") = packed_params[85]
            encoder_block_3_block_4_bias1: R.Tensor((512,), dtype="float32") = packed_params[86]
            encoder_block_4_block_0_block_0_alpha1: R.Tensor((1, 512, 1), dtype="float32") = packed_params[87]
            encoder_block_4_block_0_block_1_weight_g1: R.Tensor((512, 1, 1), dtype="float32") = packed_params[88]
            encoder_block_4_block_0_block_1_weight_v1: R.Tensor((512, 512, 7), dtype="float32") = packed_params[89]
            encoder_block_4_block_0_block_1_bias1: R.Tensor((512,), dtype="float32") = packed_params[90]
            encoder_block_4_block_0_block_2_alpha1: R.Tensor((1, 512, 1), dtype="float32") = packed_params[91]
            encoder_block_4_block_0_block_3_weight_g1: R.Tensor((512, 1, 1), dtype="float32") = packed_params[92]
            encoder_block_4_block_0_block_3_weight_v1: R.Tensor((512, 512, 1), dtype="float32") = packed_params[93]
            encoder_block_4_block_0_block_3_bias1: R.Tensor((512,), dtype="float32") = packed_params[94]
            encoder_block_4_block_1_block_0_alpha1: R.Tensor((1, 512, 1), dtype="float32") = packed_params[95]
            encoder_block_4_block_1_block_1_weight_g1: R.Tensor((512, 1, 1), dtype="float32") = packed_params[96]
            encoder_block_4_block_1_block_1_weight_v1: R.Tensor((512, 512, 7), dtype="float32") = packed_params[97]
            encoder_block_4_block_1_block_1_bias1: R.Tensor((512,), dtype="float32") = packed_params[98]
            encoder_block_4_block_1_block_2_alpha1: R.Tensor((1, 512, 1), dtype="float32") = packed_params[99]
            encoder_block_4_block_1_block_3_weight_g1: R.Tensor((512, 1, 1), dtype="float32") = packed_params[100]
            encoder_block_4_block_1_block_3_weight_v1: R.Tensor((512, 512, 1), dtype="float32") = packed_params[101]
            encoder_block_4_block_1_block_3_bias1: R.Tensor((512,), dtype="float32") = packed_params[102]
            encoder_block_4_block_2_block_0_alpha1: R.Tensor((1, 512, 1), dtype="float32") = packed_params[103]
            encoder_block_4_block_2_block_1_weight_g1: R.Tensor((512, 1, 1), dtype="float32") = packed_params[104]
            encoder_block_4_block_2_block_1_weight_v1: R.Tensor((512, 512, 7), dtype="float32") = packed_params[105]
            encoder_block_4_block_2_block_1_bias1: R.Tensor((512,), dtype="float32") = packed_params[106]
            encoder_block_4_block_2_block_2_alpha1: R.Tensor((1, 512, 1), dtype="float32") = packed_params[107]
            encoder_block_4_block_2_block_3_weight_g1: R.Tensor((512, 1, 1), dtype="float32") = packed_params[108]
            encoder_block_4_block_2_block_3_weight_v1: R.Tensor((512, 512, 1), dtype="float32") = packed_params[109]
            encoder_block_4_block_2_block_3_bias1: R.Tensor((512,), dtype="float32") = packed_params[110]
            encoder_block_4_block_3_alpha1: R.Tensor((1, 512, 1), dtype="float32") = packed_params[111]
            encoder_block_4_block_4_weight_g1: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[112]
            encoder_block_4_block_4_weight_v1: R.Tensor((1024, 512, 16), dtype="float32") = packed_params[113]
            encoder_block_4_block_4_bias1: R.Tensor((1024,), dtype="float32") = packed_params[114]
            encoder_block_5_alpha1: R.Tensor((1, 1024, 1), dtype="float32") = packed_params[115]
            encoder_block_6_weight_g1: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[116]
            encoder_block_6_weight_v1: R.Tensor((1024, 1024, 3), dtype="float32") = packed_params[117]
            encoder_block_6_bias1: R.Tensor((1024,), dtype="float32") = packed_params[118]
            quantizer_quantizers_0_in_proj_weight_g1: R.Tensor((8, 1, 1), dtype="float32") = packed_params[119]
            quantizer_quantizers_0_in_proj_weight_v1: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[120]
            quantizer_quantizers_0_in_proj_bias1: R.Tensor((8,), dtype="float32") = packed_params[121]
            quantizer_quantizers_0_out_proj_weight_g1: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[122]
            quantizer_quantizers_0_out_proj_weight_v1: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[123]
            quantizer_quantizers_0_out_proj_bias1: R.Tensor((1024,), dtype="float32") = packed_params[124]
            quantizer_quantizers_0_codebook_weight1: R.Tensor((1024, 8), dtype="float32") = packed_params[125]
            quantizer_quantizers_1_in_proj_weight_g1: R.Tensor((8, 1, 1), dtype="float32") = packed_params[126]
            quantizer_quantizers_1_in_proj_weight_v1: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[127]
            quantizer_quantizers_1_in_proj_bias1: R.Tensor((8,), dtype="float32") = packed_params[128]
            quantizer_quantizers_1_out_proj_weight_g1: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[129]
            quantizer_quantizers_1_out_proj_weight_v1: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[130]
            quantizer_quantizers_1_out_proj_bias1: R.Tensor((1024,), dtype="float32") = packed_params[131]
            quantizer_quantizers_1_codebook_weight1: R.Tensor((1024, 8), dtype="float32") = packed_params[132]
            quantizer_quantizers_2_in_proj_weight_g1: R.Tensor((8, 1, 1), dtype="float32") = packed_params[133]
            quantizer_quantizers_2_in_proj_weight_v1: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[134]
            quantizer_quantizers_2_in_proj_bias1: R.Tensor((8,), dtype="float32") = packed_params[135]
            quantizer_quantizers_2_out_proj_weight_g1: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[136]
            quantizer_quantizers_2_out_proj_weight_v1: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[137]
            quantizer_quantizers_2_out_proj_bias1: R.Tensor((1024,), dtype="float32") = packed_params[138]
            quantizer_quantizers_2_codebook_weight1: R.Tensor((1024, 8), dtype="float32") = packed_params[139]
            quantizer_quantizers_3_in_proj_weight_g1: R.Tensor((8, 1, 1), dtype="float32") = packed_params[140]
            quantizer_quantizers_3_in_proj_weight_v1: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[141]
            quantizer_quantizers_3_in_proj_bias1: R.Tensor((8,), dtype="float32") = packed_params[142]
            quantizer_quantizers_3_out_proj_weight_g1: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[143]
            quantizer_quantizers_3_out_proj_weight_v1: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[144]
            quantizer_quantizers_3_out_proj_bias1: R.Tensor((1024,), dtype="float32") = packed_params[145]
            quantizer_quantizers_3_codebook_weight1: R.Tensor((1024, 8), dtype="float32") = packed_params[146]
            quantizer_quantizers_4_in_proj_weight_g1: R.Tensor((8, 1, 1), dtype="float32") = packed_params[147]
            quantizer_quantizers_4_in_proj_weight_v1: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[148]
            quantizer_quantizers_4_in_proj_bias1: R.Tensor((8,), dtype="float32") = packed_params[149]
            quantizer_quantizers_4_out_proj_weight_g1: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[150]
            quantizer_quantizers_4_out_proj_weight_v1: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[151]
            quantizer_quantizers_4_out_proj_bias1: R.Tensor((1024,), dtype="float32") = packed_params[152]
            quantizer_quantizers_4_codebook_weight1: R.Tensor((1024, 8), dtype="float32") = packed_params[153]
            quantizer_quantizers_5_in_proj_weight_g1: R.Tensor((8, 1, 1), dtype="float32") = packed_params[154]
            quantizer_quantizers_5_in_proj_weight_v1: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[155]
            quantizer_quantizers_5_in_proj_bias1: R.Tensor((8,), dtype="float32") = packed_params[156]
            quantizer_quantizers_5_out_proj_weight_g1: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[157]
            quantizer_quantizers_5_out_proj_weight_v1: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[158]
            quantizer_quantizers_5_out_proj_bias1: R.Tensor((1024,), dtype="float32") = packed_params[159]
            quantizer_quantizers_5_codebook_weight1: R.Tensor((1024, 8), dtype="float32") = packed_params[160]
            quantizer_quantizers_6_in_proj_weight_g1: R.Tensor((8, 1, 1), dtype="float32") = packed_params[161]
            quantizer_quantizers_6_in_proj_weight_v1: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[162]
            quantizer_quantizers_6_in_proj_bias1: R.Tensor((8,), dtype="float32") = packed_params[163]
            quantizer_quantizers_6_out_proj_weight_g1: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[164]
            quantizer_quantizers_6_out_proj_weight_v1: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[165]
            quantizer_quantizers_6_out_proj_bias1: R.Tensor((1024,), dtype="float32") = packed_params[166]
            quantizer_quantizers_6_codebook_weight1: R.Tensor((1024, 8), dtype="float32") = packed_params[167]
            quantizer_quantizers_7_in_proj_weight_g1: R.Tensor((8, 1, 1), dtype="float32") = packed_params[168]
            quantizer_quantizers_7_in_proj_weight_v1: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[169]
            quantizer_quantizers_7_in_proj_bias1: R.Tensor((8,), dtype="float32") = packed_params[170]
            quantizer_quantizers_7_out_proj_weight_g1: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[171]
            quantizer_quantizers_7_out_proj_weight_v1: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[172]
            quantizer_quantizers_7_out_proj_bias1: R.Tensor((1024,), dtype="float32") = packed_params[173]
            quantizer_quantizers_7_codebook_weight1: R.Tensor((1024, 8), dtype="float32") = packed_params[174]
            quantizer_quantizers_8_in_proj_weight_g1: R.Tensor((8, 1, 1), dtype="float32") = packed_params[175]
            quantizer_quantizers_8_in_proj_weight_v1: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[176]
            quantizer_quantizers_8_in_proj_bias1: R.Tensor((8,), dtype="float32") = packed_params[177]
            quantizer_quantizers_8_out_proj_weight_g1: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[178]
            quantizer_quantizers_8_out_proj_weight_v1: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[179]
            quantizer_quantizers_8_out_proj_bias1: R.Tensor((1024,), dtype="float32") = packed_params[180]
            quantizer_quantizers_8_codebook_weight1: R.Tensor((1024, 8), dtype="float32") = packed_params[181]
            lv90 = R.call_tir(cls.fused_tir_square14_sum14, (encoder_block_0_weight_v1,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv91 = R.call_tir(cls.fused_tir_sqrt6_divide14_multiply14, (lv90, encoder_block_0_weight_v1, encoder_block_0_weight_g1), out_sinfo=R.Tensor((64, 1, 7), dtype="float32"))
            lv531 = R.call_tir(cls.reshape11, (encoder_block_0_bias1,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv92 = R.call_tir(cls.fused_conv1d18_add10, (audio_data, lv91, lv531), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape152 = R.call_tir(cls.reshape12, (lv92,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv532 = R.call_tir(cls.snake, (reshape152, encoder_block_1_block_0_block_0_alpha1), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape153 = R.call_tir(cls.reshape12, (lv532,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv93 = R.call_tir(cls.fused_tir_square15_sum15, (encoder_block_1_block_0_block_1_weight_v1,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv94 = R.call_tir(cls.fused_tir_sqrt6_divide15_multiply15, (lv93, encoder_block_1_block_0_block_1_weight_v1, encoder_block_1_block_0_block_1_weight_g1), out_sinfo=R.Tensor((64, 64, 7), dtype="float32"))
            lv538 = R.call_tir(cls.reshape11, (encoder_block_1_block_0_block_1_bias1,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv95 = R.call_tir(cls.fused_conv1d19_add10, (reshape153, lv94, lv538), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape154 = R.call_tir(cls.reshape12, (lv95,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv539 = R.call_tir(cls.snake, (reshape154, encoder_block_1_block_0_block_2_alpha1), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape155 = R.call_tir(cls.reshape12, (lv539,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv96 = R.call_tir(cls.fused_tir_square16_sum16, (encoder_block_1_block_0_block_3_weight_v1,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv97 = R.call_tir(cls.fused_tir_sqrt6_divide16_multiply16, (lv96, encoder_block_1_block_0_block_3_weight_v1, encoder_block_1_block_0_block_3_weight_g1), out_sinfo=R.Tensor((64, 64, 1), dtype="float32"))
            lv545 = R.call_tir(cls.reshape11, (encoder_block_1_block_0_block_3_bias1,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv98 = R.call_tir(cls.fused_conv1d20_add10_add11, (reshape155, lv97, lv545, lv92), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape156 = R.call_tir(cls.reshape12, (lv98,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv546 = R.call_tir(cls.snake, (reshape156, encoder_block_1_block_1_block_0_alpha1), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape157 = R.call_tir(cls.reshape12, (lv546,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv99 = R.call_tir(cls.fused_tir_square15_sum15, (encoder_block_1_block_1_block_1_weight_v1,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv100 = R.call_tir(cls.fused_tir_sqrt6_divide15_multiply15, (lv99, encoder_block_1_block_1_block_1_weight_v1, encoder_block_1_block_1_block_1_weight_g1), out_sinfo=R.Tensor((64, 64, 7), dtype="float32"))
            lv552 = R.call_tir(cls.reshape11, (encoder_block_1_block_1_block_1_bias1,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv101 = R.call_tir(cls.fused_conv1d21_add10, (reshape157, lv100, lv552), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape158 = R.call_tir(cls.reshape12, (lv101,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv553 = R.call_tir(cls.snake, (reshape158, encoder_block_1_block_1_block_2_alpha1), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape159 = R.call_tir(cls.reshape12, (lv553,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv102 = R.call_tir(cls.fused_tir_square16_sum16, (encoder_block_1_block_1_block_3_weight_v1,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv103 = R.call_tir(cls.fused_tir_sqrt6_divide16_multiply16, (lv102, encoder_block_1_block_1_block_3_weight_v1, encoder_block_1_block_1_block_3_weight_g1), out_sinfo=R.Tensor((64, 64, 1), dtype="float32"))
            lv559 = R.call_tir(cls.reshape11, (encoder_block_1_block_1_block_3_bias1,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv104 = R.call_tir(cls.fused_conv1d20_add10_add11, (reshape159, lv103, lv559, lv98), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape160 = R.call_tir(cls.reshape12, (lv104,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv560 = R.call_tir(cls.snake, (reshape160, encoder_block_1_block_2_block_0_alpha1), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape161 = R.call_tir(cls.reshape12, (lv560,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv105 = R.call_tir(cls.fused_tir_square15_sum15, (encoder_block_1_block_2_block_1_weight_v1,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv106 = R.call_tir(cls.fused_tir_sqrt6_divide15_multiply15, (lv105, encoder_block_1_block_2_block_1_weight_v1, encoder_block_1_block_2_block_1_weight_g1), out_sinfo=R.Tensor((64, 64, 7), dtype="float32"))
            lv566 = R.call_tir(cls.reshape11, (encoder_block_1_block_2_block_1_bias1,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv107 = R.call_tir(cls.fused_conv1d22_add10, (reshape161, lv106, lv566), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape162 = R.call_tir(cls.reshape12, (lv107,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv567 = R.call_tir(cls.snake, (reshape162, encoder_block_1_block_2_block_2_alpha1), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape163 = R.call_tir(cls.reshape12, (lv567,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv108 = R.call_tir(cls.fused_tir_square16_sum16, (encoder_block_1_block_2_block_3_weight_v1,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv109 = R.call_tir(cls.fused_tir_sqrt6_divide16_multiply16, (lv108, encoder_block_1_block_2_block_3_weight_v1, encoder_block_1_block_2_block_3_weight_g1), out_sinfo=R.Tensor((64, 64, 1), dtype="float32"))
            lv573 = R.call_tir(cls.reshape11, (encoder_block_1_block_2_block_3_bias1,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv110 = R.call_tir(cls.fused_conv1d20_add10_add11, (reshape163, lv109, lv573, lv104), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape164 = R.call_tir(cls.reshape12, (lv110,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv574 = R.call_tir(cls.snake, (reshape164, encoder_block_1_block_3_alpha1), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape165 = R.call_tir(cls.reshape12, (lv574,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv111 = R.call_tir(cls.fused_tir_square17_sum17, (encoder_block_1_block_4_weight_v1,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv112 = R.call_tir(cls.fused_tir_sqrt7_divide17_multiply17, (lv111, encoder_block_1_block_4_weight_v1, encoder_block_1_block_4_weight_g1), out_sinfo=R.Tensor((128, 64, 4), dtype="float32"))
            lv580 = R.call_tir(cls.reshape13, (encoder_block_1_block_4_bias1,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv113 = R.call_tir(cls.fused_conv1d23_add12, (reshape165, lv112, lv580), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"))
            reshape166 = R.call_tir(cls.reshape14, (lv113,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv581 = R.call_tir(cls.snake2, (reshape166, encoder_block_2_block_0_block_0_alpha1), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape167 = R.call_tir(cls.reshape14, (lv581,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv114 = R.call_tir(cls.fused_tir_square18_sum18, (encoder_block_2_block_0_block_1_weight_v1,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv115 = R.call_tir(cls.fused_tir_sqrt7_divide18_multiply18, (lv114, encoder_block_2_block_0_block_1_weight_v1, encoder_block_2_block_0_block_1_weight_g1), out_sinfo=R.Tensor((128, 128, 7), dtype="float32"))
            lv587 = R.call_tir(cls.reshape13, (encoder_block_2_block_0_block_1_bias1,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv116 = R.call_tir(cls.fused_conv1d24_add12, (reshape167, lv115, lv587), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape168 = R.call_tir(cls.reshape14, (lv116,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv588 = R.call_tir(cls.snake2, (reshape168, encoder_block_2_block_0_block_2_alpha1), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape169 = R.call_tir(cls.reshape14, (lv588,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv117 = R.call_tir(cls.fused_tir_square19_sum19, (encoder_block_2_block_0_block_3_weight_v1,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv118 = R.call_tir(cls.fused_tir_sqrt7_divide19_multiply19, (lv117, encoder_block_2_block_0_block_3_weight_v1, encoder_block_2_block_0_block_3_weight_g1), out_sinfo=R.Tensor((128, 128, 1), dtype="float32"))
            lv594 = R.call_tir(cls.reshape13, (encoder_block_2_block_0_block_3_bias1,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv119 = R.call_tir(cls.fused_conv1d25_add12_add13, (reshape169, lv118, lv594, lv113), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape170 = R.call_tir(cls.reshape14, (lv119,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv595 = R.call_tir(cls.snake1, (reshape170, encoder_block_2_block_1_block_0_alpha1), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape171 = R.call_tir(cls.reshape14, (lv595,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv120 = R.call_tir(cls.fused_tir_square18_sum18, (encoder_block_2_block_1_block_1_weight_v1,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv121 = R.call_tir(cls.fused_tir_sqrt7_divide18_multiply18, (lv120, encoder_block_2_block_1_block_1_weight_v1, encoder_block_2_block_1_block_1_weight_g1), out_sinfo=R.Tensor((128, 128, 7), dtype="float32"))
            lv601 = R.call_tir(cls.reshape13, (encoder_block_2_block_1_block_1_bias1,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv122 = R.call_tir(cls.fused_conv1d26_add12, (reshape171, lv121, lv601), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape172 = R.call_tir(cls.reshape14, (lv122,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv602 = R.call_tir(cls.snake2, (reshape172, encoder_block_2_block_1_block_2_alpha1), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape173 = R.call_tir(cls.reshape14, (lv602,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv123 = R.call_tir(cls.fused_tir_square19_sum19, (encoder_block_2_block_1_block_3_weight_v1,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv124 = R.call_tir(cls.fused_tir_sqrt7_divide19_multiply19, (lv123, encoder_block_2_block_1_block_3_weight_v1, encoder_block_2_block_1_block_3_weight_g1), out_sinfo=R.Tensor((128, 128, 1), dtype="float32"))
            lv608 = R.call_tir(cls.reshape13, (encoder_block_2_block_1_block_3_bias1,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv125 = R.call_tir(cls.fused_conv1d25_add12_add13, (reshape173, lv124, lv608, lv119), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape174 = R.call_tir(cls.reshape14, (lv125,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv609 = R.call_tir(cls.snake2, (reshape174, encoder_block_2_block_2_block_0_alpha1), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape175 = R.call_tir(cls.reshape14, (lv609,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv126 = R.call_tir(cls.fused_tir_square18_sum18, (encoder_block_2_block_2_block_1_weight_v1,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv127 = R.call_tir(cls.fused_tir_sqrt7_divide18_multiply18, (lv126, encoder_block_2_block_2_block_1_weight_v1, encoder_block_2_block_2_block_1_weight_g1), out_sinfo=R.Tensor((128, 128, 7), dtype="float32"))
            lv615 = R.call_tir(cls.reshape13, (encoder_block_2_block_2_block_1_bias1,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv128 = R.call_tir(cls.fused_conv1d27_add12, (reshape175, lv127, lv615), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape176 = R.call_tir(cls.reshape14, (lv128,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv616 = R.call_tir(cls.snake2, (reshape176, encoder_block_2_block_2_block_2_alpha1), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape177 = R.call_tir(cls.reshape14, (lv616,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv129 = R.call_tir(cls.fused_tir_square19_sum19, (encoder_block_2_block_2_block_3_weight_v1,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv130 = R.call_tir(cls.fused_tir_sqrt7_divide19_multiply19, (lv129, encoder_block_2_block_2_block_3_weight_v1, encoder_block_2_block_2_block_3_weight_g1), out_sinfo=R.Tensor((128, 128, 1), dtype="float32"))
            lv622 = R.call_tir(cls.reshape13, (encoder_block_2_block_2_block_3_bias1,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv131 = R.call_tir(cls.fused_conv1d25_add12_add13, (reshape177, lv130, lv622, lv125), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape178 = R.call_tir(cls.reshape14, (lv131,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv623 = R.call_tir(cls.snake2, (reshape178, encoder_block_2_block_3_alpha1), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape179 = R.call_tir(cls.reshape14, (lv623,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv132 = R.call_tir(cls.fused_tir_square20_sum20, (encoder_block_2_block_4_weight_v1,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv133 = R.call_tir(cls.fused_tir_sqrt8_divide20_multiply20, (lv132, encoder_block_2_block_4_weight_v1, encoder_block_2_block_4_weight_g1), out_sinfo=R.Tensor((256, 128, 8), dtype="float32"))
            lv629 = R.call_tir(cls.reshape15, (encoder_block_2_block_4_bias1,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv134 = R.call_tir(cls.fused_conv1d28_add14, (reshape179, lv133, lv629), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape180 = R.call_tir(cls.reshape16, (lv134,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv630 = R.call_tir(cls.snake3, (reshape180, encoder_block_3_block_0_block_0_alpha1), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape181 = R.call_tir(cls.reshape16, (lv630,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv135 = R.call_tir(cls.fused_tir_square21_sum21, (encoder_block_3_block_0_block_1_weight_v1,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv136 = R.call_tir(cls.fused_tir_sqrt8_divide21_multiply21, (lv135, encoder_block_3_block_0_block_1_weight_v1, encoder_block_3_block_0_block_1_weight_g1), out_sinfo=R.Tensor((256, 256, 7), dtype="float32"))
            lv636 = R.call_tir(cls.reshape15, (encoder_block_3_block_0_block_1_bias1,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv137 = R.call_tir(cls.fused_conv1d29_add14, (reshape181, lv136, lv636), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape182 = R.call_tir(cls.reshape16, (lv137,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv637 = R.call_tir(cls.snake3, (reshape182, encoder_block_3_block_0_block_2_alpha1), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape183 = R.call_tir(cls.reshape16, (lv637,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv138 = R.call_tir(cls.fused_tir_square22_sum22, (encoder_block_3_block_0_block_3_weight_v1,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv139 = R.call_tir(cls.fused_tir_sqrt8_divide22_multiply22, (lv138, encoder_block_3_block_0_block_3_weight_v1, encoder_block_3_block_0_block_3_weight_g1), out_sinfo=R.Tensor((256, 256, 1), dtype="float32"))
            lv643 = R.call_tir(cls.reshape15, (encoder_block_3_block_0_block_3_bias1,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv140 = R.call_tir(cls.fused_conv1d30_add14_add15, (reshape183, lv139, lv643, lv134), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape184 = R.call_tir(cls.reshape16, (lv140,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv644 = R.call_tir(cls.snake3, (reshape184, encoder_block_3_block_1_block_0_alpha1), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape185 = R.call_tir(cls.reshape16, (lv644,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv141 = R.call_tir(cls.fused_tir_square21_sum21, (encoder_block_3_block_1_block_1_weight_v1,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv142 = R.call_tir(cls.fused_tir_sqrt8_divide21_multiply21, (lv141, encoder_block_3_block_1_block_1_weight_v1, encoder_block_3_block_1_block_1_weight_g1), out_sinfo=R.Tensor((256, 256, 7), dtype="float32"))
            lv650 = R.call_tir(cls.reshape15, (encoder_block_3_block_1_block_1_bias1,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv143 = R.call_tir(cls.fused_conv1d31_add14, (reshape185, lv142, lv650), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape186 = R.call_tir(cls.reshape16, (lv143,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv651 = R.call_tir(cls.snake3, (reshape186, encoder_block_3_block_1_block_2_alpha1), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape187 = R.call_tir(cls.reshape16, (lv651,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv144 = R.call_tir(cls.fused_tir_square22_sum22, (encoder_block_3_block_1_block_3_weight_v1,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv145 = R.call_tir(cls.fused_tir_sqrt8_divide22_multiply22, (lv144, encoder_block_3_block_1_block_3_weight_v1, encoder_block_3_block_1_block_3_weight_g1), out_sinfo=R.Tensor((256, 256, 1), dtype="float32"))
            lv657 = R.call_tir(cls.reshape15, (encoder_block_3_block_1_block_3_bias1,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv146 = R.call_tir(cls.fused_conv1d30_add14_add15, (reshape187, lv145, lv657, lv140), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape188 = R.call_tir(cls.reshape16, (lv146,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv658 = R.call_tir(cls.snake4, (reshape188, encoder_block_3_block_2_block_0_alpha1), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape189 = R.call_tir(cls.reshape16, (lv658,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv147 = R.call_tir(cls.fused_tir_square21_sum21, (encoder_block_3_block_2_block_1_weight_v1,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv148 = R.call_tir(cls.fused_tir_sqrt8_divide21_multiply21, (lv147, encoder_block_3_block_2_block_1_weight_v1, encoder_block_3_block_2_block_1_weight_g1), out_sinfo=R.Tensor((256, 256, 7), dtype="float32"))
            lv664 = R.call_tir(cls.reshape15, (encoder_block_3_block_2_block_1_bias1,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv149 = R.call_tir(cls.fused_conv1d32_add14, (reshape189, lv148, lv664), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape190 = R.call_tir(cls.reshape16, (lv149,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv665 = R.call_tir(cls.snake3, (reshape190, encoder_block_3_block_2_block_2_alpha1), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape191 = R.call_tir(cls.reshape16, (lv665,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv150 = R.call_tir(cls.fused_tir_square22_sum22, (encoder_block_3_block_2_block_3_weight_v1,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv151 = R.call_tir(cls.fused_tir_sqrt8_divide22_multiply22, (lv150, encoder_block_3_block_2_block_3_weight_v1, encoder_block_3_block_2_block_3_weight_g1), out_sinfo=R.Tensor((256, 256, 1), dtype="float32"))
            lv671 = R.call_tir(cls.reshape15, (encoder_block_3_block_2_block_3_bias1,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv152 = R.call_tir(cls.fused_conv1d30_add14_add15, (reshape191, lv151, lv671, lv146), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape192 = R.call_tir(cls.reshape16, (lv152,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv672 = R.call_tir(cls.snake3, (reshape192, encoder_block_3_block_3_alpha1), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape193 = R.call_tir(cls.reshape16, (lv672,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv153 = R.call_tir(cls.fused_tir_square23_sum23, (encoder_block_3_block_4_weight_v1,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv154 = R.call_tir(cls.fused_tir_sqrt9_divide23_multiply23, (lv153, encoder_block_3_block_4_weight_v1, encoder_block_3_block_4_weight_g1), out_sinfo=R.Tensor((512, 256, 16), dtype="float32"))
            lv678 = R.call_tir(cls.reshape17, (encoder_block_3_block_4_bias1,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv155 = R.call_tir(cls.fused_conv1d33_add16, (reshape193, lv154, lv678), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape194 = R.call_tir(cls.reshape18, (lv155,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv679 = R.call_tir(cls.snake5, (reshape194, encoder_block_4_block_0_block_0_alpha1), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape195 = R.call_tir(cls.reshape18, (lv679,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv156 = R.call_tir(cls.fused_tir_square24_sum24, (encoder_block_4_block_0_block_1_weight_v1,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv157 = R.call_tir(cls.fused_tir_sqrt9_divide24_multiply24, (lv156, encoder_block_4_block_0_block_1_weight_v1, encoder_block_4_block_0_block_1_weight_g1), out_sinfo=R.Tensor((512, 512, 7), dtype="float32"))
            lv685 = R.call_tir(cls.reshape17, (encoder_block_4_block_0_block_1_bias1,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv158 = R.call_tir(cls.fused_conv1d34_add16, (reshape195, lv157, lv685), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape196 = R.call_tir(cls.reshape18, (lv158,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv686 = R.call_tir(cls.snake5, (reshape196, encoder_block_4_block_0_block_2_alpha1), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape197 = R.call_tir(cls.reshape18, (lv686,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv159 = R.call_tir(cls.fused_tir_square25_sum25, (encoder_block_4_block_0_block_3_weight_v1,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv160 = R.call_tir(cls.fused_tir_sqrt9_divide25_multiply25, (lv159, encoder_block_4_block_0_block_3_weight_v1, encoder_block_4_block_0_block_3_weight_g1), out_sinfo=R.Tensor((512, 512, 1), dtype="float32"))
            lv692 = R.call_tir(cls.reshape17, (encoder_block_4_block_0_block_3_bias1,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv161 = R.call_tir(cls.fused_conv1d35_add16_add17, (reshape197, lv160, lv692, lv155), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape198 = R.call_tir(cls.reshape18, (lv161,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv693 = R.call_tir(cls.snake5, (reshape198, encoder_block_4_block_1_block_0_alpha1), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape199 = R.call_tir(cls.reshape18, (lv693,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv162 = R.call_tir(cls.fused_tir_square24_sum24, (encoder_block_4_block_1_block_1_weight_v1,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv163 = R.call_tir(cls.fused_tir_sqrt9_divide24_multiply24, (lv162, encoder_block_4_block_1_block_1_weight_v1, encoder_block_4_block_1_block_1_weight_g1), out_sinfo=R.Tensor((512, 512, 7), dtype="float32"))
            lv699 = R.call_tir(cls.reshape17, (encoder_block_4_block_1_block_1_bias1,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv164 = R.call_tir(cls.fused_conv1d36_add16, (reshape199, lv163, lv699), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape200 = R.call_tir(cls.reshape18, (lv164,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv700 = R.call_tir(cls.snake16, (reshape200, encoder_block_4_block_1_block_2_alpha1), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape201 = R.call_tir(cls.reshape18, (lv700,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv165 = R.call_tir(cls.fused_tir_square25_sum25, (encoder_block_4_block_1_block_3_weight_v1,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv166 = R.call_tir(cls.fused_tir_sqrt9_divide25_multiply25, (lv165, encoder_block_4_block_1_block_3_weight_v1, encoder_block_4_block_1_block_3_weight_g1), out_sinfo=R.Tensor((512, 512, 1), dtype="float32"))
            lv706 = R.call_tir(cls.reshape17, (encoder_block_4_block_1_block_3_bias1,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv167 = R.call_tir(cls.fused_conv1d35_add16_add17, (reshape201, lv166, lv706, lv161), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape202 = R.call_tir(cls.reshape18, (lv167,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv707 = R.call_tir(cls.snake16, (reshape202, encoder_block_4_block_2_block_0_alpha1), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape203 = R.call_tir(cls.reshape18, (lv707,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv168 = R.call_tir(cls.fused_tir_square24_sum24, (encoder_block_4_block_2_block_1_weight_v1,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv169 = R.call_tir(cls.fused_tir_sqrt9_divide24_multiply24, (lv168, encoder_block_4_block_2_block_1_weight_v1, encoder_block_4_block_2_block_1_weight_g1), out_sinfo=R.Tensor((512, 512, 7), dtype="float32"))
            lv713 = R.call_tir(cls.reshape17, (encoder_block_4_block_2_block_1_bias1,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv170 = R.call_tir(cls.fused_conv1d37_add16, (reshape203, lv169, lv713), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape204 = R.call_tir(cls.reshape18, (lv170,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv714 = R.call_tir(cls.snake5, (reshape204, encoder_block_4_block_2_block_2_alpha1), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape205 = R.call_tir(cls.reshape18, (lv714,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv171 = R.call_tir(cls.fused_tir_square25_sum25, (encoder_block_4_block_2_block_3_weight_v1,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv172 = R.call_tir(cls.fused_tir_sqrt9_divide25_multiply25, (lv171, encoder_block_4_block_2_block_3_weight_v1, encoder_block_4_block_2_block_3_weight_g1), out_sinfo=R.Tensor((512, 512, 1), dtype="float32"))
            lv720 = R.call_tir(cls.reshape17, (encoder_block_4_block_2_block_3_bias1,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv173 = R.call_tir(cls.fused_conv1d35_add16_add17, (reshape205, lv172, lv720, lv167), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape206 = R.call_tir(cls.reshape18, (lv173,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv721 = R.call_tir(cls.snake5, (reshape206, encoder_block_4_block_3_alpha1), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape207 = R.call_tir(cls.reshape18, (lv721,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv174 = R.call_tir(cls.fused_tir_square26_sum26, (encoder_block_4_block_4_weight_v1,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv175 = R.call_tir(cls.fused_tir_sqrt10_divide26_multiply26, (lv174, encoder_block_4_block_4_weight_v1, encoder_block_4_block_4_weight_g1), out_sinfo=R.Tensor((1024, 512, 16), dtype="float32"))
            lv727 = R.call_tir(cls.reshape19, (encoder_block_4_block_4_bias1,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv176 = R.call_tir(cls.fused_conv1d38_add18, (reshape207, lv175, lv727), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape208 = R.call_tir(cls.reshape20, (lv176,), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv728 = R.call_tir(cls.snake6, (reshape208, encoder_block_5_alpha1), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape209 = R.call_tir(cls.reshape20, (lv728,), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv177 = R.call_tir(cls.fused_tir_square27_sum27, (encoder_block_6_weight_v1,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv178 = R.call_tir(cls.fused_tir_sqrt10_divide27_multiply27, (lv177, encoder_block_6_weight_v1, encoder_block_6_weight_g1), out_sinfo=R.Tensor((1024, 1024, 3), dtype="float32"))
            lv734 = R.call_tir(cls.reshape19, (encoder_block_6_bias1,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv179 = R.call_tir(cls.fused_conv1d39_add18, (reshape209, lv178, lv734), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv180 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_0_in_proj_weight_v1,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv181 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv180, quantizer_quantizers_0_in_proj_weight_v1, quantizer_quantizers_0_in_proj_weight_g1), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv740 = R.call_tir(cls.reshape21, (quantizer_quantizers_0_in_proj_bias1,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv182 = R.call_tir(cls.fused_conv1d40_add19, (lv179, lv181, lv740), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims36 = R.call_tir(cls.transpose, (lv182,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape210 = R.call_tir(cls.reshape22, (permute_dims36,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv183 = R.call_tir(cls.fused_tir_square29_sum29, (reshape210,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv184 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt12_divide29, (lv183, reshape210), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv185 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_0_codebook_weight1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv186 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv185, quantizer_quantizers_0_codebook_weight1), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims37 = R.call_tir(cls.transpose1, (lv186,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv187 = R.call_tir(cls.fused_tir_square29_sum29, (lv184,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv188 = R.call_tir(cls.fused_tir_square30_sum30, (lv186,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims38 = R.call_tir(cls.transpose2, (lv188,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv189 = R.call_tir(cls.fused_matmul_multiply29_subtract_add20, (lv184, permute_dims37, lv187, permute_dims38), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort9: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv189, axis=1, descending=False, dtype="int32")
            take18 = R.call_tir(cls.take, (argsort9, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape211 = R.call_tir(cls.reshape23, (take18,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape212 = R.call_tir(cls.reshape24, (reshape211,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take19 = R.call_tir(cls.take1, (quantizer_quantizers_0_codebook_weight1, reshape212), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape213 = R.call_tir(cls.reshape25, (take19,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims39 = R.call_tir(cls.transpose3, (reshape213,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv190 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_0_out_proj_weight_v1,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv191 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv190, quantizer_quantizers_0_out_proj_weight_v1, quantizer_quantizers_0_out_proj_weight_g1), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv746 = R.call_tir(cls.reshape19, (quantizer_quantizers_0_out_proj_bias1,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv192 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims39, lv191, lv746), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract19 = R.call_tir(cls.subtract1, (lv179, lv192), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv193 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_1_in_proj_weight_v1,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv194 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv193, quantizer_quantizers_1_in_proj_weight_v1, quantizer_quantizers_1_in_proj_weight_g1), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv752 = R.call_tir(cls.reshape21, (quantizer_quantizers_1_in_proj_bias1,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv195 = R.call_tir(cls.fused_conv1d40_add19, (subtract19, lv194, lv752), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims40 = R.call_tir(cls.transpose, (lv195,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape214 = R.call_tir(cls.reshape22, (permute_dims40,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv196 = R.call_tir(cls.fused_tir_square32_sum32, (reshape214,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv197 = R.call_tir(cls.fused_broadcast_to_maximum2_tir_sqrt12_divide32, (lv196, reshape214), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv198 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_1_codebook_weight1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv199 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv198, quantizer_quantizers_1_codebook_weight1), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims41 = R.call_tir(cls.transpose1, (lv199,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv200 = R.call_tir(cls.fused_tir_square32_sum29, (lv197,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv201 = R.call_tir(cls.fused_tir_square30_sum30, (lv199,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims42 = R.call_tir(cls.transpose2, (lv201,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv202 = R.call_tir(cls.fused_matmul1_multiply31_subtract_add20, (lv197, permute_dims41, lv200, permute_dims42), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort10: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv202, axis=1, descending=False, dtype="int32")
            take20 = R.call_tir(cls.take, (argsort10, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape215 = R.call_tir(cls.reshape23, (take20,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape216 = R.call_tir(cls.reshape24, (reshape215,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take21 = R.call_tir(cls.take1, (quantizer_quantizers_1_codebook_weight1, reshape216), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape217 = R.call_tir(cls.reshape25, (take21,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims43 = R.call_tir(cls.transpose3, (reshape217,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv203 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_1_out_proj_weight_v1,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv204 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv203, quantizer_quantizers_1_out_proj_weight_v1, quantizer_quantizers_1_out_proj_weight_g1), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv758 = R.call_tir(cls.reshape19, (quantizer_quantizers_1_out_proj_bias1,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv205 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims43, lv204, lv758), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract21 = R.call_tir(cls.subtract1, (subtract19, lv205), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv206 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_2_in_proj_weight_v1,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv207 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv206, quantizer_quantizers_2_in_proj_weight_v1, quantizer_quantizers_2_in_proj_weight_g1), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv764 = R.call_tir(cls.reshape21, (quantizer_quantizers_2_in_proj_bias1,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv208 = R.call_tir(cls.fused_conv1d40_add19, (subtract21, lv207, lv764), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims44 = R.call_tir(cls.transpose, (lv208,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape218 = R.call_tir(cls.reshape22, (permute_dims44,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv209 = R.call_tir(cls.fused_tir_square32_sum29, (reshape218,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv210 = R.call_tir(cls.fused_broadcast_to2_maximum_tir_sqrt12_divide32, (lv209, reshape218), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv211 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_2_codebook_weight1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv212 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv211, quantizer_quantizers_2_codebook_weight1), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims45 = R.call_tir(cls.transpose1, (lv212,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv213 = R.call_tir(cls.fused_tir_square32_sum32, (lv210,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv214 = R.call_tir(cls.fused_tir_square30_sum30, (lv212,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims46 = R.call_tir(cls.transpose2, (lv214,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv215 = R.call_tir(cls.fused_matmul1_multiply31_subtract_add20, (lv210, permute_dims45, lv213, permute_dims46), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort11: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv215, axis=1, descending=False, dtype="int32")
            take22 = R.call_tir(cls.take, (argsort11, metadata["relax.expr.Constant"][2]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape219 = R.call_tir(cls.reshape23, (take22,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape220 = R.call_tir(cls.reshape24, (reshape219,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take23 = R.call_tir(cls.take2, (quantizer_quantizers_2_codebook_weight1, reshape220), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            reshape221 = R.call_tir(cls.reshape25, (take23,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims47 = R.call_tir(cls.transpose3, (reshape221,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv216 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_2_out_proj_weight_v1,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv217 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv216, quantizer_quantizers_2_out_proj_weight_v1, quantizer_quantizers_2_out_proj_weight_g1), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv770 = R.call_tir(cls.reshape19, (quantizer_quantizers_2_out_proj_bias1,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv218 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims47, lv217, lv770), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract23 = R.call_tir(cls.subtract1, (subtract21, lv218), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv219 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_3_in_proj_weight_v1,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv220 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv219, quantizer_quantizers_3_in_proj_weight_v1, quantizer_quantizers_3_in_proj_weight_g1), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv776 = R.call_tir(cls.reshape21, (quantizer_quantizers_3_in_proj_bias1,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv221 = R.call_tir(cls.fused_conv1d40_add19, (subtract23, lv220, lv776), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims48 = R.call_tir(cls.transpose, (lv221,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape222 = R.call_tir(cls.reshape22, (permute_dims48,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv222 = R.call_tir(cls.fused_tir_square29_sum29, (reshape222,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv223 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt14_divide32, (lv222, reshape222), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv224 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_3_codebook_weight1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv225 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv224, quantizer_quantizers_3_codebook_weight1), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims49 = R.call_tir(cls.transpose1, (lv225,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv226 = R.call_tir(cls.fused_tir_square32_sum29, (lv223,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv227 = R.call_tir(cls.fused_tir_square30_sum30, (lv225,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims50 = R.call_tir(cls.transpose2, (lv227,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv228 = R.call_tir(cls.fused_matmul1_multiply29_subtract_add22, (lv223, permute_dims49, lv226, permute_dims50), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort12: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv228, axis=1, descending=False, dtype="int32")
            take24 = R.call_tir(cls.take3, (argsort12, metadata["relax.expr.Constant"][3]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([seq_len, batch_size]))
            reshape223 = R.call_tir(cls.reshape23, (take24,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape224 = R.call_tir(cls.reshape24, (reshape223,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take25 = R.call_tir(cls.take1, (quantizer_quantizers_3_codebook_weight1, reshape224), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape225 = R.call_tir(cls.reshape25, (take25,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims51 = R.call_tir(cls.transpose3, (reshape225,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv229 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_3_out_proj_weight_v1,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv230 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv229, quantizer_quantizers_3_out_proj_weight_v1, quantizer_quantizers_3_out_proj_weight_g1), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv782 = R.call_tir(cls.reshape19, (quantizer_quantizers_3_out_proj_bias1,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv231 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims51, lv230, lv782), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract25 = R.call_tir(cls.subtract1, (subtract23, lv231), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv232 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_4_in_proj_weight_v1,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv233 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv232, quantizer_quantizers_4_in_proj_weight_v1, quantizer_quantizers_4_in_proj_weight_g1), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv788 = R.call_tir(cls.reshape21, (quantizer_quantizers_4_in_proj_bias1,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv234 = R.call_tir(cls.fused_conv1d40_add19, (subtract25, lv233, lv788), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims52 = R.call_tir(cls.transpose, (lv234,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape226 = R.call_tir(cls.reshape22, (permute_dims52,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv235 = R.call_tir(cls.fused_tir_square32_sum29, (reshape226,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv236 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt12_divide32, (lv235, reshape226), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv237 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_4_codebook_weight1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv238 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv237, quantizer_quantizers_4_codebook_weight1), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims53 = R.call_tir(cls.transpose1, (lv238,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv239 = R.call_tir(cls.fused_tir_square29_sum32, (lv236,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv240 = R.call_tir(cls.fused_tir_square30_sum30, (lv238,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims54 = R.call_tir(cls.transpose2, (lv240,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv241 = R.call_tir(cls.fused_matmul1_multiply31_subtract2_add20, (lv236, permute_dims53, lv239, permute_dims54), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort13: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv241, axis=1, descending=False, dtype="int32")
            take26 = R.call_tir(cls.take, (argsort13, metadata["relax.expr.Constant"][4]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape227 = R.call_tir(cls.reshape23, (take26,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape228 = R.call_tir(cls.reshape24, (reshape227,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take27 = R.call_tir(cls.take1, (quantizer_quantizers_4_codebook_weight1, reshape228), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape229 = R.call_tir(cls.reshape25, (take27,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims55 = R.call_tir(cls.transpose3, (reshape229,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv242 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_4_out_proj_weight_v1,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv243 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv242, quantizer_quantizers_4_out_proj_weight_v1, quantizer_quantizers_4_out_proj_weight_g1), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv794 = R.call_tir(cls.reshape19, (quantizer_quantizers_4_out_proj_bias1,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv244 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims55, lv243, lv794), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract27 = R.call_tir(cls.subtract1, (subtract25, lv244), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv245 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_5_in_proj_weight_v1,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv246 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv245, quantizer_quantizers_5_in_proj_weight_v1, quantizer_quantizers_5_in_proj_weight_g1), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv800 = R.call_tir(cls.reshape21, (quantizer_quantizers_5_in_proj_bias1,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv247 = R.call_tir(cls.fused_conv1d40_add19, (subtract27, lv246, lv800), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims56 = R.call_tir(cls.transpose, (lv247,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape230 = R.call_tir(cls.reshape22, (permute_dims56,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv248 = R.call_tir(cls.fused_tir_square32_sum32, (reshape230,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv249 = R.call_tir(cls.fused_broadcast_to2_maximum_tir_sqrt12_divide32, (lv248, reshape230), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv250 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_5_codebook_weight1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv251 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv250, quantizer_quantizers_5_codebook_weight1), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims57 = R.call_tir(cls.transpose1, (lv251,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv252 = R.call_tir(cls.fused_tir_square29_sum32, (lv249,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv253 = R.call_tir(cls.fused_tir_square30_sum30, (lv251,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims58 = R.call_tir(cls.transpose2, (lv253,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv254 = R.call_tir(cls.fused_matmul_multiply31_subtract2_add20, (lv249, permute_dims57, lv252, permute_dims58), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort14: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv254, axis=1, descending=False, dtype="int32")
            take28 = R.call_tir(cls.take, (argsort14, metadata["relax.expr.Constant"][5]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape231 = R.call_tir(cls.reshape23, (take28,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape232 = R.call_tir(cls.reshape24, (reshape231,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take29 = R.call_tir(cls.take1, (quantizer_quantizers_5_codebook_weight1, reshape232), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape233 = R.call_tir(cls.reshape25, (take29,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims59 = R.call_tir(cls.transpose3, (reshape233,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv255 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_5_out_proj_weight_v1,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv256 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv255, quantizer_quantizers_5_out_proj_weight_v1, quantizer_quantizers_5_out_proj_weight_g1), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv806 = R.call_tir(cls.reshape19, (quantizer_quantizers_5_out_proj_bias1,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv257 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims59, lv256, lv806), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract29 = R.call_tir(cls.subtract1, (subtract27, lv257), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv258 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_6_in_proj_weight_v1,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv259 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv258, quantizer_quantizers_6_in_proj_weight_v1, quantizer_quantizers_6_in_proj_weight_g1), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv812 = R.call_tir(cls.reshape21, (quantizer_quantizers_6_in_proj_bias1,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv260 = R.call_tir(cls.fused_conv1d40_add19, (subtract29, lv259, lv812), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims60 = R.call_tir(cls.transpose, (lv260,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape234 = R.call_tir(cls.reshape22, (permute_dims60,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv261 = R.call_tir(cls.fused_tir_square32_sum29, (reshape234,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv262 = R.call_tir(cls.fused_broadcast_to2_maximum_tir_sqrt14_divide32, (lv261, reshape234), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv263 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_6_codebook_weight1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv264 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv263, quantizer_quantizers_6_codebook_weight1), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims61 = R.call_tir(cls.transpose1, (lv264,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv265 = R.call_tir(cls.fused_tir_square29_sum29, (lv262,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv266 = R.call_tir(cls.fused_tir_square30_sum30, (lv264,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims62 = R.call_tir(cls.transpose2, (lv266,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv267 = R.call_tir(cls.fused_matmul1_multiply31_subtract2_add22, (lv262, permute_dims61, lv265, permute_dims62), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort15: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv267, axis=1, descending=False, dtype="int32")
            take30 = R.call_tir(cls.take, (argsort15, metadata["relax.expr.Constant"][6]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape235 = R.call_tir(cls.reshape23, (take30,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape236 = R.call_tir(cls.reshape24, (reshape235,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take31 = R.call_tir(cls.take1, (quantizer_quantizers_6_codebook_weight1, reshape236), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape237 = R.call_tir(cls.reshape25, (take31,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims63 = R.call_tir(cls.transpose3, (reshape237,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv268 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_6_out_proj_weight_v1,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv269 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv268, quantizer_quantizers_6_out_proj_weight_v1, quantizer_quantizers_6_out_proj_weight_g1), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv818 = R.call_tir(cls.reshape19, (quantizer_quantizers_6_out_proj_bias1,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv270 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims63, lv269, lv818), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract31 = R.call_tir(cls.subtract1, (subtract29, lv270), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv271 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_7_in_proj_weight_v1,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv272 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv271, quantizer_quantizers_7_in_proj_weight_v1, quantizer_quantizers_7_in_proj_weight_g1), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv824 = R.call_tir(cls.reshape21, (quantizer_quantizers_7_in_proj_bias1,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv273 = R.call_tir(cls.fused_conv1d40_add19, (subtract31, lv272, lv824), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims64 = R.call_tir(cls.transpose, (lv273,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape238 = R.call_tir(cls.reshape22, (permute_dims64,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv274 = R.call_tir(cls.fused_tir_square32_sum32, (reshape238,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv275 = R.call_tir(cls.fused_broadcast_to2_maximum2_tir_sqrt12_divide29, (lv274, reshape238), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv276 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_7_codebook_weight1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv277 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv276, quantizer_quantizers_7_codebook_weight1), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims65 = R.call_tir(cls.transpose1, (lv277,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv278 = R.call_tir(cls.fused_tir_square32_sum29, (lv275,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv279 = R.call_tir(cls.fused_tir_square30_sum30, (lv277,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims66 = R.call_tir(cls.transpose2, (lv279,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv280 = R.call_tir(cls.fused_matmul1_multiply29_subtract_add22, (lv275, permute_dims65, lv278, permute_dims66), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort16: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv280, axis=1, descending=False, dtype="int32")
            take32 = R.call_tir(cls.take, (argsort16, metadata["relax.expr.Constant"][7]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape239 = R.call_tir(cls.reshape23, (take32,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape240 = R.call_tir(cls.reshape24, (reshape239,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take33 = R.call_tir(cls.take1, (quantizer_quantizers_7_codebook_weight1, reshape240), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape241 = R.call_tir(cls.reshape25, (take33,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims67 = R.call_tir(cls.transpose3, (reshape241,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv281 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_7_out_proj_weight_v1,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv282 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv281, quantizer_quantizers_7_out_proj_weight_v1, quantizer_quantizers_7_out_proj_weight_g1), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv830 = R.call_tir(cls.reshape19, (quantizer_quantizers_7_out_proj_bias1,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv283 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims67, lv282, lv830), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract33 = R.call_tir(cls.subtract1, (subtract31, lv283), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv284 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_8_in_proj_weight_v1,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv285 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv284, quantizer_quantizers_8_in_proj_weight_v1, quantizer_quantizers_8_in_proj_weight_g1), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv836 = R.call_tir(cls.reshape21, (quantizer_quantizers_8_in_proj_bias1,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv286 = R.call_tir(cls.fused_conv1d40_add19, (subtract33, lv285, lv836), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims68 = R.call_tir(cls.transpose, (lv286,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape242 = R.call_tir(cls.reshape22, (permute_dims68,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv287 = R.call_tir(cls.fused_tir_square29_sum29, (reshape242,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv288 = R.call_tir(cls.fused_broadcast_to2_maximum2_tir_sqrt14_divide29, (lv287, reshape242), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv289 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_8_codebook_weight1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv290 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv289, quantizer_quantizers_8_codebook_weight1), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims69 = R.call_tir(cls.transpose1, (lv290,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv291 = R.call_tir(cls.fused_tir_square29_sum32, (lv288,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv292 = R.call_tir(cls.fused_tir_square30_sum30, (lv290,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims70 = R.call_tir(cls.transpose2, (lv292,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv293 = R.call_tir(cls.fused_matmul_multiply31_subtract2_add22, (lv288, permute_dims69, lv291, permute_dims70), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort17: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv293, axis=1, descending=False, dtype="int32")
            take34 = R.call_tir(cls.take, (argsort17, metadata["relax.expr.Constant"][8]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape243 = R.call_tir(cls.reshape23, (take34,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape244 = R.call_tir(cls.reshape24, (reshape243,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take35 = R.call_tir(cls.take1, (quantizer_quantizers_8_codebook_weight1, reshape244), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape245 = R.call_tir(cls.reshape25, (take35,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims71 = R.call_tir(cls.transpose3, (reshape245,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv294 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_8_out_proj_weight_v1,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv295 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv294, quantizer_quantizers_8_out_proj_weight_v1, quantizer_quantizers_8_out_proj_weight_g1), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv842 = R.call_tir(cls.reshape19, (quantizer_quantizers_8_out_proj_bias1,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv296 = R.call_tir(cls.fused_zeros_add21_add21_add21_add21_add21_add21_add21_add21_conv1d41_add18_add21, (lv192, lv205, lv218, lv231, lv244, lv257, lv270, lv283, permute_dims71, lv295, lv842), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            gv1: R.Tuple(R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), R.Tuple(R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"))) = lv296, (reshape211, reshape215, reshape219, reshape223, reshape227, reshape231, reshape235, reshape239, reshape243)
            R.output(gv1)
        return gv1

    @R.function
    def forward(audio_data: R.Tensor(("batch_size", 1, "seq_len"), dtype="float32"), packed_params: R.Tuple(R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 1, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 64, 4), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 128, 8), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 256, 16), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 512, 16), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1, 1024, 1), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 1024, 3), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((1536, 1, 1), dtype="float32"), R.Tensor((1536, 1024, 7), dtype="float32"), R.Tensor((1536,), dtype="float32"), R.Tensor((1, 1536, 1), dtype="float32"), R.Tensor((1536, 1, 1), dtype="float32"), R.Tensor((1536, 768, 16), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 384, 16), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 192, 8), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 96, 4), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((1, 1, 1), dtype="float32"), R.Tensor((1, 96, 7), dtype="float32"), R.Tensor((1,), dtype="float32"))) -> R.Tuple(R.Tensor(("batch_size", 1, "seq_len // 512 * 512"), dtype="float32"), R.Tensor(("batch_size", 1024, "seq_len // 512"), dtype="float32"), R.Tuple(R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"), R.Tensor(("batch_size", "seq_len // 512"), dtype="int32"))):
        batch_size = T.int64()
        seq_len = T.int64()
        R.func_attr({"num_input": 1})
        cls = Module
        with R.dataflow():
            encoder_block_0_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[0]
            encoder_block_0_weight_v: R.Tensor((64, 1, 7), dtype="float32") = packed_params[1]
            encoder_block_0_bias: R.Tensor((64,), dtype="float32") = packed_params[2]
            encoder_block_1_block_0_block_0_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[3]
            encoder_block_1_block_0_block_1_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[4]
            encoder_block_1_block_0_block_1_weight_v: R.Tensor((64, 64, 7), dtype="float32") = packed_params[5]
            encoder_block_1_block_0_block_1_bias: R.Tensor((64,), dtype="float32") = packed_params[6]
            encoder_block_1_block_0_block_2_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[7]
            encoder_block_1_block_0_block_3_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[8]
            encoder_block_1_block_0_block_3_weight_v: R.Tensor((64, 64, 1), dtype="float32") = packed_params[9]
            encoder_block_1_block_0_block_3_bias: R.Tensor((64,), dtype="float32") = packed_params[10]
            encoder_block_1_block_1_block_0_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[11]
            encoder_block_1_block_1_block_1_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[12]
            encoder_block_1_block_1_block_1_weight_v: R.Tensor((64, 64, 7), dtype="float32") = packed_params[13]
            encoder_block_1_block_1_block_1_bias: R.Tensor((64,), dtype="float32") = packed_params[14]
            encoder_block_1_block_1_block_2_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[15]
            encoder_block_1_block_1_block_3_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[16]
            encoder_block_1_block_1_block_3_weight_v: R.Tensor((64, 64, 1), dtype="float32") = packed_params[17]
            encoder_block_1_block_1_block_3_bias: R.Tensor((64,), dtype="float32") = packed_params[18]
            encoder_block_1_block_2_block_0_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[19]
            encoder_block_1_block_2_block_1_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[20]
            encoder_block_1_block_2_block_1_weight_v: R.Tensor((64, 64, 7), dtype="float32") = packed_params[21]
            encoder_block_1_block_2_block_1_bias: R.Tensor((64,), dtype="float32") = packed_params[22]
            encoder_block_1_block_2_block_2_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[23]
            encoder_block_1_block_2_block_3_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[24]
            encoder_block_1_block_2_block_3_weight_v: R.Tensor((64, 64, 1), dtype="float32") = packed_params[25]
            encoder_block_1_block_2_block_3_bias: R.Tensor((64,), dtype="float32") = packed_params[26]
            encoder_block_1_block_3_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[27]
            encoder_block_1_block_4_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[28]
            encoder_block_1_block_4_weight_v: R.Tensor((128, 64, 4), dtype="float32") = packed_params[29]
            encoder_block_1_block_4_bias: R.Tensor((128,), dtype="float32") = packed_params[30]
            encoder_block_2_block_0_block_0_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[31]
            encoder_block_2_block_0_block_1_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[32]
            encoder_block_2_block_0_block_1_weight_v: R.Tensor((128, 128, 7), dtype="float32") = packed_params[33]
            encoder_block_2_block_0_block_1_bias: R.Tensor((128,), dtype="float32") = packed_params[34]
            encoder_block_2_block_0_block_2_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[35]
            encoder_block_2_block_0_block_3_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[36]
            encoder_block_2_block_0_block_3_weight_v: R.Tensor((128, 128, 1), dtype="float32") = packed_params[37]
            encoder_block_2_block_0_block_3_bias: R.Tensor((128,), dtype="float32") = packed_params[38]
            encoder_block_2_block_1_block_0_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[39]
            encoder_block_2_block_1_block_1_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[40]
            encoder_block_2_block_1_block_1_weight_v: R.Tensor((128, 128, 7), dtype="float32") = packed_params[41]
            encoder_block_2_block_1_block_1_bias: R.Tensor((128,), dtype="float32") = packed_params[42]
            encoder_block_2_block_1_block_2_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[43]
            encoder_block_2_block_1_block_3_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[44]
            encoder_block_2_block_1_block_3_weight_v: R.Tensor((128, 128, 1), dtype="float32") = packed_params[45]
            encoder_block_2_block_1_block_3_bias: R.Tensor((128,), dtype="float32") = packed_params[46]
            encoder_block_2_block_2_block_0_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[47]
            encoder_block_2_block_2_block_1_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[48]
            encoder_block_2_block_2_block_1_weight_v: R.Tensor((128, 128, 7), dtype="float32") = packed_params[49]
            encoder_block_2_block_2_block_1_bias: R.Tensor((128,), dtype="float32") = packed_params[50]
            encoder_block_2_block_2_block_2_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[51]
            encoder_block_2_block_2_block_3_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[52]
            encoder_block_2_block_2_block_3_weight_v: R.Tensor((128, 128, 1), dtype="float32") = packed_params[53]
            encoder_block_2_block_2_block_3_bias: R.Tensor((128,), dtype="float32") = packed_params[54]
            encoder_block_2_block_3_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[55]
            encoder_block_2_block_4_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[56]
            encoder_block_2_block_4_weight_v: R.Tensor((256, 128, 8), dtype="float32") = packed_params[57]
            encoder_block_2_block_4_bias: R.Tensor((256,), dtype="float32") = packed_params[58]
            encoder_block_3_block_0_block_0_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[59]
            encoder_block_3_block_0_block_1_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[60]
            encoder_block_3_block_0_block_1_weight_v: R.Tensor((256, 256, 7), dtype="float32") = packed_params[61]
            encoder_block_3_block_0_block_1_bias: R.Tensor((256,), dtype="float32") = packed_params[62]
            encoder_block_3_block_0_block_2_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[63]
            encoder_block_3_block_0_block_3_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[64]
            encoder_block_3_block_0_block_3_weight_v: R.Tensor((256, 256, 1), dtype="float32") = packed_params[65]
            encoder_block_3_block_0_block_3_bias: R.Tensor((256,), dtype="float32") = packed_params[66]
            encoder_block_3_block_1_block_0_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[67]
            encoder_block_3_block_1_block_1_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[68]
            encoder_block_3_block_1_block_1_weight_v: R.Tensor((256, 256, 7), dtype="float32") = packed_params[69]
            encoder_block_3_block_1_block_1_bias: R.Tensor((256,), dtype="float32") = packed_params[70]
            encoder_block_3_block_1_block_2_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[71]
            encoder_block_3_block_1_block_3_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[72]
            encoder_block_3_block_1_block_3_weight_v: R.Tensor((256, 256, 1), dtype="float32") = packed_params[73]
            encoder_block_3_block_1_block_3_bias: R.Tensor((256,), dtype="float32") = packed_params[74]
            encoder_block_3_block_2_block_0_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[75]
            encoder_block_3_block_2_block_1_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[76]
            encoder_block_3_block_2_block_1_weight_v: R.Tensor((256, 256, 7), dtype="float32") = packed_params[77]
            encoder_block_3_block_2_block_1_bias: R.Tensor((256,), dtype="float32") = packed_params[78]
            encoder_block_3_block_2_block_2_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[79]
            encoder_block_3_block_2_block_3_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[80]
            encoder_block_3_block_2_block_3_weight_v: R.Tensor((256, 256, 1), dtype="float32") = packed_params[81]
            encoder_block_3_block_2_block_3_bias: R.Tensor((256,), dtype="float32") = packed_params[82]
            encoder_block_3_block_3_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[83]
            encoder_block_3_block_4_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[84]
            encoder_block_3_block_4_weight_v: R.Tensor((512, 256, 16), dtype="float32") = packed_params[85]
            encoder_block_3_block_4_bias: R.Tensor((512,), dtype="float32") = packed_params[86]
            encoder_block_4_block_0_block_0_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[87]
            encoder_block_4_block_0_block_1_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[88]
            encoder_block_4_block_0_block_1_weight_v: R.Tensor((512, 512, 7), dtype="float32") = packed_params[89]
            encoder_block_4_block_0_block_1_bias: R.Tensor((512,), dtype="float32") = packed_params[90]
            encoder_block_4_block_0_block_2_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[91]
            encoder_block_4_block_0_block_3_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[92]
            encoder_block_4_block_0_block_3_weight_v: R.Tensor((512, 512, 1), dtype="float32") = packed_params[93]
            encoder_block_4_block_0_block_3_bias: R.Tensor((512,), dtype="float32") = packed_params[94]
            encoder_block_4_block_1_block_0_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[95]
            encoder_block_4_block_1_block_1_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[96]
            encoder_block_4_block_1_block_1_weight_v: R.Tensor((512, 512, 7), dtype="float32") = packed_params[97]
            encoder_block_4_block_1_block_1_bias: R.Tensor((512,), dtype="float32") = packed_params[98]
            encoder_block_4_block_1_block_2_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[99]
            encoder_block_4_block_1_block_3_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[100]
            encoder_block_4_block_1_block_3_weight_v: R.Tensor((512, 512, 1), dtype="float32") = packed_params[101]
            encoder_block_4_block_1_block_3_bias: R.Tensor((512,), dtype="float32") = packed_params[102]
            encoder_block_4_block_2_block_0_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[103]
            encoder_block_4_block_2_block_1_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[104]
            encoder_block_4_block_2_block_1_weight_v: R.Tensor((512, 512, 7), dtype="float32") = packed_params[105]
            encoder_block_4_block_2_block_1_bias: R.Tensor((512,), dtype="float32") = packed_params[106]
            encoder_block_4_block_2_block_2_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[107]
            encoder_block_4_block_2_block_3_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[108]
            encoder_block_4_block_2_block_3_weight_v: R.Tensor((512, 512, 1), dtype="float32") = packed_params[109]
            encoder_block_4_block_2_block_3_bias: R.Tensor((512,), dtype="float32") = packed_params[110]
            encoder_block_4_block_3_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[111]
            encoder_block_4_block_4_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[112]
            encoder_block_4_block_4_weight_v: R.Tensor((1024, 512, 16), dtype="float32") = packed_params[113]
            encoder_block_4_block_4_bias: R.Tensor((1024,), dtype="float32") = packed_params[114]
            encoder_block_5_alpha: R.Tensor((1, 1024, 1), dtype="float32") = packed_params[115]
            encoder_block_6_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[116]
            encoder_block_6_weight_v: R.Tensor((1024, 1024, 3), dtype="float32") = packed_params[117]
            encoder_block_6_bias: R.Tensor((1024,), dtype="float32") = packed_params[118]
            quantizer_quantizers_0_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[119]
            quantizer_quantizers_0_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[120]
            quantizer_quantizers_0_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[121]
            quantizer_quantizers_0_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[122]
            quantizer_quantizers_0_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[123]
            quantizer_quantizers_0_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[124]
            quantizer_quantizers_0_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[125]
            quantizer_quantizers_1_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[126]
            quantizer_quantizers_1_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[127]
            quantizer_quantizers_1_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[128]
            quantizer_quantizers_1_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[129]
            quantizer_quantizers_1_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[130]
            quantizer_quantizers_1_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[131]
            quantizer_quantizers_1_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[132]
            quantizer_quantizers_2_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[133]
            quantizer_quantizers_2_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[134]
            quantizer_quantizers_2_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[135]
            quantizer_quantizers_2_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[136]
            quantizer_quantizers_2_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[137]
            quantizer_quantizers_2_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[138]
            quantizer_quantizers_2_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[139]
            quantizer_quantizers_3_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[140]
            quantizer_quantizers_3_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[141]
            quantizer_quantizers_3_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[142]
            quantizer_quantizers_3_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[143]
            quantizer_quantizers_3_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[144]
            quantizer_quantizers_3_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[145]
            quantizer_quantizers_3_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[146]
            quantizer_quantizers_4_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[147]
            quantizer_quantizers_4_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[148]
            quantizer_quantizers_4_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[149]
            quantizer_quantizers_4_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[150]
            quantizer_quantizers_4_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[151]
            quantizer_quantizers_4_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[152]
            quantizer_quantizers_4_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[153]
            quantizer_quantizers_5_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[154]
            quantizer_quantizers_5_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[155]
            quantizer_quantizers_5_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[156]
            quantizer_quantizers_5_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[157]
            quantizer_quantizers_5_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[158]
            quantizer_quantizers_5_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[159]
            quantizer_quantizers_5_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[160]
            quantizer_quantizers_6_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[161]
            quantizer_quantizers_6_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[162]
            quantizer_quantizers_6_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[163]
            quantizer_quantizers_6_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[164]
            quantizer_quantizers_6_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[165]
            quantizer_quantizers_6_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[166]
            quantizer_quantizers_6_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[167]
            quantizer_quantizers_7_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[168]
            quantizer_quantizers_7_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[169]
            quantizer_quantizers_7_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[170]
            quantizer_quantizers_7_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[171]
            quantizer_quantizers_7_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[172]
            quantizer_quantizers_7_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[173]
            quantizer_quantizers_7_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[174]
            quantizer_quantizers_8_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[175]
            quantizer_quantizers_8_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[176]
            quantizer_quantizers_8_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[177]
            quantizer_quantizers_8_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[178]
            quantizer_quantizers_8_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[179]
            quantizer_quantizers_8_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[180]
            quantizer_quantizers_8_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[181]
            decoder_model_0_weight_g: R.Tensor((1536, 1, 1), dtype="float32") = packed_params[182]
            decoder_model_0_weight_v: R.Tensor((1536, 1024, 7), dtype="float32") = packed_params[183]
            decoder_model_0_bias: R.Tensor((1536,), dtype="float32") = packed_params[184]
            decoder_model_1_block_0_alpha: R.Tensor((1, 1536, 1), dtype="float32") = packed_params[185]
            decoder_model_1_block_1_weight_g: R.Tensor((1536, 1, 1), dtype="float32") = packed_params[186]
            decoder_model_1_block_1_weight_v: R.Tensor((1536, 768, 16), dtype="float32") = packed_params[187]
            decoder_model_1_block_1_bias: R.Tensor((768,), dtype="float32") = packed_params[188]
            decoder_model_1_block_2_block_0_alpha: R.Tensor((1, 768, 1), dtype="float32") = packed_params[189]
            decoder_model_1_block_2_block_1_weight_g: R.Tensor((768, 1, 1), dtype="float32") = packed_params[190]
            decoder_model_1_block_2_block_1_weight_v: R.Tensor((768, 768, 7), dtype="float32") = packed_params[191]
            decoder_model_1_block_2_block_1_bias: R.Tensor((768,), dtype="float32") = packed_params[192]
            decoder_model_1_block_2_block_2_alpha: R.Tensor((1, 768, 1), dtype="float32") = packed_params[193]
            decoder_model_1_block_2_block_3_weight_g: R.Tensor((768, 1, 1), dtype="float32") = packed_params[194]
            decoder_model_1_block_2_block_3_weight_v: R.Tensor((768, 768, 1), dtype="float32") = packed_params[195]
            decoder_model_1_block_2_block_3_bias: R.Tensor((768,), dtype="float32") = packed_params[196]
            decoder_model_1_block_3_block_0_alpha: R.Tensor((1, 768, 1), dtype="float32") = packed_params[197]
            decoder_model_1_block_3_block_1_weight_g: R.Tensor((768, 1, 1), dtype="float32") = packed_params[198]
            decoder_model_1_block_3_block_1_weight_v: R.Tensor((768, 768, 7), dtype="float32") = packed_params[199]
            decoder_model_1_block_3_block_1_bias: R.Tensor((768,), dtype="float32") = packed_params[200]
            decoder_model_1_block_3_block_2_alpha: R.Tensor((1, 768, 1), dtype="float32") = packed_params[201]
            decoder_model_1_block_3_block_3_weight_g: R.Tensor((768, 1, 1), dtype="float32") = packed_params[202]
            decoder_model_1_block_3_block_3_weight_v: R.Tensor((768, 768, 1), dtype="float32") = packed_params[203]
            decoder_model_1_block_3_block_3_bias: R.Tensor((768,), dtype="float32") = packed_params[204]
            decoder_model_1_block_4_block_0_alpha: R.Tensor((1, 768, 1), dtype="float32") = packed_params[205]
            decoder_model_1_block_4_block_1_weight_g: R.Tensor((768, 1, 1), dtype="float32") = packed_params[206]
            decoder_model_1_block_4_block_1_weight_v: R.Tensor((768, 768, 7), dtype="float32") = packed_params[207]
            decoder_model_1_block_4_block_1_bias: R.Tensor((768,), dtype="float32") = packed_params[208]
            decoder_model_1_block_4_block_2_alpha: R.Tensor((1, 768, 1), dtype="float32") = packed_params[209]
            decoder_model_1_block_4_block_3_weight_g: R.Tensor((768, 1, 1), dtype="float32") = packed_params[210]
            decoder_model_1_block_4_block_3_weight_v: R.Tensor((768, 768, 1), dtype="float32") = packed_params[211]
            decoder_model_1_block_4_block_3_bias: R.Tensor((768,), dtype="float32") = packed_params[212]
            decoder_model_2_block_0_alpha: R.Tensor((1, 768, 1), dtype="float32") = packed_params[213]
            decoder_model_2_block_1_weight_g: R.Tensor((768, 1, 1), dtype="float32") = packed_params[214]
            decoder_model_2_block_1_weight_v: R.Tensor((768, 384, 16), dtype="float32") = packed_params[215]
            decoder_model_2_block_1_bias: R.Tensor((384,), dtype="float32") = packed_params[216]
            decoder_model_2_block_2_block_0_alpha: R.Tensor((1, 384, 1), dtype="float32") = packed_params[217]
            decoder_model_2_block_2_block_1_weight_g: R.Tensor((384, 1, 1), dtype="float32") = packed_params[218]
            decoder_model_2_block_2_block_1_weight_v: R.Tensor((384, 384, 7), dtype="float32") = packed_params[219]
            decoder_model_2_block_2_block_1_bias: R.Tensor((384,), dtype="float32") = packed_params[220]
            decoder_model_2_block_2_block_2_alpha: R.Tensor((1, 384, 1), dtype="float32") = packed_params[221]
            decoder_model_2_block_2_block_3_weight_g: R.Tensor((384, 1, 1), dtype="float32") = packed_params[222]
            decoder_model_2_block_2_block_3_weight_v: R.Tensor((384, 384, 1), dtype="float32") = packed_params[223]
            decoder_model_2_block_2_block_3_bias: R.Tensor((384,), dtype="float32") = packed_params[224]
            decoder_model_2_block_3_block_0_alpha: R.Tensor((1, 384, 1), dtype="float32") = packed_params[225]
            decoder_model_2_block_3_block_1_weight_g: R.Tensor((384, 1, 1), dtype="float32") = packed_params[226]
            decoder_model_2_block_3_block_1_weight_v: R.Tensor((384, 384, 7), dtype="float32") = packed_params[227]
            decoder_model_2_block_3_block_1_bias: R.Tensor((384,), dtype="float32") = packed_params[228]
            decoder_model_2_block_3_block_2_alpha: R.Tensor((1, 384, 1), dtype="float32") = packed_params[229]
            decoder_model_2_block_3_block_3_weight_g: R.Tensor((384, 1, 1), dtype="float32") = packed_params[230]
            decoder_model_2_block_3_block_3_weight_v: R.Tensor((384, 384, 1), dtype="float32") = packed_params[231]
            decoder_model_2_block_3_block_3_bias: R.Tensor((384,), dtype="float32") = packed_params[232]
            decoder_model_2_block_4_block_0_alpha: R.Tensor((1, 384, 1), dtype="float32") = packed_params[233]
            decoder_model_2_block_4_block_1_weight_g: R.Tensor((384, 1, 1), dtype="float32") = packed_params[234]
            decoder_model_2_block_4_block_1_weight_v: R.Tensor((384, 384, 7), dtype="float32") = packed_params[235]
            decoder_model_2_block_4_block_1_bias: R.Tensor((384,), dtype="float32") = packed_params[236]
            decoder_model_2_block_4_block_2_alpha: R.Tensor((1, 384, 1), dtype="float32") = packed_params[237]
            decoder_model_2_block_4_block_3_weight_g: R.Tensor((384, 1, 1), dtype="float32") = packed_params[238]
            decoder_model_2_block_4_block_3_weight_v: R.Tensor((384, 384, 1), dtype="float32") = packed_params[239]
            decoder_model_2_block_4_block_3_bias: R.Tensor((384,), dtype="float32") = packed_params[240]
            decoder_model_3_block_0_alpha: R.Tensor((1, 384, 1), dtype="float32") = packed_params[241]
            decoder_model_3_block_1_weight_g: R.Tensor((384, 1, 1), dtype="float32") = packed_params[242]
            decoder_model_3_block_1_weight_v: R.Tensor((384, 192, 8), dtype="float32") = packed_params[243]
            decoder_model_3_block_1_bias: R.Tensor((192,), dtype="float32") = packed_params[244]
            decoder_model_3_block_2_block_0_alpha: R.Tensor((1, 192, 1), dtype="float32") = packed_params[245]
            decoder_model_3_block_2_block_1_weight_g: R.Tensor((192, 1, 1), dtype="float32") = packed_params[246]
            decoder_model_3_block_2_block_1_weight_v: R.Tensor((192, 192, 7), dtype="float32") = packed_params[247]
            decoder_model_3_block_2_block_1_bias: R.Tensor((192,), dtype="float32") = packed_params[248]
            decoder_model_3_block_2_block_2_alpha: R.Tensor((1, 192, 1), dtype="float32") = packed_params[249]
            decoder_model_3_block_2_block_3_weight_g: R.Tensor((192, 1, 1), dtype="float32") = packed_params[250]
            decoder_model_3_block_2_block_3_weight_v: R.Tensor((192, 192, 1), dtype="float32") = packed_params[251]
            decoder_model_3_block_2_block_3_bias: R.Tensor((192,), dtype="float32") = packed_params[252]
            decoder_model_3_block_3_block_0_alpha: R.Tensor((1, 192, 1), dtype="float32") = packed_params[253]
            decoder_model_3_block_3_block_1_weight_g: R.Tensor((192, 1, 1), dtype="float32") = packed_params[254]
            decoder_model_3_block_3_block_1_weight_v: R.Tensor((192, 192, 7), dtype="float32") = packed_params[255]
            decoder_model_3_block_3_block_1_bias: R.Tensor((192,), dtype="float32") = packed_params[256]
            decoder_model_3_block_3_block_2_alpha: R.Tensor((1, 192, 1), dtype="float32") = packed_params[257]
            decoder_model_3_block_3_block_3_weight_g: R.Tensor((192, 1, 1), dtype="float32") = packed_params[258]
            decoder_model_3_block_3_block_3_weight_v: R.Tensor((192, 192, 1), dtype="float32") = packed_params[259]
            decoder_model_3_block_3_block_3_bias: R.Tensor((192,), dtype="float32") = packed_params[260]
            decoder_model_3_block_4_block_0_alpha: R.Tensor((1, 192, 1), dtype="float32") = packed_params[261]
            decoder_model_3_block_4_block_1_weight_g: R.Tensor((192, 1, 1), dtype="float32") = packed_params[262]
            decoder_model_3_block_4_block_1_weight_v: R.Tensor((192, 192, 7), dtype="float32") = packed_params[263]
            decoder_model_3_block_4_block_1_bias: R.Tensor((192,), dtype="float32") = packed_params[264]
            decoder_model_3_block_4_block_2_alpha: R.Tensor((1, 192, 1), dtype="float32") = packed_params[265]
            decoder_model_3_block_4_block_3_weight_g: R.Tensor((192, 1, 1), dtype="float32") = packed_params[266]
            decoder_model_3_block_4_block_3_weight_v: R.Tensor((192, 192, 1), dtype="float32") = packed_params[267]
            decoder_model_3_block_4_block_3_bias: R.Tensor((192,), dtype="float32") = packed_params[268]
            decoder_model_4_block_0_alpha: R.Tensor((1, 192, 1), dtype="float32") = packed_params[269]
            decoder_model_4_block_1_weight_g: R.Tensor((192, 1, 1), dtype="float32") = packed_params[270]
            decoder_model_4_block_1_weight_v: R.Tensor((192, 96, 4), dtype="float32") = packed_params[271]
            decoder_model_4_block_1_bias: R.Tensor((96,), dtype="float32") = packed_params[272]
            decoder_model_4_block_2_block_0_alpha: R.Tensor((1, 96, 1), dtype="float32") = packed_params[273]
            decoder_model_4_block_2_block_1_weight_g: R.Tensor((96, 1, 1), dtype="float32") = packed_params[274]
            decoder_model_4_block_2_block_1_weight_v: R.Tensor((96, 96, 7), dtype="float32") = packed_params[275]
            decoder_model_4_block_2_block_1_bias: R.Tensor((96,), dtype="float32") = packed_params[276]
            decoder_model_4_block_2_block_2_alpha: R.Tensor((1, 96, 1), dtype="float32") = packed_params[277]
            decoder_model_4_block_2_block_3_weight_g: R.Tensor((96, 1, 1), dtype="float32") = packed_params[278]
            decoder_model_4_block_2_block_3_weight_v: R.Tensor((96, 96, 1), dtype="float32") = packed_params[279]
            decoder_model_4_block_2_block_3_bias: R.Tensor((96,), dtype="float32") = packed_params[280]
            decoder_model_4_block_3_block_0_alpha: R.Tensor((1, 96, 1), dtype="float32") = packed_params[281]
            decoder_model_4_block_3_block_1_weight_g: R.Tensor((96, 1, 1), dtype="float32") = packed_params[282]
            decoder_model_4_block_3_block_1_weight_v: R.Tensor((96, 96, 7), dtype="float32") = packed_params[283]
            decoder_model_4_block_3_block_1_bias: R.Tensor((96,), dtype="float32") = packed_params[284]
            decoder_model_4_block_3_block_2_alpha: R.Tensor((1, 96, 1), dtype="float32") = packed_params[285]
            decoder_model_4_block_3_block_3_weight_g: R.Tensor((96, 1, 1), dtype="float32") = packed_params[286]
            decoder_model_4_block_3_block_3_weight_v: R.Tensor((96, 96, 1), dtype="float32") = packed_params[287]
            decoder_model_4_block_3_block_3_bias: R.Tensor((96,), dtype="float32") = packed_params[288]
            decoder_model_4_block_4_block_0_alpha: R.Tensor((1, 96, 1), dtype="float32") = packed_params[289]
            decoder_model_4_block_4_block_1_weight_g: R.Tensor((96, 1, 1), dtype="float32") = packed_params[290]
            decoder_model_4_block_4_block_1_weight_v: R.Tensor((96, 96, 7), dtype="float32") = packed_params[291]
            decoder_model_4_block_4_block_1_bias: R.Tensor((96,), dtype="float32") = packed_params[292]
            decoder_model_4_block_4_block_2_alpha: R.Tensor((1, 96, 1), dtype="float32") = packed_params[293]
            decoder_model_4_block_4_block_3_weight_g: R.Tensor((96, 1, 1), dtype="float32") = packed_params[294]
            decoder_model_4_block_4_block_3_weight_v: R.Tensor((96, 96, 1), dtype="float32") = packed_params[295]
            decoder_model_4_block_4_block_3_bias: R.Tensor((96,), dtype="float32") = packed_params[296]
            decoder_model_5_alpha: R.Tensor((1, 96, 1), dtype="float32") = packed_params[297]
            decoder_model_6_weight_g: R.Tensor((1, 1, 1), dtype="float32") = packed_params[298]
            decoder_model_6_weight_v: R.Tensor((1, 96, 7), dtype="float32") = packed_params[299]
            decoder_model_6_bias: R.Tensor((1,), dtype="float32") = packed_params[300]
            lv297 = R.call_tir(cls.fused_tir_square14_sum14, (encoder_block_0_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv298 = R.call_tir(cls.fused_tir_sqrt6_divide14_multiply14, (lv297, encoder_block_0_weight_v, encoder_block_0_weight_g), out_sinfo=R.Tensor((64, 1, 7), dtype="float32"))
            lv5 = R.call_tir(cls.reshape11, (encoder_block_0_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv299 = R.call_tir(cls.fused_conv1d18_add10, (audio_data, lv298, lv5), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape = R.call_tir(cls.reshape12, (lv299,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv6 = R.call_tir(cls.snake, (reshape, encoder_block_1_block_0_block_0_alpha), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape1 = R.call_tir(cls.reshape12, (lv6,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv300 = R.call_tir(cls.fused_tir_square15_sum15, (encoder_block_1_block_0_block_1_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv301 = R.call_tir(cls.fused_tir_sqrt6_divide15_multiply15, (lv300, encoder_block_1_block_0_block_1_weight_v, encoder_block_1_block_0_block_1_weight_g), out_sinfo=R.Tensor((64, 64, 7), dtype="float32"))
            lv12 = R.call_tir(cls.reshape11, (encoder_block_1_block_0_block_1_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv302 = R.call_tir(cls.fused_conv1d19_add10, (reshape1, lv301, lv12), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape2 = R.call_tir(cls.reshape12, (lv302,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv13 = R.call_tir(cls.snake, (reshape2, encoder_block_1_block_0_block_2_alpha), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape3 = R.call_tir(cls.reshape12, (lv13,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv303 = R.call_tir(cls.fused_tir_square16_sum16, (encoder_block_1_block_0_block_3_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv304 = R.call_tir(cls.fused_tir_sqrt6_divide16_multiply16, (lv303, encoder_block_1_block_0_block_3_weight_v, encoder_block_1_block_0_block_3_weight_g), out_sinfo=R.Tensor((64, 64, 1), dtype="float32"))
            lv19 = R.call_tir(cls.reshape11, (encoder_block_1_block_0_block_3_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv305 = R.call_tir(cls.fused_conv1d20_add10_add11, (reshape3, lv304, lv19, lv299), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape4 = R.call_tir(cls.reshape12, (lv305,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv20 = R.call_tir(cls.snake, (reshape4, encoder_block_1_block_1_block_0_alpha), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape5 = R.call_tir(cls.reshape12, (lv20,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv306 = R.call_tir(cls.fused_tir_square15_sum15, (encoder_block_1_block_1_block_1_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv307 = R.call_tir(cls.fused_tir_sqrt6_divide15_multiply15, (lv306, encoder_block_1_block_1_block_1_weight_v, encoder_block_1_block_1_block_1_weight_g), out_sinfo=R.Tensor((64, 64, 7), dtype="float32"))
            lv26 = R.call_tir(cls.reshape11, (encoder_block_1_block_1_block_1_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv308 = R.call_tir(cls.fused_conv1d21_add10, (reshape5, lv307, lv26), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape6 = R.call_tir(cls.reshape12, (lv308,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv27 = R.call_tir(cls.snake, (reshape6, encoder_block_1_block_1_block_2_alpha), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape7 = R.call_tir(cls.reshape12, (lv27,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv309 = R.call_tir(cls.fused_tir_square16_sum16, (encoder_block_1_block_1_block_3_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv310 = R.call_tir(cls.fused_tir_sqrt6_divide16_multiply16, (lv309, encoder_block_1_block_1_block_3_weight_v, encoder_block_1_block_1_block_3_weight_g), out_sinfo=R.Tensor((64, 64, 1), dtype="float32"))
            lv33 = R.call_tir(cls.reshape11, (encoder_block_1_block_1_block_3_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv311 = R.call_tir(cls.fused_conv1d20_add10_add11, (reshape7, lv310, lv33, lv305), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape8 = R.call_tir(cls.reshape12, (lv311,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv34 = R.call_tir(cls.snake, (reshape8, encoder_block_1_block_2_block_0_alpha), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape9 = R.call_tir(cls.reshape12, (lv34,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv312 = R.call_tir(cls.fused_tir_square15_sum15, (encoder_block_1_block_2_block_1_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv313 = R.call_tir(cls.fused_tir_sqrt6_divide15_multiply15, (lv312, encoder_block_1_block_2_block_1_weight_v, encoder_block_1_block_2_block_1_weight_g), out_sinfo=R.Tensor((64, 64, 7), dtype="float32"))
            lv40 = R.call_tir(cls.reshape11, (encoder_block_1_block_2_block_1_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv314 = R.call_tir(cls.fused_conv1d22_add10, (reshape9, lv313, lv40), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape10 = R.call_tir(cls.reshape12, (lv314,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv41 = R.call_tir(cls.snake, (reshape10, encoder_block_1_block_2_block_2_alpha), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape11 = R.call_tir(cls.reshape12, (lv41,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv315 = R.call_tir(cls.fused_tir_square16_sum16, (encoder_block_1_block_2_block_3_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv316 = R.call_tir(cls.fused_tir_sqrt6_divide16_multiply16, (lv315, encoder_block_1_block_2_block_3_weight_v, encoder_block_1_block_2_block_3_weight_g), out_sinfo=R.Tensor((64, 64, 1), dtype="float32"))
            lv47 = R.call_tir(cls.reshape11, (encoder_block_1_block_2_block_3_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv317 = R.call_tir(cls.fused_conv1d20_add10_add11, (reshape11, lv316, lv47, lv311), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape12 = R.call_tir(cls.reshape12, (lv317,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv48 = R.call_tir(cls.snake, (reshape12, encoder_block_1_block_3_alpha), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            reshape13 = R.call_tir(cls.reshape12, (lv48,), out_sinfo=R.Tensor((batch_size, 64, seq_len), dtype="float32"))
            lv318 = R.call_tir(cls.fused_tir_square17_sum17, (encoder_block_1_block_4_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv319 = R.call_tir(cls.fused_tir_sqrt7_divide17_multiply17, (lv318, encoder_block_1_block_4_weight_v, encoder_block_1_block_4_weight_g), out_sinfo=R.Tensor((128, 64, 4), dtype="float32"))
            lv54 = R.call_tir(cls.reshape13, (encoder_block_1_block_4_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv320 = R.call_tir(cls.fused_conv1d23_add12, (reshape13, lv319, lv54), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"))
            reshape14 = R.call_tir(cls.reshape14, (lv320,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv55 = R.call_tir(cls.snake1, (reshape14, encoder_block_2_block_0_block_0_alpha), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape15 = R.call_tir(cls.reshape14, (lv55,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv321 = R.call_tir(cls.fused_tir_square18_sum18, (encoder_block_2_block_0_block_1_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv322 = R.call_tir(cls.fused_tir_sqrt7_divide18_multiply18, (lv321, encoder_block_2_block_0_block_1_weight_v, encoder_block_2_block_0_block_1_weight_g), out_sinfo=R.Tensor((128, 128, 7), dtype="float32"))
            lv61 = R.call_tir(cls.reshape13, (encoder_block_2_block_0_block_1_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv323 = R.call_tir(cls.fused_conv1d24_add12, (reshape15, lv322, lv61), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape16 = R.call_tir(cls.reshape14, (lv323,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv62 = R.call_tir(cls.snake1, (reshape16, encoder_block_2_block_0_block_2_alpha), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape17 = R.call_tir(cls.reshape14, (lv62,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv324 = R.call_tir(cls.fused_tir_square19_sum19, (encoder_block_2_block_0_block_3_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv325 = R.call_tir(cls.fused_tir_sqrt7_divide19_multiply19, (lv324, encoder_block_2_block_0_block_3_weight_v, encoder_block_2_block_0_block_3_weight_g), out_sinfo=R.Tensor((128, 128, 1), dtype="float32"))
            lv68 = R.call_tir(cls.reshape13, (encoder_block_2_block_0_block_3_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv326 = R.call_tir(cls.fused_conv1d25_add12_add13, (reshape17, lv325, lv68, lv320), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape18 = R.call_tir(cls.reshape14, (lv326,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv69 = R.call_tir(cls.snake2, (reshape18, encoder_block_2_block_1_block_0_alpha), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape19 = R.call_tir(cls.reshape14, (lv69,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv327 = R.call_tir(cls.fused_tir_square18_sum18, (encoder_block_2_block_1_block_1_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv328 = R.call_tir(cls.fused_tir_sqrt7_divide18_multiply18, (lv327, encoder_block_2_block_1_block_1_weight_v, encoder_block_2_block_1_block_1_weight_g), out_sinfo=R.Tensor((128, 128, 7), dtype="float32"))
            lv75 = R.call_tir(cls.reshape13, (encoder_block_2_block_1_block_1_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv329 = R.call_tir(cls.fused_conv1d26_add12, (reshape19, lv328, lv75), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape20 = R.call_tir(cls.reshape14, (lv329,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv76 = R.call_tir(cls.snake2, (reshape20, encoder_block_2_block_1_block_2_alpha), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape21 = R.call_tir(cls.reshape14, (lv76,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv330 = R.call_tir(cls.fused_tir_square19_sum19, (encoder_block_2_block_1_block_3_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv331 = R.call_tir(cls.fused_tir_sqrt7_divide19_multiply19, (lv330, encoder_block_2_block_1_block_3_weight_v, encoder_block_2_block_1_block_3_weight_g), out_sinfo=R.Tensor((128, 128, 1), dtype="float32"))
            lv82 = R.call_tir(cls.reshape13, (encoder_block_2_block_1_block_3_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv332 = R.call_tir(cls.fused_conv1d25_add12_add13, (reshape21, lv331, lv82, lv326), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape22 = R.call_tir(cls.reshape14, (lv332,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv83 = R.call_tir(cls.snake1, (reshape22, encoder_block_2_block_2_block_0_alpha), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape23 = R.call_tir(cls.reshape14, (lv83,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv333 = R.call_tir(cls.fused_tir_square18_sum18, (encoder_block_2_block_2_block_1_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv334 = R.call_tir(cls.fused_tir_sqrt7_divide18_multiply18, (lv333, encoder_block_2_block_2_block_1_weight_v, encoder_block_2_block_2_block_1_weight_g), out_sinfo=R.Tensor((128, 128, 7), dtype="float32"))
            lv89 = R.call_tir(cls.reshape13, (encoder_block_2_block_2_block_1_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv335 = R.call_tir(cls.fused_conv1d27_add12, (reshape23, lv334, lv89), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape24 = R.call_tir(cls.reshape14, (lv335,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv90 = R.call_tir(cls.snake2, (reshape24, encoder_block_2_block_2_block_2_alpha), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape25 = R.call_tir(cls.reshape14, (lv90,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv336 = R.call_tir(cls.fused_tir_square19_sum19, (encoder_block_2_block_2_block_3_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv337 = R.call_tir(cls.fused_tir_sqrt7_divide19_multiply19, (lv336, encoder_block_2_block_2_block_3_weight_v, encoder_block_2_block_2_block_3_weight_g), out_sinfo=R.Tensor((128, 128, 1), dtype="float32"))
            lv96 = R.call_tir(cls.reshape13, (encoder_block_2_block_2_block_3_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv338 = R.call_tir(cls.fused_conv1d25_add12_add13, (reshape25, lv337, lv96, lv332), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape26 = R.call_tir(cls.reshape14, (lv338,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv97 = R.call_tir(cls.snake2, (reshape26, encoder_block_2_block_3_alpha), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape27 = R.call_tir(cls.reshape14, (lv97,), out_sinfo=R.Tensor((batch_size, 128, seq_len // 2), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv339 = R.call_tir(cls.fused_tir_square20_sum20, (encoder_block_2_block_4_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv340 = R.call_tir(cls.fused_tir_sqrt8_divide20_multiply20, (lv339, encoder_block_2_block_4_weight_v, encoder_block_2_block_4_weight_g), out_sinfo=R.Tensor((256, 128, 8), dtype="float32"))
            lv103 = R.call_tir(cls.reshape15, (encoder_block_2_block_4_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv341 = R.call_tir(cls.fused_conv1d28_add14, (reshape27, lv340, lv103), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape28 = R.call_tir(cls.reshape16, (lv341,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv104 = R.call_tir(cls.snake3, (reshape28, encoder_block_3_block_0_block_0_alpha), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape29 = R.call_tir(cls.reshape16, (lv104,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv342 = R.call_tir(cls.fused_tir_square21_sum21, (encoder_block_3_block_0_block_1_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv343 = R.call_tir(cls.fused_tir_sqrt8_divide21_multiply21, (lv342, encoder_block_3_block_0_block_1_weight_v, encoder_block_3_block_0_block_1_weight_g), out_sinfo=R.Tensor((256, 256, 7), dtype="float32"))
            lv110 = R.call_tir(cls.reshape15, (encoder_block_3_block_0_block_1_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv344 = R.call_tir(cls.fused_conv1d29_add14, (reshape29, lv343, lv110), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape30 = R.call_tir(cls.reshape16, (lv344,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv111 = R.call_tir(cls.snake4, (reshape30, encoder_block_3_block_0_block_2_alpha), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape31 = R.call_tir(cls.reshape16, (lv111,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv345 = R.call_tir(cls.fused_tir_square22_sum22, (encoder_block_3_block_0_block_3_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv346 = R.call_tir(cls.fused_tir_sqrt8_divide22_multiply22, (lv345, encoder_block_3_block_0_block_3_weight_v, encoder_block_3_block_0_block_3_weight_g), out_sinfo=R.Tensor((256, 256, 1), dtype="float32"))
            lv117 = R.call_tir(cls.reshape15, (encoder_block_3_block_0_block_3_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv347 = R.call_tir(cls.fused_conv1d30_add14_add15, (reshape31, lv346, lv117, lv341), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape32 = R.call_tir(cls.reshape16, (lv347,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv118 = R.call_tir(cls.snake3, (reshape32, encoder_block_3_block_1_block_0_alpha), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape33 = R.call_tir(cls.reshape16, (lv118,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv348 = R.call_tir(cls.fused_tir_square21_sum21, (encoder_block_3_block_1_block_1_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv349 = R.call_tir(cls.fused_tir_sqrt8_divide21_multiply21, (lv348, encoder_block_3_block_1_block_1_weight_v, encoder_block_3_block_1_block_1_weight_g), out_sinfo=R.Tensor((256, 256, 7), dtype="float32"))
            lv124 = R.call_tir(cls.reshape15, (encoder_block_3_block_1_block_1_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv350 = R.call_tir(cls.fused_conv1d31_add14, (reshape33, lv349, lv124), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape34 = R.call_tir(cls.reshape16, (lv350,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv125 = R.call_tir(cls.snake3, (reshape34, encoder_block_3_block_1_block_2_alpha), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape35 = R.call_tir(cls.reshape16, (lv125,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv351 = R.call_tir(cls.fused_tir_square22_sum22, (encoder_block_3_block_1_block_3_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv352 = R.call_tir(cls.fused_tir_sqrt8_divide22_multiply22, (lv351, encoder_block_3_block_1_block_3_weight_v, encoder_block_3_block_1_block_3_weight_g), out_sinfo=R.Tensor((256, 256, 1), dtype="float32"))
            lv131 = R.call_tir(cls.reshape15, (encoder_block_3_block_1_block_3_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv353 = R.call_tir(cls.fused_conv1d30_add14_add15, (reshape35, lv352, lv131, lv347), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape36 = R.call_tir(cls.reshape16, (lv353,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv132 = R.call_tir(cls.snake3, (reshape36, encoder_block_3_block_2_block_0_alpha), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape37 = R.call_tir(cls.reshape16, (lv132,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv354 = R.call_tir(cls.fused_tir_square21_sum21, (encoder_block_3_block_2_block_1_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv355 = R.call_tir(cls.fused_tir_sqrt8_divide21_multiply21, (lv354, encoder_block_3_block_2_block_1_weight_v, encoder_block_3_block_2_block_1_weight_g), out_sinfo=R.Tensor((256, 256, 7), dtype="float32"))
            lv138 = R.call_tir(cls.reshape15, (encoder_block_3_block_2_block_1_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv356 = R.call_tir(cls.fused_conv1d32_add14, (reshape37, lv355, lv138), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape38 = R.call_tir(cls.reshape16, (lv356,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv139 = R.call_tir(cls.snake3, (reshape38, encoder_block_3_block_2_block_2_alpha), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape39 = R.call_tir(cls.reshape16, (lv139,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv357 = R.call_tir(cls.fused_tir_square22_sum22, (encoder_block_3_block_2_block_3_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv358 = R.call_tir(cls.fused_tir_sqrt8_divide22_multiply22, (lv357, encoder_block_3_block_2_block_3_weight_v, encoder_block_3_block_2_block_3_weight_g), out_sinfo=R.Tensor((256, 256, 1), dtype="float32"))
            lv145 = R.call_tir(cls.reshape15, (encoder_block_3_block_2_block_3_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv359 = R.call_tir(cls.fused_conv1d30_add14_add15, (reshape39, lv358, lv145, lv353), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape40 = R.call_tir(cls.reshape16, (lv359,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv146 = R.call_tir(cls.snake3, (reshape40, encoder_block_3_block_3_alpha), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape41 = R.call_tir(cls.reshape16, (lv146,), out_sinfo=R.Tensor((batch_size, 256, seq_len // 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv360 = R.call_tir(cls.fused_tir_square23_sum23, (encoder_block_3_block_4_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv361 = R.call_tir(cls.fused_tir_sqrt9_divide23_multiply23, (lv360, encoder_block_3_block_4_weight_v, encoder_block_3_block_4_weight_g), out_sinfo=R.Tensor((512, 256, 16), dtype="float32"))
            lv152 = R.call_tir(cls.reshape17, (encoder_block_3_block_4_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv362 = R.call_tir(cls.fused_conv1d33_add16, (reshape41, lv361, lv152), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape42 = R.call_tir(cls.reshape18, (lv362,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv153 = R.call_tir(cls.snake5, (reshape42, encoder_block_4_block_0_block_0_alpha), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape43 = R.call_tir(cls.reshape18, (lv153,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv363 = R.call_tir(cls.fused_tir_square24_sum24, (encoder_block_4_block_0_block_1_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv364 = R.call_tir(cls.fused_tir_sqrt9_divide24_multiply24, (lv363, encoder_block_4_block_0_block_1_weight_v, encoder_block_4_block_0_block_1_weight_g), out_sinfo=R.Tensor((512, 512, 7), dtype="float32"))
            lv159 = R.call_tir(cls.reshape17, (encoder_block_4_block_0_block_1_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv365 = R.call_tir(cls.fused_conv1d34_add16, (reshape43, lv364, lv159), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape44 = R.call_tir(cls.reshape18, (lv365,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv160 = R.call_tir(cls.snake5, (reshape44, encoder_block_4_block_0_block_2_alpha), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape45 = R.call_tir(cls.reshape18, (lv160,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv366 = R.call_tir(cls.fused_tir_square25_sum25, (encoder_block_4_block_0_block_3_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv367 = R.call_tir(cls.fused_tir_sqrt9_divide25_multiply25, (lv366, encoder_block_4_block_0_block_3_weight_v, encoder_block_4_block_0_block_3_weight_g), out_sinfo=R.Tensor((512, 512, 1), dtype="float32"))
            lv166 = R.call_tir(cls.reshape17, (encoder_block_4_block_0_block_3_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv368 = R.call_tir(cls.fused_conv1d35_add16_add17, (reshape45, lv367, lv166, lv362), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape46 = R.call_tir(cls.reshape18, (lv368,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv167 = R.call_tir(cls.snake5, (reshape46, encoder_block_4_block_1_block_0_alpha), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape47 = R.call_tir(cls.reshape18, (lv167,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv369 = R.call_tir(cls.fused_tir_square24_sum24, (encoder_block_4_block_1_block_1_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv370 = R.call_tir(cls.fused_tir_sqrt9_divide24_multiply24, (lv369, encoder_block_4_block_1_block_1_weight_v, encoder_block_4_block_1_block_1_weight_g), out_sinfo=R.Tensor((512, 512, 7), dtype="float32"))
            lv173 = R.call_tir(cls.reshape17, (encoder_block_4_block_1_block_1_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv371 = R.call_tir(cls.fused_conv1d36_add16, (reshape47, lv370, lv173), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape48 = R.call_tir(cls.reshape18, (lv371,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv174 = R.call_tir(cls.snake5, (reshape48, encoder_block_4_block_1_block_2_alpha), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape49 = R.call_tir(cls.reshape18, (lv174,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv372 = R.call_tir(cls.fused_tir_square25_sum25, (encoder_block_4_block_1_block_3_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv373 = R.call_tir(cls.fused_tir_sqrt9_divide25_multiply25, (lv372, encoder_block_4_block_1_block_3_weight_v, encoder_block_4_block_1_block_3_weight_g), out_sinfo=R.Tensor((512, 512, 1), dtype="float32"))
            lv180 = R.call_tir(cls.reshape17, (encoder_block_4_block_1_block_3_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv374 = R.call_tir(cls.fused_conv1d35_add16_add17, (reshape49, lv373, lv180, lv368), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape50 = R.call_tir(cls.reshape18, (lv374,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv181 = R.call_tir(cls.snake5, (reshape50, encoder_block_4_block_2_block_0_alpha), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape51 = R.call_tir(cls.reshape18, (lv181,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv375 = R.call_tir(cls.fused_tir_square24_sum24, (encoder_block_4_block_2_block_1_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv376 = R.call_tir(cls.fused_tir_sqrt9_divide24_multiply24, (lv375, encoder_block_4_block_2_block_1_weight_v, encoder_block_4_block_2_block_1_weight_g), out_sinfo=R.Tensor((512, 512, 7), dtype="float32"))
            lv187 = R.call_tir(cls.reshape17, (encoder_block_4_block_2_block_1_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv377 = R.call_tir(cls.fused_conv1d37_add16, (reshape51, lv376, lv187), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape52 = R.call_tir(cls.reshape18, (lv377,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv188 = R.call_tir(cls.snake5, (reshape52, encoder_block_4_block_2_block_2_alpha), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape53 = R.call_tir(cls.reshape18, (lv188,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv378 = R.call_tir(cls.fused_tir_square25_sum25, (encoder_block_4_block_2_block_3_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv379 = R.call_tir(cls.fused_tir_sqrt9_divide25_multiply25, (lv378, encoder_block_4_block_2_block_3_weight_v, encoder_block_4_block_2_block_3_weight_g), out_sinfo=R.Tensor((512, 512, 1), dtype="float32"))
            lv194 = R.call_tir(cls.reshape17, (encoder_block_4_block_2_block_3_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv380 = R.call_tir(cls.fused_conv1d35_add16_add17, (reshape53, lv379, lv194, lv374), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape54 = R.call_tir(cls.reshape18, (lv380,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv195 = R.call_tir(cls.snake5, (reshape54, encoder_block_4_block_3_alpha), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape55 = R.call_tir(cls.reshape18, (lv195,), out_sinfo=R.Tensor((batch_size, 512, seq_len // 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv381 = R.call_tir(cls.fused_tir_square26_sum26, (encoder_block_4_block_4_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv382 = R.call_tir(cls.fused_tir_sqrt10_divide26_multiply26, (lv381, encoder_block_4_block_4_weight_v, encoder_block_4_block_4_weight_g), out_sinfo=R.Tensor((1024, 512, 16), dtype="float32"))
            lv201 = R.call_tir(cls.reshape19, (encoder_block_4_block_4_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv383 = R.call_tir(cls.fused_conv1d38_add18, (reshape55, lv382, lv201), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape56 = R.call_tir(cls.reshape20, (lv383,), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv202 = R.call_tir(cls.snake6, (reshape56, encoder_block_5_alpha), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape57 = R.call_tir(cls.reshape20, (lv202,), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv384 = R.call_tir(cls.fused_tir_square27_sum27, (encoder_block_6_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv385 = R.call_tir(cls.fused_tir_sqrt10_divide27_multiply27, (lv384, encoder_block_6_weight_v, encoder_block_6_weight_g), out_sinfo=R.Tensor((1024, 1024, 3), dtype="float32"))
            lv208 = R.call_tir(cls.reshape19, (encoder_block_6_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv386 = R.call_tir(cls.fused_conv1d39_add18, (reshape57, lv385, lv208), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv387 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_0_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv388 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv387, quantizer_quantizers_0_in_proj_weight_v, quantizer_quantizers_0_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv214 = R.call_tir(cls.reshape21, (quantizer_quantizers_0_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv389 = R.call_tir(cls.fused_conv1d40_add19, (lv386, lv388, lv214), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims = R.call_tir(cls.transpose, (lv389,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape58 = R.call_tir(cls.reshape22, (permute_dims,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv390 = R.call_tir(cls.fused_tir_square29_sum29, (reshape58,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv391 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt12_divide32, (lv390, reshape58), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv392 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_0_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv393 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv392, quantizer_quantizers_0_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims1 = R.call_tir(cls.transpose1, (lv393,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv394 = R.call_tir(cls.fused_tir_square32_sum32, (lv391,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv395 = R.call_tir(cls.fused_tir_square30_sum30, (lv393,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims2 = R.call_tir(cls.transpose2, (lv395,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv396 = R.call_tir(cls.fused_matmul_multiply31_subtract_add20, (lv391, permute_dims1, lv394, permute_dims2), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv396, axis=1, descending=False, dtype="int32")
            take = R.call_tir(cls.take, (argsort, metadata["relax.expr.Constant"][9]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape59 = R.call_tir(cls.reshape23, (take,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape60 = R.call_tir(cls.reshape24, (reshape59,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take1 = R.call_tir(cls.take1, (quantizer_quantizers_0_codebook_weight, reshape60), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape61 = R.call_tir(cls.reshape25, (take1,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims3 = R.call_tir(cls.transpose3, (reshape61,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv397 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_0_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv398 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv397, quantizer_quantizers_0_out_proj_weight_v, quantizer_quantizers_0_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv220 = R.call_tir(cls.reshape19, (quantizer_quantizers_0_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv399 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims3, lv398, lv220), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract1 = R.call_tir(cls.subtract1, (lv386, lv399), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv400 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_1_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv401 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv400, quantizer_quantizers_1_in_proj_weight_v, quantizer_quantizers_1_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv226 = R.call_tir(cls.reshape21, (quantizer_quantizers_1_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv402 = R.call_tir(cls.fused_conv1d40_add19, (subtract1, lv401, lv226), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims4 = R.call_tir(cls.transpose, (lv402,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape62 = R.call_tir(cls.reshape22, (permute_dims4,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv403 = R.call_tir(cls.fused_tir_square29_sum32, (reshape62,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv404 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt14_divide32, (lv403, reshape62), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv405 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_1_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv406 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv405, quantizer_quantizers_1_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims5 = R.call_tir(cls.transpose1, (lv406,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv407 = R.call_tir(cls.fused_tir_square32_sum32, (lv404,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv408 = R.call_tir(cls.fused_tir_square30_sum30, (lv406,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims6 = R.call_tir(cls.transpose2, (lv408,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv409 = R.call_tir(cls.fused_matmul1_multiply31_subtract_add22, (lv404, permute_dims5, lv407, permute_dims6), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort1: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv409, axis=1, descending=False, dtype="int32")
            take2 = R.call_tir(cls.take, (argsort1, metadata["relax.expr.Constant"][10]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape63 = R.call_tir(cls.reshape23, (take2,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape64 = R.call_tir(cls.reshape24, (reshape63,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take3 = R.call_tir(cls.take2, (quantizer_quantizers_1_codebook_weight, reshape64), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            reshape65 = R.call_tir(cls.reshape25, (take3,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims7 = R.call_tir(cls.transpose3, (reshape65,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv410 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_1_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv411 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv410, quantizer_quantizers_1_out_proj_weight_v, quantizer_quantizers_1_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv232 = R.call_tir(cls.reshape19, (quantizer_quantizers_1_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv412 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims7, lv411, lv232), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract3 = R.call_tir(cls.subtract1, (subtract1, lv412), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv413 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_2_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv414 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv413, quantizer_quantizers_2_in_proj_weight_v, quantizer_quantizers_2_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv238 = R.call_tir(cls.reshape21, (quantizer_quantizers_2_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv415 = R.call_tir(cls.fused_conv1d40_add19, (subtract3, lv414, lv238), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims8 = R.call_tir(cls.transpose, (lv415,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape66 = R.call_tir(cls.reshape22, (permute_dims8,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv416 = R.call_tir(cls.fused_tir_square32_sum29, (reshape66,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv417 = R.call_tir(cls.fused_broadcast_to2_maximum_tir_sqrt14_divide32, (lv416, reshape66), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv418 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_2_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv419 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv418, quantizer_quantizers_2_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims9 = R.call_tir(cls.transpose1, (lv419,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv420 = R.call_tir(cls.fused_tir_square29_sum32, (lv417,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv421 = R.call_tir(cls.fused_tir_square30_sum30, (lv419,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims10 = R.call_tir(cls.transpose2, (lv421,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv422 = R.call_tir(cls.fused_matmul_multiply29_subtract_add20, (lv417, permute_dims9, lv420, permute_dims10), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort2: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv422, axis=1, descending=False, dtype="int32")
            take4 = R.call_tir(cls.take, (argsort2, metadata["relax.expr.Constant"][11]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape67 = R.call_tir(cls.reshape23, (take4,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape68 = R.call_tir(cls.reshape24, (reshape67,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take5 = R.call_tir(cls.take2, (quantizer_quantizers_2_codebook_weight, reshape68), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            reshape69 = R.call_tir(cls.reshape25, (take5,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims11 = R.call_tir(cls.transpose3, (reshape69,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv423 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_2_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv424 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv423, quantizer_quantizers_2_out_proj_weight_v, quantizer_quantizers_2_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv244 = R.call_tir(cls.reshape19, (quantizer_quantizers_2_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv425 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims11, lv424, lv244), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract5 = R.call_tir(cls.subtract1, (subtract3, lv425), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv426 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_3_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv427 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv426, quantizer_quantizers_3_in_proj_weight_v, quantizer_quantizers_3_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv250 = R.call_tir(cls.reshape21, (quantizer_quantizers_3_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv428 = R.call_tir(cls.fused_conv1d40_add19, (subtract5, lv427, lv250), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims12 = R.call_tir(cls.transpose, (lv428,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape70 = R.call_tir(cls.reshape22, (permute_dims12,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv429 = R.call_tir(cls.fused_tir_square29_sum29, (reshape70,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv430 = R.call_tir(cls.fused_broadcast_to2_maximum_tir_sqrt12_divide29, (lv429, reshape70), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv431 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_3_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv432 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv431, quantizer_quantizers_3_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims13 = R.call_tir(cls.transpose1, (lv432,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv433 = R.call_tir(cls.fused_tir_square32_sum29, (lv430,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv434 = R.call_tir(cls.fused_tir_square30_sum30, (lv432,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims14 = R.call_tir(cls.transpose2, (lv434,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv435 = R.call_tir(cls.fused_matmul_multiply29_subtract_add20, (lv430, permute_dims13, lv433, permute_dims14), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort3: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv435, axis=1, descending=False, dtype="int32")
            take6 = R.call_tir(cls.take, (argsort3, metadata["relax.expr.Constant"][12]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape71 = R.call_tir(cls.reshape23, (take6,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape72 = R.call_tir(cls.reshape24, (reshape71,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take7 = R.call_tir(cls.take2, (quantizer_quantizers_3_codebook_weight, reshape72), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            reshape73 = R.call_tir(cls.reshape25, (take7,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims15 = R.call_tir(cls.transpose3, (reshape73,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv436 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_3_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv437 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv436, quantizer_quantizers_3_out_proj_weight_v, quantizer_quantizers_3_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv256 = R.call_tir(cls.reshape19, (quantizer_quantizers_3_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv438 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims15, lv437, lv256), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract7 = R.call_tir(cls.subtract1, (subtract5, lv438), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv439 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_4_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv440 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv439, quantizer_quantizers_4_in_proj_weight_v, quantizer_quantizers_4_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv262 = R.call_tir(cls.reshape21, (quantizer_quantizers_4_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv441 = R.call_tir(cls.fused_conv1d40_add19, (subtract7, lv440, lv262), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims16 = R.call_tir(cls.transpose, (lv441,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape74 = R.call_tir(cls.reshape22, (permute_dims16,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv442 = R.call_tir(cls.fused_tir_square29_sum29, (reshape74,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv443 = R.call_tir(cls.fused_broadcast_to2_maximum_tir_sqrt12_divide29, (lv442, reshape74), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv444 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_4_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv445 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv444, quantizer_quantizers_4_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims17 = R.call_tir(cls.transpose1, (lv445,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv446 = R.call_tir(cls.fused_tir_square29_sum32, (lv443,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv447 = R.call_tir(cls.fused_tir_square30_sum30, (lv445,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims18 = R.call_tir(cls.transpose2, (lv447,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv448 = R.call_tir(cls.fused_matmul_multiply31_subtract_add20, (lv443, permute_dims17, lv446, permute_dims18), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort4: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv448, axis=1, descending=False, dtype="int32")
            take8 = R.call_tir(cls.take3, (argsort4, metadata["relax.expr.Constant"][13]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([seq_len, batch_size]))
            reshape75 = R.call_tir(cls.reshape23, (take8,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape76 = R.call_tir(cls.reshape24, (reshape75,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take9 = R.call_tir(cls.take1, (quantizer_quantizers_4_codebook_weight, reshape76), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape77 = R.call_tir(cls.reshape25, (take9,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims19 = R.call_tir(cls.transpose3, (reshape77,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv449 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_4_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv450 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv449, quantizer_quantizers_4_out_proj_weight_v, quantizer_quantizers_4_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv268 = R.call_tir(cls.reshape19, (quantizer_quantizers_4_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv451 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims19, lv450, lv268), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract9 = R.call_tir(cls.subtract1, (subtract7, lv451), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv452 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_5_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv453 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv452, quantizer_quantizers_5_in_proj_weight_v, quantizer_quantizers_5_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv274 = R.call_tir(cls.reshape21, (quantizer_quantizers_5_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv454 = R.call_tir(cls.fused_conv1d40_add19, (subtract9, lv453, lv274), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims20 = R.call_tir(cls.transpose, (lv454,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape78 = R.call_tir(cls.reshape22, (permute_dims20,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv455 = R.call_tir(cls.fused_tir_square29_sum29, (reshape78,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv456 = R.call_tir(cls.fused_broadcast_to_maximum2_tir_sqrt12_divide29, (lv455, reshape78), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv457 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_5_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv458 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv457, quantizer_quantizers_5_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims21 = R.call_tir(cls.transpose1, (lv458,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv459 = R.call_tir(cls.fused_tir_square29_sum29, (lv456,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv460 = R.call_tir(cls.fused_tir_square30_sum30, (lv458,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims22 = R.call_tir(cls.transpose2, (lv460,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv461 = R.call_tir(cls.fused_matmul1_multiply31_subtract_add20, (lv456, permute_dims21, lv459, permute_dims22), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort5: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv461, axis=1, descending=False, dtype="int32")
            take10 = R.call_tir(cls.take, (argsort5, metadata["relax.expr.Constant"][14]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape79 = R.call_tir(cls.reshape23, (take10,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape80 = R.call_tir(cls.reshape24, (reshape79,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take11 = R.call_tir(cls.take2, (quantizer_quantizers_5_codebook_weight, reshape80), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            reshape81 = R.call_tir(cls.reshape25, (take11,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims23 = R.call_tir(cls.transpose3, (reshape81,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv462 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_5_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv463 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv462, quantizer_quantizers_5_out_proj_weight_v, quantizer_quantizers_5_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv280 = R.call_tir(cls.reshape19, (quantizer_quantizers_5_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv464 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims23, lv463, lv280), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract11 = R.call_tir(cls.subtract1, (subtract9, lv464), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv465 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_6_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv466 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv465, quantizer_quantizers_6_in_proj_weight_v, quantizer_quantizers_6_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv286 = R.call_tir(cls.reshape21, (quantizer_quantizers_6_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv467 = R.call_tir(cls.fused_conv1d40_add19, (subtract11, lv466, lv286), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims24 = R.call_tir(cls.transpose, (lv467,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape82 = R.call_tir(cls.reshape22, (permute_dims24,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv468 = R.call_tir(cls.fused_tir_square29_sum29, (reshape82,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv469 = R.call_tir(cls.fused_broadcast_to2_maximum_tir_sqrt12_divide32, (lv468, reshape82), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv470 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_6_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv471 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv470, quantizer_quantizers_6_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims25 = R.call_tir(cls.transpose1, (lv471,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv472 = R.call_tir(cls.fused_tir_square32_sum32, (lv469,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv473 = R.call_tir(cls.fused_tir_square30_sum30, (lv471,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims26 = R.call_tir(cls.transpose2, (lv473,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv474 = R.call_tir(cls.fused_matmul1_multiply29_subtract2_add22, (lv469, permute_dims25, lv472, permute_dims26), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort6: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv474, axis=1, descending=False, dtype="int32")
            take12 = R.call_tir(cls.take3, (argsort6, metadata["relax.expr.Constant"][15]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([seq_len, batch_size]))
            reshape83 = R.call_tir(cls.reshape23, (take12,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape84 = R.call_tir(cls.reshape24, (reshape83,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take13 = R.call_tir(cls.take2, (quantizer_quantizers_6_codebook_weight, reshape84), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            reshape85 = R.call_tir(cls.reshape25, (take13,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims27 = R.call_tir(cls.transpose3, (reshape85,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv475 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_6_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv476 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv475, quantizer_quantizers_6_out_proj_weight_v, quantizer_quantizers_6_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv292 = R.call_tir(cls.reshape19, (quantizer_quantizers_6_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv477 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims27, lv476, lv292), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract13 = R.call_tir(cls.subtract1, (subtract11, lv477), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv478 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_7_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv479 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv478, quantizer_quantizers_7_in_proj_weight_v, quantizer_quantizers_7_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv298_1 = R.call_tir(cls.reshape21, (quantizer_quantizers_7_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv480 = R.call_tir(cls.fused_conv1d40_add19, (subtract13, lv479, lv298_1), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims28 = R.call_tir(cls.transpose, (lv480,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape86 = R.call_tir(cls.reshape22, (permute_dims28,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv481 = R.call_tir(cls.fused_tir_square32_sum29, (reshape86,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv482 = R.call_tir(cls.fused_broadcast_to2_maximum2_tir_sqrt12_divide32, (lv481, reshape86), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv483 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_7_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv484 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv483, quantizer_quantizers_7_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims29 = R.call_tir(cls.transpose1, (lv484,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv485 = R.call_tir(cls.fused_tir_square29_sum32, (lv482,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv486 = R.call_tir(cls.fused_tir_square30_sum30, (lv484,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims30 = R.call_tir(cls.transpose2, (lv486,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv487 = R.call_tir(cls.fused_matmul1_multiply29_subtract2_add22, (lv482, permute_dims29, lv485, permute_dims30), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort7: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv487, axis=1, descending=False, dtype="int32")
            take14 = R.call_tir(cls.take3, (argsort7, metadata["relax.expr.Constant"][16]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([seq_len, batch_size]))
            reshape87 = R.call_tir(cls.reshape23, (take14,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape88 = R.call_tir(cls.reshape24, (reshape87,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take15 = R.call_tir(cls.take1, (quantizer_quantizers_7_codebook_weight, reshape88), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape89 = R.call_tir(cls.reshape25, (take15,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims31 = R.call_tir(cls.transpose3, (reshape89,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv488 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_7_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv489 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv488, quantizer_quantizers_7_out_proj_weight_v, quantizer_quantizers_7_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv304_1 = R.call_tir(cls.reshape19, (quantizer_quantizers_7_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv490 = R.call_tir(cls.fused_conv1d41_add18, (permute_dims31, lv489, lv304_1), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            subtract15 = R.call_tir(cls.subtract1, (subtract13, lv490), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv491 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_8_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv492 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv491, quantizer_quantizers_8_in_proj_weight_v, quantizer_quantizers_8_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv310_1 = R.call_tir(cls.reshape21, (quantizer_quantizers_8_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv493 = R.call_tir(cls.fused_conv1d40_add19, (subtract15, lv492, lv310_1), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims32 = R.call_tir(cls.transpose, (lv493,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape90 = R.call_tir(cls.reshape22, (permute_dims32,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv494 = R.call_tir(cls.fused_tir_square29_sum32, (reshape90,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv495 = R.call_tir(cls.fused_broadcast_to2_maximum2_tir_sqrt14_divide29, (lv494, reshape90), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv496 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_8_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv497 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv496, quantizer_quantizers_8_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims33 = R.call_tir(cls.transpose1, (lv497,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv498 = R.call_tir(cls.fused_tir_square29_sum29, (lv495,), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            lv499 = R.call_tir(cls.fused_tir_square30_sum30, (lv497,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims34 = R.call_tir(cls.transpose2, (lv499,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv500 = R.call_tir(cls.fused_matmul_multiply31_subtract2_add20, (lv495, permute_dims33, lv498, permute_dims34), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1024), dtype="float32"), tir_vars=R.shape([seq_len, batch_size]))
            argsort8: R.Tensor((batch_size * (seq_len // 512), 1024), dtype="int32") = R.argsort(lv500, axis=1, descending=False, dtype="int32")
            take16 = R.call_tir(cls.take, (argsort8, metadata["relax.expr.Constant"][17]), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 1), dtype="int32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape91 = R.call_tir(cls.reshape23, (take16,), out_sinfo=R.Tensor((batch_size, seq_len // 512), dtype="int32"), tir_vars=R.shape([seq_len]))
            reshape92 = R.call_tir(cls.reshape24, (reshape91,), out_sinfo=R.Tensor((batch_size * (seq_len // 512),), dtype="int32"), tir_vars=R.shape([seq_len]))
            take17 = R.call_tir(cls.take1, (quantizer_quantizers_8_codebook_weight, reshape92), out_sinfo=R.Tensor((batch_size * (seq_len // 512), 8), dtype="float32"), tir_vars=R.shape([batch_size, seq_len]))
            reshape93 = R.call_tir(cls.reshape25, (take17,), out_sinfo=R.Tensor((batch_size, seq_len // 512, 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            permute_dims35 = R.call_tir(cls.transpose3, (reshape93,), out_sinfo=R.Tensor((batch_size, 8, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv501 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_8_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv502 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv501, quantizer_quantizers_8_out_proj_weight_v, quantizer_quantizers_8_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv316_1 = R.call_tir(cls.reshape19, (quantizer_quantizers_8_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv503 = R.call_tir(cls.fused_zeros_add21_add21_add21_add21_add21_add21_add21_add21_conv1d41_add18_add21, (lv399, lv412, lv425, lv438, lv451, lv464, lv477, lv490, permute_dims35, lv502, lv316_1), out_sinfo=R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv504 = R.call_tir(cls.fused_tir_square_sum, (decoder_model_0_weight_v,), out_sinfo=R.Tensor((1536, 1, 1), dtype="float32"))
            lv505 = R.call_tir(cls.fused_tir_sqrt_divide_multiply, (lv504, decoder_model_0_weight_v, decoder_model_0_weight_g), out_sinfo=R.Tensor((1536, 1024, 7), dtype="float32"))
            lv322_1 = R.call_tir(cls.reshape, (decoder_model_0_bias,), out_sinfo=R.Tensor((1, 1536, 1), dtype="float32"))
            lv506 = R.call_tir(cls.fused_conv1d42_add23, (lv503, lv505, lv322_1), out_sinfo=R.Tensor((batch_size, 1536, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape94 = R.call_tir(cls.reshape26, (lv506,), out_sinfo=R.Tensor((batch_size, 1536, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv323_1 = R.call_tir(cls.snake7, (reshape94, decoder_model_1_block_0_alpha), out_sinfo=R.Tensor((batch_size, 1536, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape95 = R.call_tir(cls.reshape26, (lv323_1,), out_sinfo=R.Tensor((batch_size, 1536, seq_len // 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv507 = R.call_tir(cls.fused_tir_square1_sum1, (decoder_model_1_block_1_weight_v,), out_sinfo=R.Tensor((1536, 1, 1), dtype="float32"))
            lv508 = R.call_tir(cls.fused_tir_sqrt_divide1_multiply1, (lv507, decoder_model_1_block_1_weight_v, decoder_model_1_block_1_weight_g), out_sinfo=R.Tensor((1536, 768, 16), dtype="float32"))
            lv329_1 = R.call_tir(cls.reshape2, (decoder_model_1_block_1_bias,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv509 = R.call_tir(cls.fused_conv1d_transpose4_add24, (reshape95, lv508, lv329_1), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape96 = R.call_tir(cls.reshape27, (lv509,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv330_1 = R.call_tir(cls.snake8, (reshape96, decoder_model_1_block_2_block_0_alpha), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape97 = R.call_tir(cls.reshape27, (lv330_1,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv510 = R.call_tir(cls.fused_tir_square2_sum2, (decoder_model_1_block_2_block_1_weight_v,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv511 = R.call_tir(cls.fused_tir_sqrt1_divide2_multiply2, (lv510, decoder_model_1_block_2_block_1_weight_v, decoder_model_1_block_2_block_1_weight_g), out_sinfo=R.Tensor((768, 768, 7), dtype="float32"))
            lv336_1 = R.call_tir(cls.reshape2, (decoder_model_1_block_2_block_1_bias,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv512 = R.call_tir(cls.fused_conv1d43_add24, (reshape97, lv511, lv336_1), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape98 = R.call_tir(cls.reshape27, (lv512,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv337_1 = R.call_tir(cls.snake9, (reshape98, decoder_model_1_block_2_block_2_alpha), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape99 = R.call_tir(cls.reshape27, (lv337_1,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv513 = R.call_tir(cls.fused_tir_square3_sum3, (decoder_model_1_block_2_block_3_weight_v,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv514 = R.call_tir(cls.fused_tir_sqrt1_divide3_multiply3, (lv513, decoder_model_1_block_2_block_3_weight_v, decoder_model_1_block_2_block_3_weight_g), out_sinfo=R.Tensor((768, 768, 1), dtype="float32"))
            lv343_1 = R.call_tir(cls.reshape2, (decoder_model_1_block_2_block_3_bias,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv515 = R.call_tir(cls.fused_conv1d44_add24_add25, (reshape99, lv514, lv343_1, lv509), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape100 = R.call_tir(cls.reshape27, (lv515,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv344_1 = R.call_tir(cls.snake8, (reshape100, decoder_model_1_block_3_block_0_alpha), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape101 = R.call_tir(cls.reshape27, (lv344_1,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv516 = R.call_tir(cls.fused_tir_square2_sum2, (decoder_model_1_block_3_block_1_weight_v,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv517 = R.call_tir(cls.fused_tir_sqrt1_divide2_multiply2, (lv516, decoder_model_1_block_3_block_1_weight_v, decoder_model_1_block_3_block_1_weight_g), out_sinfo=R.Tensor((768, 768, 7), dtype="float32"))
            lv350_1 = R.call_tir(cls.reshape2, (decoder_model_1_block_3_block_1_bias,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv518 = R.call_tir(cls.fused_conv1d45_add24, (reshape101, lv517, lv350_1), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape102 = R.call_tir(cls.reshape27, (lv518,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv351_1 = R.call_tir(cls.snake8, (reshape102, decoder_model_1_block_3_block_2_alpha), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape103 = R.call_tir(cls.reshape27, (lv351_1,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv519 = R.call_tir(cls.fused_tir_square3_sum3, (decoder_model_1_block_3_block_3_weight_v,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv520 = R.call_tir(cls.fused_tir_sqrt1_divide3_multiply3, (lv519, decoder_model_1_block_3_block_3_weight_v, decoder_model_1_block_3_block_3_weight_g), out_sinfo=R.Tensor((768, 768, 1), dtype="float32"))
            lv357_1 = R.call_tir(cls.reshape2, (decoder_model_1_block_3_block_3_bias,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv521 = R.call_tir(cls.fused_conv1d44_add24_add25, (reshape103, lv520, lv357_1, lv515), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape104 = R.call_tir(cls.reshape27, (lv521,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv358_1 = R.call_tir(cls.snake8, (reshape104, decoder_model_1_block_4_block_0_alpha), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape105 = R.call_tir(cls.reshape27, (lv358_1,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv522 = R.call_tir(cls.fused_tir_square2_sum2, (decoder_model_1_block_4_block_1_weight_v,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv523 = R.call_tir(cls.fused_tir_sqrt1_divide2_multiply2, (lv522, decoder_model_1_block_4_block_1_weight_v, decoder_model_1_block_4_block_1_weight_g), out_sinfo=R.Tensor((768, 768, 7), dtype="float32"))
            lv364_1 = R.call_tir(cls.reshape2, (decoder_model_1_block_4_block_1_bias,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv524 = R.call_tir(cls.fused_conv1d46_add24, (reshape105, lv523, lv364_1), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape106 = R.call_tir(cls.reshape27, (lv524,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv365_1 = R.call_tir(cls.snake9, (reshape106, decoder_model_1_block_4_block_2_alpha), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape107 = R.call_tir(cls.reshape27, (lv365_1,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv525 = R.call_tir(cls.fused_tir_square3_sum3, (decoder_model_1_block_4_block_3_weight_v,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv526 = R.call_tir(cls.fused_tir_sqrt1_divide3_multiply3, (lv525, decoder_model_1_block_4_block_3_weight_v, decoder_model_1_block_4_block_3_weight_g), out_sinfo=R.Tensor((768, 768, 1), dtype="float32"))
            lv371_1 = R.call_tir(cls.reshape2, (decoder_model_1_block_4_block_3_bias,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv527 = R.call_tir(cls.fused_conv1d44_add24_add25, (reshape107, lv526, lv371_1, lv521), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape108 = R.call_tir(cls.reshape27, (lv527,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv372_1 = R.call_tir(cls.snake9, (reshape108, decoder_model_2_block_0_alpha), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape109 = R.call_tir(cls.reshape27, (lv372_1,), out_sinfo=R.Tensor((batch_size, 768, seq_len // 512 * 8), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv528 = R.call_tir(cls.fused_tir_square4_sum4, (decoder_model_2_block_1_weight_v,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv529 = R.call_tir(cls.fused_tir_sqrt1_divide4_multiply4, (lv528, decoder_model_2_block_1_weight_v, decoder_model_2_block_1_weight_g), out_sinfo=R.Tensor((768, 384, 16), dtype="float32"))
            lv378_1 = R.call_tir(cls.reshape4, (decoder_model_2_block_1_bias,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv530 = R.call_tir(cls.fused_conv1d_transpose5_add26, (reshape109, lv529, lv378_1), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape110 = R.call_tir(cls.reshape28, (lv530,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv379_1 = R.call_tir(cls.snake10, (reshape110, decoder_model_2_block_2_block_0_alpha), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape111 = R.call_tir(cls.reshape28, (lv379_1,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv531 = R.call_tir(cls.fused_tir_square5_sum5, (decoder_model_2_block_2_block_1_weight_v,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv532 = R.call_tir(cls.fused_tir_sqrt2_divide5_multiply5, (lv531, decoder_model_2_block_2_block_1_weight_v, decoder_model_2_block_2_block_1_weight_g), out_sinfo=R.Tensor((384, 384, 7), dtype="float32"))
            lv385_1 = R.call_tir(cls.reshape4, (decoder_model_2_block_2_block_1_bias,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv533 = R.call_tir(cls.fused_conv1d47_add26, (reshape111, lv532, lv385_1), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape112 = R.call_tir(cls.reshape28, (lv533,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv386_1 = R.call_tir(cls.snake10, (reshape112, decoder_model_2_block_2_block_2_alpha), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape113 = R.call_tir(cls.reshape28, (lv386_1,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv534 = R.call_tir(cls.fused_tir_square6_sum6, (decoder_model_2_block_2_block_3_weight_v,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv535 = R.call_tir(cls.fused_tir_sqrt2_divide6_multiply6, (lv534, decoder_model_2_block_2_block_3_weight_v, decoder_model_2_block_2_block_3_weight_g), out_sinfo=R.Tensor((384, 384, 1), dtype="float32"))
            lv392_1 = R.call_tir(cls.reshape4, (decoder_model_2_block_2_block_3_bias,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv536 = R.call_tir(cls.fused_conv1d48_add26_add27, (reshape113, lv535, lv392_1, lv530), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape114 = R.call_tir(cls.reshape28, (lv536,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv393_1 = R.call_tir(cls.snake10, (reshape114, decoder_model_2_block_3_block_0_alpha), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape115 = R.call_tir(cls.reshape28, (lv393_1,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv537 = R.call_tir(cls.fused_tir_square5_sum5, (decoder_model_2_block_3_block_1_weight_v,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv538 = R.call_tir(cls.fused_tir_sqrt2_divide5_multiply5, (lv537, decoder_model_2_block_3_block_1_weight_v, decoder_model_2_block_3_block_1_weight_g), out_sinfo=R.Tensor((384, 384, 7), dtype="float32"))
            lv399_1 = R.call_tir(cls.reshape4, (decoder_model_2_block_3_block_1_bias,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv539 = R.call_tir(cls.fused_conv1d49_add26, (reshape115, lv538, lv399_1), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape116 = R.call_tir(cls.reshape28, (lv539,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv400_1 = R.call_tir(cls.snake11, (reshape116, decoder_model_2_block_3_block_2_alpha), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape117 = R.call_tir(cls.reshape28, (lv400_1,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv540 = R.call_tir(cls.fused_tir_square6_sum6, (decoder_model_2_block_3_block_3_weight_v,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv541 = R.call_tir(cls.fused_tir_sqrt2_divide6_multiply6, (lv540, decoder_model_2_block_3_block_3_weight_v, decoder_model_2_block_3_block_3_weight_g), out_sinfo=R.Tensor((384, 384, 1), dtype="float32"))
            lv406_1 = R.call_tir(cls.reshape4, (decoder_model_2_block_3_block_3_bias,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv542 = R.call_tir(cls.fused_conv1d48_add26_add27, (reshape117, lv541, lv406_1, lv536), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape118 = R.call_tir(cls.reshape28, (lv542,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv407_1 = R.call_tir(cls.snake10, (reshape118, decoder_model_2_block_4_block_0_alpha), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape119 = R.call_tir(cls.reshape28, (lv407_1,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv543 = R.call_tir(cls.fused_tir_square5_sum5, (decoder_model_2_block_4_block_1_weight_v,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv544 = R.call_tir(cls.fused_tir_sqrt2_divide5_multiply5, (lv543, decoder_model_2_block_4_block_1_weight_v, decoder_model_2_block_4_block_1_weight_g), out_sinfo=R.Tensor((384, 384, 7), dtype="float32"))
            lv413_1 = R.call_tir(cls.reshape4, (decoder_model_2_block_4_block_1_bias,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv545 = R.call_tir(cls.fused_conv1d50_add26, (reshape119, lv544, lv413_1), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape120 = R.call_tir(cls.reshape28, (lv545,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv414_1 = R.call_tir(cls.snake10, (reshape120, decoder_model_2_block_4_block_2_alpha), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape121 = R.call_tir(cls.reshape28, (lv414_1,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv546 = R.call_tir(cls.fused_tir_square6_sum6, (decoder_model_2_block_4_block_3_weight_v,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv547 = R.call_tir(cls.fused_tir_sqrt2_divide6_multiply6, (lv546, decoder_model_2_block_4_block_3_weight_v, decoder_model_2_block_4_block_3_weight_g), out_sinfo=R.Tensor((384, 384, 1), dtype="float32"))
            lv420_1 = R.call_tir(cls.reshape4, (decoder_model_2_block_4_block_3_bias,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv548 = R.call_tir(cls.fused_conv1d48_add26_add27, (reshape121, lv547, lv420_1, lv542), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape122 = R.call_tir(cls.reshape28, (lv548,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv421_1 = R.call_tir(cls.snake10, (reshape122, decoder_model_3_block_0_alpha), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape123 = R.call_tir(cls.reshape28, (lv421_1,), out_sinfo=R.Tensor((batch_size, 384, seq_len // 512 * 64), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv549 = R.call_tir(cls.fused_tir_square7_sum7, (decoder_model_3_block_1_weight_v,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv550 = R.call_tir(cls.fused_tir_sqrt2_divide7_multiply7, (lv549, decoder_model_3_block_1_weight_v, decoder_model_3_block_1_weight_g), out_sinfo=R.Tensor((384, 192, 8), dtype="float32"))
            lv427_1 = R.call_tir(cls.reshape6, (decoder_model_3_block_1_bias,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv551 = R.call_tir(cls.fused_conv1d_transpose6_add28, (reshape123, lv550, lv427_1), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape124 = R.call_tir(cls.reshape29, (lv551,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv428_1 = R.call_tir(cls.snake12, (reshape124, decoder_model_3_block_2_block_0_alpha), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape125 = R.call_tir(cls.reshape29, (lv428_1,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv552 = R.call_tir(cls.fused_tir_square8_sum8, (decoder_model_3_block_2_block_1_weight_v,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv553 = R.call_tir(cls.fused_tir_sqrt3_divide8_multiply8, (lv552, decoder_model_3_block_2_block_1_weight_v, decoder_model_3_block_2_block_1_weight_g), out_sinfo=R.Tensor((192, 192, 7), dtype="float32"))
            lv434_1 = R.call_tir(cls.reshape6, (decoder_model_3_block_2_block_1_bias,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv554 = R.call_tir(cls.fused_conv1d51_add28, (reshape125, lv553, lv434_1), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape126 = R.call_tir(cls.reshape29, (lv554,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv435_1 = R.call_tir(cls.snake13, (reshape126, decoder_model_3_block_2_block_2_alpha), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape127 = R.call_tir(cls.reshape29, (lv435_1,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv555 = R.call_tir(cls.fused_tir_square9_sum9, (decoder_model_3_block_2_block_3_weight_v,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv556 = R.call_tir(cls.fused_tir_sqrt3_divide9_multiply9, (lv555, decoder_model_3_block_2_block_3_weight_v, decoder_model_3_block_2_block_3_weight_g), out_sinfo=R.Tensor((192, 192, 1), dtype="float32"))
            lv441_1 = R.call_tir(cls.reshape6, (decoder_model_3_block_2_block_3_bias,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv557 = R.call_tir(cls.fused_conv1d52_add28_add29, (reshape127, lv556, lv441_1, lv551), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape128 = R.call_tir(cls.reshape29, (lv557,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv442_1 = R.call_tir(cls.snake13, (reshape128, decoder_model_3_block_3_block_0_alpha), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape129 = R.call_tir(cls.reshape29, (lv442_1,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv558 = R.call_tir(cls.fused_tir_square8_sum8, (decoder_model_3_block_3_block_1_weight_v,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv559 = R.call_tir(cls.fused_tir_sqrt3_divide8_multiply8, (lv558, decoder_model_3_block_3_block_1_weight_v, decoder_model_3_block_3_block_1_weight_g), out_sinfo=R.Tensor((192, 192, 7), dtype="float32"))
            lv448_1 = R.call_tir(cls.reshape6, (decoder_model_3_block_3_block_1_bias,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv560 = R.call_tir(cls.fused_conv1d53_add28, (reshape129, lv559, lv448_1), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape130 = R.call_tir(cls.reshape29, (lv560,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv449_1 = R.call_tir(cls.snake12, (reshape130, decoder_model_3_block_3_block_2_alpha), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape131 = R.call_tir(cls.reshape29, (lv449_1,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv561 = R.call_tir(cls.fused_tir_square9_sum9, (decoder_model_3_block_3_block_3_weight_v,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv562 = R.call_tir(cls.fused_tir_sqrt3_divide9_multiply9, (lv561, decoder_model_3_block_3_block_3_weight_v, decoder_model_3_block_3_block_3_weight_g), out_sinfo=R.Tensor((192, 192, 1), dtype="float32"))
            lv455_1 = R.call_tir(cls.reshape6, (decoder_model_3_block_3_block_3_bias,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv563 = R.call_tir(cls.fused_conv1d52_add28_add29, (reshape131, lv562, lv455_1, lv557), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape132 = R.call_tir(cls.reshape29, (lv563,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv456_1 = R.call_tir(cls.snake13, (reshape132, decoder_model_3_block_4_block_0_alpha), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape133 = R.call_tir(cls.reshape29, (lv456_1,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv564 = R.call_tir(cls.fused_tir_square8_sum8, (decoder_model_3_block_4_block_1_weight_v,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv565 = R.call_tir(cls.fused_tir_sqrt3_divide8_multiply8, (lv564, decoder_model_3_block_4_block_1_weight_v, decoder_model_3_block_4_block_1_weight_g), out_sinfo=R.Tensor((192, 192, 7), dtype="float32"))
            lv462_1 = R.call_tir(cls.reshape6, (decoder_model_3_block_4_block_1_bias,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv566 = R.call_tir(cls.fused_conv1d54_add28, (reshape133, lv565, lv462_1), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape134 = R.call_tir(cls.reshape29, (lv566,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv463_1 = R.call_tir(cls.snake13, (reshape134, decoder_model_3_block_4_block_2_alpha), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape135 = R.call_tir(cls.reshape29, (lv463_1,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv567 = R.call_tir(cls.fused_tir_square9_sum9, (decoder_model_3_block_4_block_3_weight_v,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv568 = R.call_tir(cls.fused_tir_sqrt3_divide9_multiply9, (lv567, decoder_model_3_block_4_block_3_weight_v, decoder_model_3_block_4_block_3_weight_g), out_sinfo=R.Tensor((192, 192, 1), dtype="float32"))
            lv469_1 = R.call_tir(cls.reshape6, (decoder_model_3_block_4_block_3_bias,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv569 = R.call_tir(cls.fused_conv1d52_add28_add29, (reshape135, lv568, lv469_1, lv563), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape136 = R.call_tir(cls.reshape29, (lv569,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv470_1 = R.call_tir(cls.snake12, (reshape136, decoder_model_4_block_0_alpha), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape137 = R.call_tir(cls.reshape29, (lv470_1,), out_sinfo=R.Tensor((batch_size, 192, seq_len // 512 * 256), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv570 = R.call_tir(cls.fused_tir_square10_sum10, (decoder_model_4_block_1_weight_v,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv571 = R.call_tir(cls.fused_tir_sqrt3_divide10_multiply10, (lv570, decoder_model_4_block_1_weight_v, decoder_model_4_block_1_weight_g), out_sinfo=R.Tensor((192, 96, 4), dtype="float32"))
            lv476_1 = R.call_tir(cls.reshape8, (decoder_model_4_block_1_bias,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv572 = R.call_tir(cls.fused_conv1d_transpose7_add30, (reshape137, lv571, lv476_1), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape138 = R.call_tir(cls.reshape30, (lv572,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv477_1 = R.call_tir(cls.snake14, (reshape138, decoder_model_4_block_2_block_0_alpha), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape139 = R.call_tir(cls.reshape30, (lv477_1,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv573 = R.call_tir(cls.fused_tir_square11_sum11, (decoder_model_4_block_2_block_1_weight_v,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv574 = R.call_tir(cls.fused_tir_sqrt4_divide11_multiply11, (lv573, decoder_model_4_block_2_block_1_weight_v, decoder_model_4_block_2_block_1_weight_g), out_sinfo=R.Tensor((96, 96, 7), dtype="float32"))
            lv483_1 = R.call_tir(cls.reshape8, (decoder_model_4_block_2_block_1_bias,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv575 = R.call_tir(cls.fused_conv1d55_add30, (reshape139, lv574, lv483_1), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape140 = R.call_tir(cls.reshape30, (lv575,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv484_1 = R.call_tir(cls.snake14, (reshape140, decoder_model_4_block_2_block_2_alpha), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape141 = R.call_tir(cls.reshape30, (lv484_1,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv576 = R.call_tir(cls.fused_tir_square12_sum12, (decoder_model_4_block_2_block_3_weight_v,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv577 = R.call_tir(cls.fused_tir_sqrt4_divide12_multiply12, (lv576, decoder_model_4_block_2_block_3_weight_v, decoder_model_4_block_2_block_3_weight_g), out_sinfo=R.Tensor((96, 96, 1), dtype="float32"))
            lv490_1 = R.call_tir(cls.reshape8, (decoder_model_4_block_2_block_3_bias,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv578 = R.call_tir(cls.fused_conv1d56_add30_add31, (reshape141, lv577, lv490_1, lv572), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape142 = R.call_tir(cls.reshape30, (lv578,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv491_1 = R.call_tir(cls.snake15, (reshape142, decoder_model_4_block_3_block_0_alpha), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape143 = R.call_tir(cls.reshape30, (lv491_1,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv579 = R.call_tir(cls.fused_tir_square11_sum11, (decoder_model_4_block_3_block_1_weight_v,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv580 = R.call_tir(cls.fused_tir_sqrt4_divide11_multiply11, (lv579, decoder_model_4_block_3_block_1_weight_v, decoder_model_4_block_3_block_1_weight_g), out_sinfo=R.Tensor((96, 96, 7), dtype="float32"))
            lv497_1 = R.call_tir(cls.reshape8, (decoder_model_4_block_3_block_1_bias,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv581 = R.call_tir(cls.fused_conv1d57_add30, (reshape143, lv580, lv497_1), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape144 = R.call_tir(cls.reshape30, (lv581,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv498_1 = R.call_tir(cls.snake14, (reshape144, decoder_model_4_block_3_block_2_alpha), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape145 = R.call_tir(cls.reshape30, (lv498_1,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv582 = R.call_tir(cls.fused_tir_square12_sum12, (decoder_model_4_block_3_block_3_weight_v,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv583 = R.call_tir(cls.fused_tir_sqrt4_divide12_multiply12, (lv582, decoder_model_4_block_3_block_3_weight_v, decoder_model_4_block_3_block_3_weight_g), out_sinfo=R.Tensor((96, 96, 1), dtype="float32"))
            lv504_1 = R.call_tir(cls.reshape8, (decoder_model_4_block_3_block_3_bias,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv584 = R.call_tir(cls.fused_conv1d56_add30_add31, (reshape145, lv583, lv504_1, lv578), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape146 = R.call_tir(cls.reshape30, (lv584,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv505_1 = R.call_tir(cls.snake15, (reshape146, decoder_model_4_block_4_block_0_alpha), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape147 = R.call_tir(cls.reshape30, (lv505_1,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv585 = R.call_tir(cls.fused_tir_square11_sum11, (decoder_model_4_block_4_block_1_weight_v,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv586 = R.call_tir(cls.fused_tir_sqrt4_divide11_multiply11, (lv585, decoder_model_4_block_4_block_1_weight_v, decoder_model_4_block_4_block_1_weight_g), out_sinfo=R.Tensor((96, 96, 7), dtype="float32"))
            lv511_1 = R.call_tir(cls.reshape8, (decoder_model_4_block_4_block_1_bias,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv587 = R.call_tir(cls.fused_conv1d58_add30, (reshape147, lv586, lv511_1), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape148 = R.call_tir(cls.reshape30, (lv587,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv512_1 = R.call_tir(cls.snake14, (reshape148, decoder_model_4_block_4_block_2_alpha), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape149 = R.call_tir(cls.reshape30, (lv512_1,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv588 = R.call_tir(cls.fused_tir_square12_sum12, (decoder_model_4_block_4_block_3_weight_v,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv589 = R.call_tir(cls.fused_tir_sqrt4_divide12_multiply12, (lv588, decoder_model_4_block_4_block_3_weight_v, decoder_model_4_block_4_block_3_weight_g), out_sinfo=R.Tensor((96, 96, 1), dtype="float32"))
            lv518_1 = R.call_tir(cls.reshape8, (decoder_model_4_block_4_block_3_bias,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv590 = R.call_tir(cls.fused_conv1d56_add30_add31, (reshape149, lv589, lv518_1, lv584), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            reshape150 = R.call_tir(cls.reshape30, (lv590,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv519_1 = R.call_tir(cls.snake14, (reshape150, decoder_model_5_alpha), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len, seq_len]))
            reshape151 = R.call_tir(cls.reshape30, (lv519_1,), out_sinfo=R.Tensor((batch_size, 96, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            lv591 = R.call_tir(cls.fused_tir_square13_sum13, (decoder_model_6_weight_v,), out_sinfo=R.Tensor((1, 1, 1), dtype="float32"))
            lv592 = R.call_tir(cls.fused_tir_sqrt5_divide13_multiply13, (lv591, decoder_model_6_weight_v, decoder_model_6_weight_g), out_sinfo=R.Tensor((1, 96, 7), dtype="float32"))
            lv593 = R.call_tir(cls.fused_conv1d59_reshape10_add32_tir_tanh1, (reshape151, lv592, decoder_model_6_bias), out_sinfo=R.Tensor((batch_size, 1, seq_len // 512 * 512), dtype="float32"), tir_vars=R.shape([seq_len]))
            gv: R.Tuple(R.Tensor((batch_size, 1, seq_len // 512 * 512), dtype="float32"), R.Tensor((batch_size, 1024, seq_len // 512), dtype="float32"), R.Tuple(R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"), R.Tensor((batch_size, seq_len // 512), dtype="int32"))) = lv593, lv503, (reshape59, reshape63, reshape67, reshape71, reshape75, reshape79, reshape83, reshape87, reshape91)
            R.output(gv)
        return gv

# Metadata omitted. Use show_meta=True in script() method to show it.