# from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func
    def cached_padding_1d_crop(var_x: T.handle, var_res: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1})
        b, c, out = T.int32(), T.int32(), T.int32()
        x = T.match_buffer(var_x, (b, c, out))
        n = T.int32()
        res = T.match_buffer(var_res, (b, c, n))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((b * c * n + 1023) // 1024, thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("res_crop"):
                    v0 = T.axis.spatial(b, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % (n * c * b) // (n * c))
                    v1 = T.axis.spatial(c, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % (n * c) // n)
                    v2 = T.axis.spatial(n, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % n)
                    T.where(ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1 < b * c * n)
                    T.reads(x[v0, v1, v2])
                    T.writes(res[v0, v1, v2])
                    res[v0, v1, v2] = x[v0, v1, v2]

    @T.prim_func
    def cached_padding_1d_init(var_cache: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1})
        b, c, p = T.int32(), T.int32(), T.int32()
        cache = T.match_buffer(var_cache, (b, c, p))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((b * c * p + 1023) // 1024, thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("cache_init"):
                    v0 = T.axis.spatial(b, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % (p * c * b) // (p * c))
                    v1 = T.axis.spatial(c, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % (p * c) // p)
                    v2 = T.axis.spatial(p, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % p)
                    T.where(ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1 < b * c * p)
                    T.reads()
                    T.writes(cache[v0, v1, v2])
                    cache[v0, v1, v2] = T.float32(0.0)

    @T.prim_func
    def cached_padding_1d_update(var_cache: T.handle, var_data: T.handle, var_res: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1})
        B, c, p = T.int32(), T.int32(), T.int32()
        cache = T.match_buffer(var_cache, (B, c, p))
        b, n = T.int32(), T.int32()
        data = T.match_buffer(var_data, (b, c, n))
        out = T.int32()
        res = T.match_buffer(var_res, (b, c, out))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((b * c * out + 1023) // 1024, thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("res_update"):
                    v0 = T.axis.spatial(b, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % (out * c * b) // (out * c))
                    v1 = T.axis.spatial(c, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % (out * c) // out)
                    v2 = T.axis.spatial(out, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % out)
                    T.where(ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1 < b * c * out)
                    T.reads(cache[v0, v1, v2], data[v0, v1, v2 + (0 - p)])
                    T.writes(res[v0, v1, v2])
                    res[v0, v1, v2] = T.if_then_else(v2 < p, cache[v0, v1, v2], data[v0, v1, v2 + (0 - p)])
        for ax0_ax1_ax2_fused_0 in T.thread_binding((b * c * p + 1023) // 1024, thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("cache_update"):
                    v0 = T.axis.spatial(b, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % (p * c * b) // (p * c))
                    v1 = T.axis.spatial(c, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % (p * c) // p)
                    v2 = T.axis.spatial(p, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % p)
                    T.where(ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1 < b * c * p)
                    T.reads(res[v0, v1, v2 + (out - p)])
                    T.writes(cache[v0, v1, v2])
                    cache[v0, v1, v2] = res[v0, v1, v2 + (out - p)]

    @T.prim_func
    def cached_padding_transpose_1d_update(var_cache: T.handle, var_data: T.handle, var_res: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1})
        B, c, p = T.int32(), T.int32(), T.int32()
        cache = T.match_buffer(var_cache, (B, c, p))
        b, n = T.int32(), T.int32()
        data = T.match_buffer(var_data, (b, c, n))
        out = T.int32()
        res = T.match_buffer(var_res, (b, c, out))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding((b * c * out + 1023) // 1024, thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("res_update"):
                    v0 = T.axis.spatial(b, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % (out * c * b) // (out * c))
                    v1 = T.axis.spatial(c, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % (out * c) // out)
                    v2 = T.axis.spatial(out, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % out)
                    T.where(ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1 < b * c * out)
                    T.reads(cache[v0, v1, v2], data[v0, v1, v2])
                    T.writes(res[v0, v1, v2])
                    res[v0, v1, v2] = T.if_then_else(v2 < p, cache[v0, v1, v2] + data[v0, v1, v2], data[v0, v1, v2])
        for ax0_ax1_ax2_fused_0 in T.thread_binding((b * c * p + 1023) // 1024, thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("cache_update"):
                    v0 = T.axis.spatial(b, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % (p * c * b) // (p * c))
                    v1 = T.axis.spatial(c, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % (p * c) // p)
                    v2 = T.axis.spatial(p, (ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1) % p)
                    T.where(ax0_ax1_ax2_fused_0 * 1024 + ax0_ax1_ax2_fused_1 < b * c * p)
                    T.reads(res[v0, v1, v2 + (out - p)])
                    T.writes(cache[v0, v1, v2])
                    cache[v0, v1, v2] = res[v0, v1, v2 + (out - p)]

    @T.prim_func(private=True)
    def conv1d_transpose(var_reshape95: T.handle, wnconvtranspose1d: T.Buffer((T.int64(1536), T.int64(768), T.int64(16)), "float32"), var_compute: T.handle):
        T.func_attr({"op_pattern": 4, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape95 = T.match_buffer(var_reshape95, (batch_size, T.int64(1536), T.int64(1)))
        compute = T.match_buffer(var_compute, (batch_size, T.int64(768), T.int64(16)))
        # with T.block("root"):
        compute_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size * T.int64(16) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(768)), scope="local")
        data_pad_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size * T.int64(16) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(24576)), scope="shared")
        kernel_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(768), T.int64(24576)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(24), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size * T.int64(16) + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("compute_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size * T.int64(16) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(compute_reindex_pad_local[T.int64(0), v1, v2])
                                            compute_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(3072)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("data_pad_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size * T.int64(16) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(24576), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(reshape95[v1 // T.int64(16), v2 // T.int64(16), (v1 % T.int64(16) + v2 % T.int64(16) - T.int64(15)) // T.int64(8)])
                                                        T.writes(data_pad_reindex_pad_shared[v0, v1, v2])
                                                        data_pad_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size * T.int64(16), T.if_then_else(T.int64(15) <= v1 % T.int64(16) + v2 % T.int64(16) and v1 % T.int64(16) + v2 % T.int64(16) < T.int64(16), T.if_then_else((v1 % T.int64(16) + v2 % T.int64(16) - T.int64(15)) % T.int64(8) == T.int64(0), reshape95[v1 // T.int64(16), v2 // T.int64(16), (v1 % T.int64(16) + v2 % T.int64(16) - T.int64(15)) // T.int64(8)], T.float32(0.0)), T.float32(0.0)), T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("kernel_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(24576), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconvtranspose1d[v2 // T.int64(16), v1, T.int64(15) - v2 % T.int64(16)])
                                                        T.writes(kernel_reindex_shared[v0, v1, v2])
                                                        kernel_reindex_shared[v0, v1, v2] = wnconvtranspose1d[v2 // T.int64(16), v1, T.int64(15) - v2 % T.int64(16)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("compute_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size * T.int64(16) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(24576), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(compute_reindex_pad_local[T.int64(0), v1, v2], data_pad_reindex_pad_shared[T.int64(0), v1, v3], kernel_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(compute_reindex_pad_local[T.int64(0), v1, v2])
                                                compute_reindex_pad_local[T.int64(0), v1, v2] = compute_reindex_pad_local[T.int64(0), v1, v2] + data_pad_reindex_pad_shared[T.int64(0), v1, v3] * kernel_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("compute_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size * T.int64(16) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(2) + (ax1_2 * T.int64(4) + ax1) // T.int64(16) < batch_size)
                                            T.reads(compute_reindex_pad_local[v0, v1, v2])
                                            T.writes(compute[v1 // T.int64(16), v2, v1 % T.int64(16)])
                                            compute[v1 // T.int64(16), v2, v1 % T.int64(16)] = compute_reindex_pad_local[v0, v1, v2]

    @T.prim_func(private=True)
    def conv1d_transpose1(var_reshape109: T.handle, wnconvtranspose1d1: T.Buffer((T.int64(768), T.int64(384), T.int64(16)), "float32"), var_compute: T.handle):
        T.func_attr({"op_pattern": 4, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape109 = T.match_buffer(var_reshape109, (batch_size, T.int64(768), T.int64(8)))
        compute = T.match_buffer(var_compute, (batch_size, T.int64(384), T.int64(72)))
        # with T.block("root"):
        compute_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size * T.int64(72) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(384)), scope="local")
        data_pad_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size * T.int64(72) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(12288)), scope="shared")
        kernel_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(12288)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(12), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size * T.int64(72) + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("compute_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size * T.int64(72) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(compute_reindex_pad_local[T.int64(0), v1, v2])
                                            compute_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(1536)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("data_pad_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size * T.int64(72) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(12288), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(reshape109[v1 // T.int64(72), v2 // T.int64(16), (v1 % T.int64(72) + v2 % T.int64(16) - T.int64(15)) // T.int64(8)])
                                                        T.writes(data_pad_reindex_pad_shared[v0, v1, v2])
                                                        data_pad_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size * T.int64(72), T.if_then_else(T.int64(15) <= v1 % T.int64(72) + v2 % T.int64(16) and v1 % T.int64(72) + v2 % T.int64(16) < T.int64(72), T.if_then_else((v1 % T.int64(72) + v2 % T.int64(16) - T.int64(15)) % T.int64(8) == T.int64(0), reshape109[v1 // T.int64(72), v2 // T.int64(16), (v1 % T.int64(72) + v2 % T.int64(16) - T.int64(15)) // T.int64(8)], T.float32(0.0)), T.float32(0.0)), T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("kernel_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(12288), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconvtranspose1d1[v2 // T.int64(16), v1, T.int64(15) - v2 % T.int64(16)])
                                                        T.writes(kernel_reindex_shared[v0, v1, v2])
                                                        kernel_reindex_shared[v0, v1, v2] = wnconvtranspose1d1[v2 // T.int64(16), v1, T.int64(15) - v2 % T.int64(16)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("compute_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size * T.int64(72) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(12288), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(compute_reindex_pad_local[T.int64(0), v1, v2], data_pad_reindex_pad_shared[T.int64(0), v1, v3], kernel_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(compute_reindex_pad_local[T.int64(0), v1, v2])
                                                compute_reindex_pad_local[T.int64(0), v1, v2] = compute_reindex_pad_local[T.int64(0), v1, v2] + data_pad_reindex_pad_shared[T.int64(0), v1, v3] * kernel_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("compute_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size * T.int64(72) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(72) < batch_size)
                                            T.reads(compute_reindex_pad_local[v0, v1, v2])
                                            T.writes(compute[v1 // T.int64(72), v2, v1 % T.int64(72)])
                                            compute[v1 // T.int64(72), v2, v1 % T.int64(72)] = compute_reindex_pad_local[v0, v1, v2]

    @T.prim_func(private=True)
    def conv1d_transpose2(var_reshape123: T.handle, wnconvtranspose1d2: T.Buffer((T.int64(384), T.int64(192), T.int64(8)), "float32"), var_compute: T.handle):
        T.func_attr({"op_pattern": 4, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape123 = T.match_buffer(var_reshape123, (batch_size, T.int64(384), T.int64(64)))
        compute = T.match_buffer(var_compute, (batch_size, T.int64(192), T.int64(260)))
        # with T.block("root"):
        compute_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size * T.int64(260) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(192)), scope="local")
        data_pad_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size * T.int64(260) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(3072)), scope="shared")
        kernel_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(192), T.int64(3072)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(6), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size * T.int64(260) + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("compute_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size * T.int64(260) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(compute_reindex_pad_local[T.int64(0), v1, v2])
                                            compute_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(384)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("data_pad_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size * T.int64(260) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(3072), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(reshape123[v1 // T.int64(260), v2 // T.int64(8), (v1 % T.int64(260) + v2 % T.int64(8) - T.int64(7)) // T.int64(4)])
                                                        T.writes(data_pad_reindex_pad_shared[v0, v1, v2])
                                                        data_pad_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size * T.int64(260), T.if_then_else(T.int64(7) <= v1 % T.int64(260) + v2 % T.int64(8) and v1 % T.int64(260) + v2 % T.int64(8) < T.int64(260), T.if_then_else((v1 % T.int64(260) + v2 % T.int64(8) - T.int64(7)) % T.int64(4) == T.int64(0), reshape123[v1 // T.int64(260), v2 // T.int64(8), (v1 % T.int64(260) + v2 % T.int64(8) - T.int64(7)) // T.int64(4)], T.float32(0.0)), T.float32(0.0)), T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("kernel_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(3072), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconvtranspose1d2[v2 // T.int64(8), v1, T.int64(7) - v2 % T.int64(8)])
                                                        T.writes(kernel_reindex_shared[v0, v1, v2])
                                                        kernel_reindex_shared[v0, v1, v2] = wnconvtranspose1d2[v2 // T.int64(8), v1, T.int64(7) - v2 % T.int64(8)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("compute_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size * T.int64(260) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(3072), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(compute_reindex_pad_local[T.int64(0), v1, v2], data_pad_reindex_pad_shared[T.int64(0), v1, v3], kernel_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(compute_reindex_pad_local[T.int64(0), v1, v2])
                                                compute_reindex_pad_local[T.int64(0), v1, v2] = compute_reindex_pad_local[T.int64(0), v1, v2] + data_pad_reindex_pad_shared[T.int64(0), v1, v3] * kernel_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("compute_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size * T.int64(260) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(260) < batch_size)
                                            T.reads(compute_reindex_pad_local[v0, v1, v2])
                                            T.writes(compute[v1 // T.int64(260), v2, v1 % T.int64(260)])
                                            compute[v1 // T.int64(260), v2, v1 % T.int64(260)] = compute_reindex_pad_local[v0, v1, v2]

    @T.prim_func(private=True)
    def conv1d_transpose3(var_reshape137: T.handle, wnconvtranspose1d3: T.Buffer((T.int64(192), T.int64(96), T.int64(4)), "float32"), var_compute: T.handle):
        T.func_attr({"op_pattern": 4, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape137 = T.match_buffer(var_reshape137, (batch_size, T.int64(192), T.int64(256)))
        compute = T.match_buffer(var_compute, (batch_size, T.int64(96), T.int64(514)))
        # with T.block("root"):
        compute_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size * T.int64(514) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(96)), scope="local")
        data_pad_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size * T.int64(514) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(768)), scope="shared")
        kernel_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(96), T.int64(768)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(3), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size * T.int64(514) + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("compute_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size * T.int64(514) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(compute_reindex_pad_local[T.int64(0), v1, v2])
                                            compute_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(96)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("data_pad_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size * T.int64(514) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(768), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(reshape137[v1 // T.int64(514), v2 // T.int64(4), (v1 % T.int64(514) + v2 % T.int64(4) - T.int64(3)) // T.int64(2)])
                                                        T.writes(data_pad_reindex_pad_shared[v0, v1, v2])
                                                        data_pad_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size * T.int64(514), T.if_then_else(T.int64(3) <= v1 % T.int64(514) + v2 % T.int64(4) and v1 % T.int64(514) + v2 % T.int64(4) < T.int64(514), T.if_then_else((v1 % T.int64(514) + v2 % T.int64(4) - T.int64(3)) % T.int64(2) == T.int64(0), reshape137[v1 // T.int64(514), v2 // T.int64(4), (v1 % T.int64(514) + v2 % T.int64(4) - T.int64(3)) // T.int64(2)], T.float32(0.0)), T.float32(0.0)), T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("kernel_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(768), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconvtranspose1d3[v2 // T.int64(4), v1, T.int64(3) - v2 % T.int64(4)])
                                                        T.writes(kernel_reindex_shared[v0, v1, v2])
                                                        kernel_reindex_shared[v0, v1, v2] = wnconvtranspose1d3[v2 // T.int64(4), v1, T.int64(3) - v2 % T.int64(4)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("compute_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size * T.int64(514) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(768), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(compute_reindex_pad_local[T.int64(0), v1, v2], data_pad_reindex_pad_shared[T.int64(0), v1, v3], kernel_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(compute_reindex_pad_local[T.int64(0), v1, v2])
                                                compute_reindex_pad_local[T.int64(0), v1, v2] = compute_reindex_pad_local[T.int64(0), v1, v2] + data_pad_reindex_pad_shared[T.int64(0), v1, v3] * kernel_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("compute_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size * T.int64(514) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(514) < batch_size)
                                            T.reads(compute_reindex_pad_local[v0, v1, v2])
                                            T.writes(compute[v1 // T.int64(514), v2, v1 % T.int64(514)])
                                            compute[v1 // T.int64(514), v2, v1 % T.int64(514)] = compute_reindex_pad_local[v0, v1, v2]

    @T.prim_func(private=True)
    def fused_broadcast_to1_maximum1_tir_sqrt13_divide30(sum1: T.Buffer((T.int64(1024), T.int64(1)), "float32"), quantizer_quantizers_0_codebook_weight: T.Buffer((T.int64(1024), T.int64(8)), "float32"), T_divide_intermediate: T.Buffer((T.int64(1024), T.int64(8)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(8), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(T.int64(1024), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.reads(quantizer_quantizers_0_codebook_weight[v0, v1], sum1[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = quantizer_quantizers_0_codebook_weight[v0, v1] / T.sqrt(T.max(sum1[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_broadcast_to_maximum_tir_sqrt12_divide29(p_sum: T.handle, p_reshape58: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        sum = T.match_buffer(p_sum, (batch_size, T.int64(1)))
        reshape58 = T.match_buffer(p_reshape58, (batch_size, T.int64(8)))
        T_divide_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_divide"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * T.int64(8))
                    T.reads(reshape58[v0, v1], sum[v0, T.int64(0)])
                    T.writes(T_divide_intermediate[v0, v1])
                    T_divide_intermediate[v0, v1] = reshape58[v0, v1] / T.sqrt(T.max(sum[v0, T.int64(0)], T.float32(9.999999960041972e-13)))

    @T.prim_func(private=True)
    def fused_conv1d10_add10_add11_add12(p_lv560: T.handle, wnconv1d62: T.Buffer((T.int64(192), T.int64(192), T.int64(1)), "float32"), lv566: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_lv567: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv560 = T.match_buffer(p_lv560, (batch_size, T.int64(192), T.int64(256)))
        lv567 = T.match_buffer(p_lv567, (batch_size, T.int64(192), T.int64(256)))
        T_add_intermediate_1_2 = T.match_buffer(p_output0, (batch_size, T.int64(192), T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(192)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(192)), scope="shared")
        wnconv1d62_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(192), T.int64(192)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(6), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(8), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(24)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(192), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv560[v1 // T.int64(256), v2, v1 % T.int64(256)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv560[v1 // T.int64(256), v2, v1 % T.int64(256)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d62_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(192), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d62[v1, v2, T.int64(0)])
                                                        T.writes(wnconv1d62_reindex_shared[v0, v1, v2])
                                                        wnconv1d62_reindex_shared[v0, v1, v2] = wnconv1d62[v1, v2, T.int64(0)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(192), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d62_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d62_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(256) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv566[T.int64(0), v2, T.int64(0)], lv567[v1 // T.int64(256), v2, v1 % T.int64(256)])
                                            T.writes(T_add_intermediate_1_2[v1 // T.int64(256), v2, v1 % T.int64(256)])
                                            T_add_intermediate_1_2[v1 // T.int64(256), v2, v1 % T.int64(256)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv566[T.int64(0), v2, T.int64(0)] + lv567[v1 // T.int64(256), v2, v1 % T.int64(256)]

    @T.prim_func(private=True)
    def fused_conv1d11_add10(p_lv571: T.handle, wnconv1d63: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32"), lv577: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv571 = T.match_buffer(p_lv571, (batch_size, T.int64(192), T.int64(274)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(192), T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(192)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(1344)), scope="shared")
        wnconv1d63_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(192), T.int64(1344)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(6), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(8), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(168)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1344), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv571[v1 // T.int64(256), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(256)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv571[v1 // T.int64(256), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(256)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d63_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1344), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d63[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d63_reindex_shared[v0, v1, v2])
                                                        wnconv1d63_reindex_shared[v0, v1, v2] = wnconv1d63[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(1344), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d63_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d63_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(256) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv577[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)])
                                            T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv577[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d12_add10(p_lv591: T.handle, wnconv1d65: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32"), lv597: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv591 = T.match_buffer(p_lv591, (batch_size, T.int64(192), T.int64(310)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(192), T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(192)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(1344)), scope="shared")
        wnconv1d65_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(192), T.int64(1344)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(6), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(8), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(168)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1344), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv591[v1 // T.int64(256), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(256)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv591[v1 // T.int64(256), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(256)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d65_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1344), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d65[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d65_reindex_shared[v0, v1, v2])
                                                        wnconv1d65_reindex_shared[v0, v1, v2] = wnconv1d65[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(1344), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d65_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d65_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(256) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv597[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)])
                                            T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv597[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d13_add14(p_lv617: T.handle, wnconv1d67: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32"), lv623: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv617 = T.match_buffer(p_lv617, (batch_size, T.int64(96), T.int64(518)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(96), T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(96)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(672)), scope="shared")
        wnconv1d67_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(96), T.int64(672)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(3), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(16), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(84)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(672), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv617[v1 // T.int64(512), v2 // T.int64(7), v1 % T.int64(512) + v2 % T.int64(7)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv617[v1 // T.int64(512), v2 // T.int64(7), v1 % T.int64(512) + v2 % T.int64(7)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d67_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(672), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d67[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d67_reindex_shared[v0, v1, v2])
                                                        wnconv1d67_reindex_shared[v0, v1, v2] = wnconv1d67[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(672), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d67_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d67_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(512) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv623[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)])
                                            T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv623[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d14_add14_add15_add16(p_lv626: T.handle, wnconv1d68: T.Buffer((T.int64(96), T.int64(96), T.int64(1)), "float32"), lv632: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_lv633: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv626 = T.match_buffer(p_lv626, (batch_size, T.int64(96), T.int64(512)))
        lv633 = T.match_buffer(p_lv633, (batch_size, T.int64(96), T.int64(512)))
        T_add_intermediate_1_2 = T.match_buffer(p_output0, (batch_size, T.int64(96), T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(96)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(96)), scope="shared")
        wnconv1d68_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(96), T.int64(96)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(3), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(16), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(12)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(96), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv626[v1 // T.int64(512), v2, v1 % T.int64(512)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv626[v1 // T.int64(512), v2, v1 % T.int64(512)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d68_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(96), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d68[v1, v2, T.int64(0)])
                                                        T.writes(wnconv1d68_reindex_shared[v0, v1, v2])
                                                        wnconv1d68_reindex_shared[v0, v1, v2] = wnconv1d68[v1, v2, T.int64(0)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(96), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d68_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d68_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(512) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv632[T.int64(0), v2, T.int64(0)], lv633[v1 // T.int64(512), v2, v1 % T.int64(512)])
                                            T.writes(T_add_intermediate_1_2[v1 // T.int64(512), v2, v1 % T.int64(512)])
                                            T_add_intermediate_1_2[v1 // T.int64(512), v2, v1 % T.int64(512)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv632[T.int64(0), v2, T.int64(0)] + lv633[v1 // T.int64(512), v2, v1 % T.int64(512)]

    @T.prim_func(private=True)
    def fused_conv1d15_add14(p_lv637: T.handle, wnconv1d69: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32"), lv643: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv637 = T.match_buffer(p_lv637, (batch_size, T.int64(96), T.int64(530)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(96), T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(96)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(672)), scope="shared")
        wnconv1d69_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(96), T.int64(672)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(3), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(16), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(84)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(672), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv637[v1 // T.int64(512), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(512)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv637[v1 // T.int64(512), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(512)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d69_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(672), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d69[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d69_reindex_shared[v0, v1, v2])
                                                        wnconv1d69_reindex_shared[v0, v1, v2] = wnconv1d69[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(672), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d69_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d69_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(512) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv643[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)])
                                            T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv643[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d16_add14(p_lv657: T.handle, wnconv1d71: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32"), lv663: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv657 = T.match_buffer(p_lv657, (batch_size, T.int64(96), T.int64(566)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(96), T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(96)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(672)), scope="shared")
        wnconv1d71_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(96), T.int64(672)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(3), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(16), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(84)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(672), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv657[v1 // T.int64(512), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(512)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv657[v1 // T.int64(512), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(512)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d71_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(672), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d71[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d71_reindex_shared[v0, v1, v2])
                                                        wnconv1d71_reindex_shared[v0, v1, v2] = wnconv1d71[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(672), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d71_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d71_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(96), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(512) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv663[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)])
                                            T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv663[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d17_reshape10_add17_tir_tanh(p_lv676: T.handle, wnconv1d73: T.Buffer((T.int64(1), T.int64(96), T.int64(7)), "float32"), decoder_model_layers_6_bias1: T.Buffer((T.int64(1),), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv676 = T.match_buffer(p_lv676, (batch_size, T.int64(96), T.int64(518)))
        compute_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1), T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_local = T.alloc_buffer((batch_size, T.int64(1), T.int64(512)), scope="local")
        conv1d_ncw_intermediate_rf_local = T.alloc_buffer((T.int64(256), batch_size, T.int64(1), T.int64(512)), scope="local")
        for ax0_ax1_fused in T.thread_binding(batch_size * T.int64(512), thread="blockIdx.x"):
            for ax2_ax3_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("conv1d_ncw_rf_init"):
                    vax2_ax3_fused_1 = T.axis.spatial(T.int64(256), ax2_ax3_fused_1)
                    v0 = T.axis.spatial(batch_size, ax0_ax1_fused // T.int64(512))
                    v1 = T.axis.spatial(T.int64(512), ax0_ax1_fused % T.int64(512))
                    T.reads()
                    T.writes(conv1d_ncw_intermediate_rf_local[vax2_ax3_fused_1, v0, T.int64(0), v1])
                    conv1d_ncw_intermediate_rf_local[vax2_ax3_fused_1, v0, T.int64(0), v1] = T.float32(0.0)
                for ax2_ax3_fused_0, u in T.grid(T.int64(3), 1):
                    with T.block("conv1d_ncw_rf_update"):
                        vax2_ax3_fused_1 = T.axis.spatial(T.int64(256), ax2_ax3_fused_1)
                        v0 = T.axis.spatial(batch_size, ax0_ax1_fused // T.int64(512))
                        v1 = T.axis.spatial(T.int64(512), ax0_ax1_fused % T.int64(512))
                        vax2_ax3_fused_0 = T.axis.reduce(T.int64(3), ax2_ax3_fused_0)
                        T.where(ax2_ax3_fused_0 * T.int64(256) + ax2_ax3_fused_1 < T.int64(672))
                        T.reads(conv1d_ncw_intermediate_rf_local[vax2_ax3_fused_1, v0, T.int64(0), v1], lv676[v0, (vax2_ax3_fused_0 * T.int64(256) + vax2_ax3_fused_1) // T.int64(7), v1 + (vax2_ax3_fused_0 * T.int64(256) + vax2_ax3_fused_1) % T.int64(7)], wnconv1d73[T.int64(0), (vax2_ax3_fused_0 * T.int64(256) + vax2_ax3_fused_1) // T.int64(7), (vax2_ax3_fused_0 * T.int64(256) + vax2_ax3_fused_1) % T.int64(7)])
                        T.writes(conv1d_ncw_intermediate_rf_local[vax2_ax3_fused_1, v0, T.int64(0), v1])
                        conv1d_ncw_intermediate_rf_local[vax2_ax3_fused_1, v0, T.int64(0), v1] = conv1d_ncw_intermediate_rf_local[vax2_ax3_fused_1, v0, T.int64(0), v1] + lv676[v0, (vax2_ax3_fused_0 * T.int64(256) + vax2_ax3_fused_1) // T.int64(7), v1 + (vax2_ax3_fused_0 * T.int64(256) + vax2_ax3_fused_1) % T.int64(7)] * wnconv1d73[T.int64(0), (vax2_ax3_fused_0 * T.int64(256) + vax2_ax3_fused_1) // T.int64(7), (vax2_ax3_fused_0 * T.int64(256) + vax2_ax3_fused_1) % T.int64(7)]
            for ax1_ax2_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("conv1d_ncw"):
                        vax2_ax3_fused_1 = T.axis.reduce(T.int64(256), ax0)
                        v0 = T.axis.spatial(batch_size, ax0_ax1_fused // T.int64(512))
                        v1 = T.axis.spatial(T.int64(512), ax0_ax1_fused % T.int64(512))
                        T.reads(conv1d_ncw_intermediate_rf_local[vax2_ax3_fused_1, v0, T.int64(0), v1])
                        T.writes(conv1d_ncw_intermediate_local[v0, T.int64(0), v1])
                        with T.init():
                            conv1d_ncw_intermediate_local[v0, T.int64(0), v1] = T.float32(0.0)
                        conv1d_ncw_intermediate_local[v0, T.int64(0), v1] = conv1d_ncw_intermediate_local[v0, T.int64(0), v1] + conv1d_ncw_intermediate_rf_local[vax2_ax3_fused_1, v0, T.int64(0), v1]
            for ax0, ax1 in T.grid(T.int64(1), T.int64(1)):
                with T.block("compute"):
                    v0 = T.axis.spatial(batch_size, ax0_ax1_fused // T.int64(512) + ax0)
                    v1 = T.axis.spatial(T.int64(512), ax0_ax1_fused % T.int64(512) + ax1)
                    T.reads(conv1d_ncw_intermediate_local[v0, T.int64(0), v1], decoder_model_layers_6_bias1[T.int64(0)])
                    T.writes(compute_intermediate[v0, T.int64(0), v1])
                    compute_intermediate[v0, T.int64(0), v1] = T.tanh(conv1d_ncw_intermediate_local[v0, T.int64(0), v1] + decoder_model_layers_6_bias1[T.int64(0)])

    @T.prim_func(private=True)
    def fused_conv1d18_add18(p_lv2: T.handle, wnconv1d: T.Buffer((T.int64(64), T.int64(1), T.int64(7)), "float32"), lv8: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv2 = T.match_buffer(p_lv2, (batch_size, T.int64(1), T.int64(518)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(64), T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(64)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(8)), scope="shared")
        wnconv1d_reindex_pad_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(8)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(16), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(1)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv2[v1 // T.int64(512), T.int64(0), v1 % T.int64(512) + v2])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v2 < T.int64(7), lv2[v1 // T.int64(512), T.int64(0), v1 % T.int64(512) + v2], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d[v1, T.int64(0), v2])
                                                        T.writes(wnconv1d_reindex_pad_shared[v0, v1, v2])
                                                        wnconv1d_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v2 < T.int64(7), wnconv1d[v1, T.int64(0), v2], T.float32(0.0))
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(8), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d_reindex_pad_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d_reindex_pad_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(512) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv8[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)])
                                            T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv8[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d19_add18(p_lv12: T.handle, wnconv1d1: T.Buffer((T.int64(64), T.int64(64), T.int64(7)), "float32"), lv18: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv12 = T.match_buffer(p_lv12, (batch_size, T.int64(64), T.int64(518)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(64), T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(64)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(448)), scope="shared")
        wnconv1d1_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(448)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(16), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(56)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(448), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv12[v1 // T.int64(512), v2 // T.int64(7), v1 % T.int64(512) + v2 % T.int64(7)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv12[v1 // T.int64(512), v2 // T.int64(7), v1 % T.int64(512) + v2 % T.int64(7)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d1_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(448), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d1[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d1_reindex_shared[v0, v1, v2])
                                                        wnconv1d1_reindex_shared[v0, v1, v2] = wnconv1d1[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(448), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d1_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d1_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(512) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv18[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)])
                                            T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv18[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d1_add2(p_lv419: T.handle, wnconv1d49: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32"), lv425: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv419 = T.match_buffer(p_lv419, (batch_size, T.int64(768), T.int64(14)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(768), T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(768)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(5376)), scope="shared")
        wnconv1d49_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(768), T.int64(5376)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(24), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(672)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(5376), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv419[v1 // T.int64(8), v2 // T.int64(7), v1 % T.int64(8) + v2 % T.int64(7)])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size * T.int64(8), lv419[v1 // T.int64(8), v2 // T.int64(7), v1 % T.int64(8) + v2 % T.int64(7)], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d49_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(5376), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d49[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d49_reindex_shared[v0, v1, v2])
                                                        wnconv1d49_reindex_shared[v0, v1, v2] = wnconv1d49[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(5376), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d49_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d49_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(4) + (ax1_2 * T.int64(4) + ax1) // T.int64(8) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv425[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)])
                                            T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv425[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d20_add18_add19_add20(p_lv21: T.handle, wnconv1d2: T.Buffer((T.int64(64), T.int64(64), T.int64(1)), "float32"), lv27: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32"), p_lv28: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv21 = T.match_buffer(p_lv21, (batch_size, T.int64(64), T.int64(512)))
        lv28 = T.match_buffer(p_lv28, (batch_size, T.int64(64), T.int64(512)))
        T_add_intermediate_1_2 = T.match_buffer(p_output0, (batch_size, T.int64(64), T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(64)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(64)), scope="shared")
        wnconv1d2_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(64)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(16), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(8)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(64), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv21[v1 // T.int64(512), v2, v1 % T.int64(512)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv21[v1 // T.int64(512), v2, v1 % T.int64(512)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d2_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(64), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d2[v1, v2, T.int64(0)])
                                                        T.writes(wnconv1d2_reindex_shared[v0, v1, v2])
                                                        wnconv1d2_reindex_shared[v0, v1, v2] = wnconv1d2[v1, v2, T.int64(0)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(64), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d2_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d2_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(512) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv27[T.int64(0), v2, T.int64(0)], lv28[v1 // T.int64(512), v2, v1 % T.int64(512)])
                                            T.writes(T_add_intermediate_1_2[v1 // T.int64(512), v2, v1 % T.int64(512)])
                                            T_add_intermediate_1_2[v1 // T.int64(512), v2, v1 % T.int64(512)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv27[T.int64(0), v2, T.int64(0)] + lv28[v1 // T.int64(512), v2, v1 % T.int64(512)]

    @T.prim_func(private=True)
    def fused_conv1d21_add18(p_lv32: T.handle, wnconv1d3: T.Buffer((T.int64(64), T.int64(64), T.int64(7)), "float32"), lv38: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv32 = T.match_buffer(p_lv32, (batch_size, T.int64(64), T.int64(530)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(64), T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(64)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(448)), scope="shared")
        wnconv1d3_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(448)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(16), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(56)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(448), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv32[v1 // T.int64(512), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(512)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv32[v1 // T.int64(512), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(512)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d3_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(448), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d3[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d3_reindex_shared[v0, v1, v2])
                                                        wnconv1d3_reindex_shared[v0, v1, v2] = wnconv1d3[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(448), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d3_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d3_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(512) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv38[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)])
                                            T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv38[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d22_add18(p_lv52: T.handle, wnconv1d5: T.Buffer((T.int64(64), T.int64(64), T.int64(7)), "float32"), lv58: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv52 = T.match_buffer(p_lv52, (batch_size, T.int64(64), T.int64(566)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(64), T.int64(512)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(64)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(512), T.int64(448)), scope="shared")
        wnconv1d5_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(448)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(16), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(56)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(448), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv52[v1 // T.int64(512), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(512)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv52[v1 // T.int64(512), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(512)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d5_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(448), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d5[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d5_reindex_shared[v0, v1, v2])
                                                        wnconv1d5_reindex_shared[v0, v1, v2] = wnconv1d5[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(448), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d5_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d5_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(512), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(64), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(512) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv58[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)])
                                            T_add_intermediate[v1 // T.int64(512), v2, v1 % T.int64(512)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv58[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d23_add21(p_lv71: T.handle, wnconv1d7: T.Buffer((T.int64(128), T.int64(64), T.int64(4)), "float32"), lv77: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv71 = T.match_buffer(p_lv71, (batch_size, T.int64(64), T.int64(514)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(128), T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(128)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(256)), scope="shared")
        wnconv1d7_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(256)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(8), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(32)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(256), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv71[v1 // T.int64(256), v2 // T.int64(4), v1 % T.int64(256) * T.int64(2) + v2 % T.int64(4)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv71[v1 // T.int64(256), v2 // T.int64(4), v1 % T.int64(256) * T.int64(2) + v2 % T.int64(4)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d7_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(256), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d7[v1, v2 // T.int64(4), v2 % T.int64(4)])
                                                        T.writes(wnconv1d7_reindex_shared[v0, v1, v2])
                                                        wnconv1d7_reindex_shared[v0, v1, v2] = wnconv1d7[v1, v2 // T.int64(4), v2 % T.int64(4)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(256), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d7_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d7_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(256) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv77[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)])
                                            T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv77[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d24_add21(p_lv81: T.handle, wnconv1d8: T.Buffer((T.int64(128), T.int64(128), T.int64(7)), "float32"), lv87: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv81 = T.match_buffer(p_lv81, (batch_size, T.int64(128), T.int64(262)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(128), T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(128)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(896)), scope="shared")
        wnconv1d8_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(896)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(8), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(112)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(896), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv81[v1 // T.int64(256), v2 // T.int64(7), v1 % T.int64(256) + v2 % T.int64(7)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv81[v1 // T.int64(256), v2 // T.int64(7), v1 % T.int64(256) + v2 % T.int64(7)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d8_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(896), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d8[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d8_reindex_shared[v0, v1, v2])
                                                        wnconv1d8_reindex_shared[v0, v1, v2] = wnconv1d8[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(896), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d8_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d8_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(256) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv87[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)])
                                            T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv87[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d25_add21_add22_add23(p_lv90: T.handle, wnconv1d9: T.Buffer((T.int64(128), T.int64(128), T.int64(1)), "float32"), lv96: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32"), p_lv97: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv90 = T.match_buffer(p_lv90, (batch_size, T.int64(128), T.int64(256)))
        lv97 = T.match_buffer(p_lv97, (batch_size, T.int64(128), T.int64(256)))
        T_add_intermediate_1_2 = T.match_buffer(p_output0, (batch_size, T.int64(128), T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(128)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(128)), scope="shared")
        wnconv1d9_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(128)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(8), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(16)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(128), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv90[v1 // T.int64(256), v2, v1 % T.int64(256)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv90[v1 // T.int64(256), v2, v1 % T.int64(256)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d9_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(128), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d9[v1, v2, T.int64(0)])
                                                        T.writes(wnconv1d9_reindex_shared[v0, v1, v2])
                                                        wnconv1d9_reindex_shared[v0, v1, v2] = wnconv1d9[v1, v2, T.int64(0)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(128), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d9_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d9_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(256) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv96[T.int64(0), v2, T.int64(0)], lv97[v1 // T.int64(256), v2, v1 % T.int64(256)])
                                            T.writes(T_add_intermediate_1_2[v1 // T.int64(256), v2, v1 % T.int64(256)])
                                            T_add_intermediate_1_2[v1 // T.int64(256), v2, v1 % T.int64(256)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv96[T.int64(0), v2, T.int64(0)] + lv97[v1 // T.int64(256), v2, v1 % T.int64(256)]

    @T.prim_func(private=True)
    def fused_conv1d26_add21(p_lv101: T.handle, wnconv1d10: T.Buffer((T.int64(128), T.int64(128), T.int64(7)), "float32"), lv107: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv101 = T.match_buffer(p_lv101, (batch_size, T.int64(128), T.int64(274)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(128), T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(128)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(896)), scope="shared")
        wnconv1d10_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(896)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(8), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(112)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(896), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv101[v1 // T.int64(256), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(256)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv101[v1 // T.int64(256), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(256)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d10_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(896), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d10[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d10_reindex_shared[v0, v1, v2])
                                                        wnconv1d10_reindex_shared[v0, v1, v2] = wnconv1d10[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(896), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d10_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d10_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(256) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv107[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)])
                                            T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv107[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d27_add21(p_lv121: T.handle, wnconv1d12: T.Buffer((T.int64(128), T.int64(128), T.int64(7)), "float32"), lv127: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv121 = T.match_buffer(p_lv121, (batch_size, T.int64(128), T.int64(310)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(128), T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(128)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(896)), scope="shared")
        wnconv1d12_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(128), T.int64(896)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(4), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(8), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(112)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(896), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv121[v1 // T.int64(256), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(256)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv121[v1 // T.int64(256), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(256)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d12_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(896), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d12[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d12_reindex_shared[v0, v1, v2])
                                                        wnconv1d12_reindex_shared[v0, v1, v2] = wnconv1d12[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(896), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d12_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d12_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(128), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(256) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv127[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)])
                                            T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv127[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d28_add24(p_lv140: T.handle, wnconv1d14: T.Buffer((T.int64(256), T.int64(128), T.int64(8)), "float32"), lv146: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv140 = T.match_buffer(p_lv140, (batch_size, T.int64(128), T.int64(260)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(256), T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(256)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(1024)), scope="shared")
        wnconv1d14_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(1024)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(2), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(128)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1024), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv140[v1 // T.int64(64), v2 // T.int64(8), v1 % T.int64(64) * T.int64(4) + v2 % T.int64(8)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv140[v1 // T.int64(64), v2 // T.int64(8), v1 % T.int64(64) * T.int64(4) + v2 % T.int64(8)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d14_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1024), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d14[v1, v2 // T.int64(8), v2 % T.int64(8)])
                                                        T.writes(wnconv1d14_reindex_shared[v0, v1, v2])
                                                        wnconv1d14_reindex_shared[v0, v1, v2] = wnconv1d14[v1, v2 // T.int64(8), v2 % T.int64(8)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(1024), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d14_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d14_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(64) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv146[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)])
                                            T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv146[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d29_add24(p_lv150: T.handle, wnconv1d15: T.Buffer((T.int64(256), T.int64(256), T.int64(7)), "float32"), lv156: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv150 = T.match_buffer(p_lv150, (batch_size, T.int64(256), T.int64(70)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(256), T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(256)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(1792)), scope="shared")
        wnconv1d15_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(1792)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(2), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(224)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1792), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv150[v1 // T.int64(64), v2 // T.int64(7), v1 % T.int64(64) + v2 % T.int64(7)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv150[v1 // T.int64(64), v2 // T.int64(7), v1 % T.int64(64) + v2 % T.int64(7)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d15_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1792), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d15[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d15_reindex_shared[v0, v1, v2])
                                                        wnconv1d15_reindex_shared[v0, v1, v2] = wnconv1d15[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(1792), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d15_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d15_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(64) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv156[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)])
                                            T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv156[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d2_add2_add3_add4(p_lv428: T.handle, wnconv1d50: T.Buffer((T.int64(768), T.int64(768), T.int64(1)), "float32"), lv434: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_lv435: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv428 = T.match_buffer(p_lv428, (batch_size, T.int64(768), T.int64(8)))
        lv435 = T.match_buffer(p_lv435, (batch_size, T.int64(768), T.int64(8)))
        T_add_intermediate_1_2 = T.match_buffer(p_output0, (batch_size, T.int64(768), T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(768)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(768)), scope="shared")
        wnconv1d50_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(768), T.int64(768)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(24), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(96)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(768), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv428[v1 // T.int64(8), v2, v1 % T.int64(8)])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size * T.int64(8), lv428[v1 // T.int64(8), v2, v1 % T.int64(8)], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d50_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(768), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d50[v1, v2, T.int64(0)])
                                                        T.writes(wnconv1d50_reindex_shared[v0, v1, v2])
                                                        wnconv1d50_reindex_shared[v0, v1, v2] = wnconv1d50[v1, v2, T.int64(0)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(768), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d50_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d50_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(4) + (ax1_2 * T.int64(4) + ax1) // T.int64(8) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv434[T.int64(0), v2, T.int64(0)], lv435[v1 // T.int64(8), v2, v1 % T.int64(8)])
                                            T.writes(T_add_intermediate_1_2[v1 // T.int64(8), v2, v1 % T.int64(8)])
                                            T_add_intermediate_1_2[v1 // T.int64(8), v2, v1 % T.int64(8)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv434[T.int64(0), v2, T.int64(0)] + lv435[v1 // T.int64(8), v2, v1 % T.int64(8)]

    @T.prim_func(private=True)
    def fused_conv1d30_add24_add25_add26(p_lv159: T.handle, wnconv1d16: T.Buffer((T.int64(256), T.int64(256), T.int64(1)), "float32"), lv165: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32"), p_lv166: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv159 = T.match_buffer(p_lv159, (batch_size, T.int64(256), T.int64(64)))
        lv166 = T.match_buffer(p_lv166, (batch_size, T.int64(256), T.int64(64)))
        T_add_intermediate_1_2 = T.match_buffer(p_output0, (batch_size, T.int64(256), T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(256)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(256)), scope="shared")
        wnconv1d16_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(256)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(2), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(32)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(256), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv159[v1 // T.int64(64), v2, v1 % T.int64(64)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv159[v1 // T.int64(64), v2, v1 % T.int64(64)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d16_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(256), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d16[v1, v2, T.int64(0)])
                                                        T.writes(wnconv1d16_reindex_shared[v0, v1, v2])
                                                        wnconv1d16_reindex_shared[v0, v1, v2] = wnconv1d16[v1, v2, T.int64(0)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(256), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d16_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d16_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(64) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv165[T.int64(0), v2, T.int64(0)], lv166[v1 // T.int64(64), v2, v1 % T.int64(64)])
                                            T.writes(T_add_intermediate_1_2[v1 // T.int64(64), v2, v1 % T.int64(64)])
                                            T_add_intermediate_1_2[v1 // T.int64(64), v2, v1 % T.int64(64)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv165[T.int64(0), v2, T.int64(0)] + lv166[v1 // T.int64(64), v2, v1 % T.int64(64)]

    @T.prim_func(private=True)
    def fused_conv1d31_add24(p_lv170: T.handle, wnconv1d17: T.Buffer((T.int64(256), T.int64(256), T.int64(7)), "float32"), lv176: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv170 = T.match_buffer(p_lv170, (batch_size, T.int64(256), T.int64(82)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(256), T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(256)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(1792)), scope="shared")
        wnconv1d17_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(1792)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(2), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(224)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1792), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv170[v1 // T.int64(64), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(64)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv170[v1 // T.int64(64), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(64)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d17_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1792), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d17[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d17_reindex_shared[v0, v1, v2])
                                                        wnconv1d17_reindex_shared[v0, v1, v2] = wnconv1d17[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(1792), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d17_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d17_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(64) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv176[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)])
                                            T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv176[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d32_add24(p_lv190: T.handle, wnconv1d19: T.Buffer((T.int64(256), T.int64(256), T.int64(7)), "float32"), lv196: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv190 = T.match_buffer(p_lv190, (batch_size, T.int64(256), T.int64(118)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(256), T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(256)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(1792)), scope="shared")
        wnconv1d19_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(256), T.int64(1792)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(8), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(2), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(224)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1792), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv190[v1 // T.int64(64), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(64)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv190[v1 // T.int64(64), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(64)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d19_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1792), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d19[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d19_reindex_shared[v0, v1, v2])
                                                        wnconv1d19_reindex_shared[v0, v1, v2] = wnconv1d19[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(1792), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d19_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d19_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(256), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(64) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv196[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)])
                                            T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv196[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d33_add27(p_lv209: T.handle, wnconv1d21: T.Buffer((T.int64(512), T.int64(256), T.int64(16)), "float32"), lv215: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv209 = T.match_buffer(p_lv209, (batch_size, T.int64(256), T.int64(72)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(512), T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(512)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(4096)), scope="shared")
        wnconv1d21_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(4096)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(512)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(4096), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv209[v1 // T.int64(8), v2 // T.int64(16), v1 % T.int64(8) * T.int64(8) + v2 % T.int64(16)])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size * T.int64(8), lv209[v1 // T.int64(8), v2 // T.int64(16), v1 % T.int64(8) * T.int64(8) + v2 % T.int64(16)], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d21_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(4096), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d21[v1, v2 // T.int64(16), v2 % T.int64(16)])
                                                        T.writes(wnconv1d21_reindex_shared[v0, v1, v2])
                                                        wnconv1d21_reindex_shared[v0, v1, v2] = wnconv1d21[v1, v2 // T.int64(16), v2 % T.int64(16)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(4096), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d21_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d21_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(4) + (ax1_2 * T.int64(4) + ax1) // T.int64(8) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv215[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)])
                                            T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv215[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d34_add27(p_lv219: T.handle, wnconv1d22: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), "float32"), lv225: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv219 = T.match_buffer(p_lv219, (batch_size, T.int64(512), T.int64(14)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(512), T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(512)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(3584)), scope="shared")
        wnconv1d22_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(3584)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(448)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(3584), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv219[v1 // T.int64(8), v2 // T.int64(7), v1 % T.int64(8) + v2 % T.int64(7)])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size * T.int64(8), lv219[v1 // T.int64(8), v2 // T.int64(7), v1 % T.int64(8) + v2 % T.int64(7)], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d22_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(3584), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d22[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d22_reindex_shared[v0, v1, v2])
                                                        wnconv1d22_reindex_shared[v0, v1, v2] = wnconv1d22[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(3584), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d22_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d22_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(4) + (ax1_2 * T.int64(4) + ax1) // T.int64(8) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv225[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)])
                                            T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv225[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d35_add27_add28_add29(p_lv228: T.handle, wnconv1d23: T.Buffer((T.int64(512), T.int64(512), T.int64(1)), "float32"), lv234: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32"), p_lv235: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv228 = T.match_buffer(p_lv228, (batch_size, T.int64(512), T.int64(8)))
        lv235 = T.match_buffer(p_lv235, (batch_size, T.int64(512), T.int64(8)))
        T_add_intermediate_1_2 = T.match_buffer(p_output0, (batch_size, T.int64(512), T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(512)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(512)), scope="shared")
        wnconv1d23_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(512)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(64)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(512), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv228[v1 // T.int64(8), v2, v1 % T.int64(8)])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size * T.int64(8), lv228[v1 // T.int64(8), v2, v1 % T.int64(8)], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d23_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(512), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d23[v1, v2, T.int64(0)])
                                                        T.writes(wnconv1d23_reindex_shared[v0, v1, v2])
                                                        wnconv1d23_reindex_shared[v0, v1, v2] = wnconv1d23[v1, v2, T.int64(0)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(512), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d23_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d23_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(4) + (ax1_2 * T.int64(4) + ax1) // T.int64(8) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv234[T.int64(0), v2, T.int64(0)], lv235[v1 // T.int64(8), v2, v1 % T.int64(8)])
                                            T.writes(T_add_intermediate_1_2[v1 // T.int64(8), v2, v1 % T.int64(8)])
                                            T_add_intermediate_1_2[v1 // T.int64(8), v2, v1 % T.int64(8)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv234[T.int64(0), v2, T.int64(0)] + lv235[v1 // T.int64(8), v2, v1 % T.int64(8)]

    @T.prim_func(private=True)
    def fused_conv1d36_add27(p_lv239: T.handle, wnconv1d24: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), "float32"), lv245: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv239 = T.match_buffer(p_lv239, (batch_size, T.int64(512), T.int64(26)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(512), T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(512)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(3584)), scope="shared")
        wnconv1d24_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(3584)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(448)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(3584), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv239[v1 // T.int64(8), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(8)])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size * T.int64(8), lv239[v1 // T.int64(8), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(8)], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d24_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(3584), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d24[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d24_reindex_shared[v0, v1, v2])
                                                        wnconv1d24_reindex_shared[v0, v1, v2] = wnconv1d24[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(3584), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d24_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d24_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(4) + (ax1_2 * T.int64(4) + ax1) // T.int64(8) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv245[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)])
                                            T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv245[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d37_add27(p_lv259: T.handle, wnconv1d26: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), "float32"), lv265: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv259 = T.match_buffer(p_lv259, (batch_size, T.int64(512), T.int64(62)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(512), T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(512)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(3584)), scope="shared")
        wnconv1d26_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(512), T.int64(3584)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(16), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(448)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(3584), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv259[v1 // T.int64(8), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(8)])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size * T.int64(8), lv259[v1 // T.int64(8), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(8)], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d26_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(3584), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d26[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d26_reindex_shared[v0, v1, v2])
                                                        wnconv1d26_reindex_shared[v0, v1, v2] = wnconv1d26[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(3584), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d26_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d26_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(512), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(4) + (ax1_2 * T.int64(4) + ax1) // T.int64(8) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv265[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)])
                                            T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv265[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d38_add30(p_lv278: T.handle, wnconv1d28: T.Buffer((T.int64(1024), T.int64(512), T.int64(16)), "float32"), lv284: T.Buffer((T.int64(1), T.int64(1024), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv278 = T.match_buffer(p_lv278, (batch_size, T.int64(512), T.int64(16)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1024), T.int64(1)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(1024)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(8192)), scope="shared")
        wnconv1d28_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(8192)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(1024)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(8192), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv278[v1, v2 // T.int64(16), v2 % T.int64(16)])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size, lv278[v1, v2 // T.int64(16), v2 % T.int64(16)], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d28_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(8192), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d28[v1, v2 // T.int64(16), v2 % T.int64(16)])
                                                        T.writes(wnconv1d28_reindex_shared[v0, v1, v2])
                                                        wnconv1d28_reindex_shared[v0, v1, v2] = wnconv1d28[v1, v2 // T.int64(16), v2 % T.int64(16)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(8192), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d28_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d28_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1 < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv284[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1, v2, T.int64(0)])
                                            T_add_intermediate[v1, v2, T.int64(0)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv284[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d39_add30(p_lv287: T.handle, wnconv1d29: T.Buffer((T.int64(1024), T.int64(1024), T.int64(3)), "float32"), lv293: T.Buffer((T.int64(1), T.int64(1024), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv287 = T.match_buffer(p_lv287, (batch_size, T.int64(1024), T.int64(3)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1024), T.int64(1)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(1024)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(3072)), scope="shared")
        wnconv1d29_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(3072)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(384)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(3072), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv287[v1, v2 // T.int64(3), v2 % T.int64(3)])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size, lv287[v1, v2 // T.int64(3), v2 % T.int64(3)], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d29_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(3072), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d29[v1, v2 // T.int64(3), v2 % T.int64(3)])
                                                        T.writes(wnconv1d29_reindex_shared[v0, v1, v2])
                                                        wnconv1d29_reindex_shared[v0, v1, v2] = wnconv1d29[v1, v2 // T.int64(3), v2 % T.int64(3)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(3072), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d29_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d29_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1 < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv293[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1, v2, T.int64(0)])
                                            T_add_intermediate[v1, v2, T.int64(0)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv293[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d3_add2(p_lv439: T.handle, wnconv1d51: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32"), lv445: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv439 = T.match_buffer(p_lv439, (batch_size, T.int64(768), T.int64(26)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(768), T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(768)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(5376)), scope="shared")
        wnconv1d51_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(768), T.int64(5376)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(24), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(672)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(5376), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv439[v1 // T.int64(8), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(8)])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size * T.int64(8), lv439[v1 // T.int64(8), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(8)], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d51_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(5376), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d51[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d51_reindex_shared[v0, v1, v2])
                                                        wnconv1d51_reindex_shared[v0, v1, v2] = wnconv1d51[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(5376), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d51_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d51_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(4) + (ax1_2 * T.int64(4) + ax1) // T.int64(8) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv445[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)])
                                            T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv445[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d40_add31(p_conv1d29: T.handle, wnconv1d30: T.Buffer((T.int64(8), T.int64(1024), T.int64(1)), "float32"), lv299: T.Buffer((T.int64(1), T.int64(8), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d29 = T.match_buffer(p_conv1d29, (batch_size, T.int64(1024), T.int64(1)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(8), T.int64(1)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(32)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(1024)), scope="shared")
        wnconv1d30_reindex_pad_shared = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(1024)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(1), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(32), ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(128)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1024), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(conv1d29[v1, v2, T.Add(T.int64(0), T.int64(0))])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size, conv1d29[v1, v2, T.Add(T.int64(0), T.int64(0))], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d30_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1024), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d30[v1, v2, T.int64(0)])
                                                        T.writes(wnconv1d30_reindex_pad_shared[v0, v1, v2])
                                                        wnconv1d30_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < T.int64(8), wnconv1d30[v1, v2, T.int64(0)], T.float32(0.0))
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(32), ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(1024), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d30_reindex_pad_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d30_reindex_pad_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(32), ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1 < batch_size and ax2_2 * T.int64(4) + ax2_0 + ax2_1_1 < T.int64(8))
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv299[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1, v2, T.int64(0)])
                                            T_add_intermediate[v1, v2, T.int64(0)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv299[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d41_add30(p_permute_dims3: T.handle, wnconv1d31: T.Buffer((T.int64(1024), T.int64(8), T.int64(1)), "float32"), lv305: T.Buffer((T.int64(1), T.int64(1024), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        permute_dims3 = T.match_buffer(p_permute_dims3, (batch_size, T.int64(8), T.int64(1)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1024), T.int64(1)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(1024)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(8)), scope="shared")
        wnconv1d31_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(8)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(1)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(permute_dims3[v1, v2, T.Add(T.int64(0), T.int64(0))])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size, permute_dims3[v1, v2, T.Add(T.int64(0), T.int64(0))], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d31_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d31[v1, v2, T.int64(0)])
                                                        T.writes(wnconv1d31_reindex_shared[v0, v1, v2])
                                                        wnconv1d31_reindex_shared[v0, v1, v2] = wnconv1d31[v1, v2, T.int64(0)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(8), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d31_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d31_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1 < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv305[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1, v2, T.int64(0)])
                                            T_add_intermediate[v1, v2, T.int64(0)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv305[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d4_add2(p_lv459: T.handle, wnconv1d53: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32"), lv465: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv459 = T.match_buffer(p_lv459, (batch_size, T.int64(768), T.int64(62)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(768), T.int64(8)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(768)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(5376)), scope="shared")
        wnconv1d53_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(768), T.int64(5376)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(24), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(672)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(5376), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv459[v1 // T.int64(8), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(8)])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size * T.int64(8), lv459[v1 // T.int64(8), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(8)], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d53_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(5376), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d53[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d53_reindex_shared[v0, v1, v2])
                                                        wnconv1d53_reindex_shared[v0, v1, v2] = wnconv1d53[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(5376), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d53_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d53_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size * T.int64(8) + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(768), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(4) + (ax1_2 * T.int64(4) + ax1) // T.int64(8) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv465[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)])
                                            T_add_intermediate[v1 // T.int64(8), v2, v1 % T.int64(8)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv465[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d5_add6(p_lv485: T.handle, wnconv1d55: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32"), lv491: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv485 = T.match_buffer(p_lv485, (batch_size, T.int64(384), T.int64(70)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(384), T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(384)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(2688)), scope="shared")
        wnconv1d55_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(2688)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(12), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(2), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(336)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(2688), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv485[v1 // T.int64(64), v2 // T.int64(7), v1 % T.int64(64) + v2 % T.int64(7)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv485[v1 // T.int64(64), v2 // T.int64(7), v1 % T.int64(64) + v2 % T.int64(7)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d55_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(2688), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d55[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d55_reindex_shared[v0, v1, v2])
                                                        wnconv1d55_reindex_shared[v0, v1, v2] = wnconv1d55[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(2688), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d55_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d55_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(64) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv491[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)])
                                            T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv491[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d6_add6_add7_add8(p_lv494: T.handle, wnconv1d56: T.Buffer((T.int64(384), T.int64(384), T.int64(1)), "float32"), lv500: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_lv501: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv494 = T.match_buffer(p_lv494, (batch_size, T.int64(384), T.int64(64)))
        lv501 = T.match_buffer(p_lv501, (batch_size, T.int64(384), T.int64(64)))
        T_add_intermediate_1_2 = T.match_buffer(p_output0, (batch_size, T.int64(384), T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(384)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(384)), scope="shared")
        wnconv1d56_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(384)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(12), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(2), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(48)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(384), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv494[v1 // T.int64(64), v2, v1 % T.int64(64)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv494[v1 // T.int64(64), v2, v1 % T.int64(64)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d56_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(384), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d56[v1, v2, T.int64(0)])
                                                        T.writes(wnconv1d56_reindex_shared[v0, v1, v2])
                                                        wnconv1d56_reindex_shared[v0, v1, v2] = wnconv1d56[v1, v2, T.int64(0)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(384), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d56_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d56_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(64) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv500[T.int64(0), v2, T.int64(0)], lv501[v1 // T.int64(64), v2, v1 % T.int64(64)])
                                            T.writes(T_add_intermediate_1_2[v1 // T.int64(64), v2, v1 % T.int64(64)])
                                            T_add_intermediate_1_2[v1 // T.int64(64), v2, v1 % T.int64(64)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv500[T.int64(0), v2, T.int64(0)] + lv501[v1 // T.int64(64), v2, v1 % T.int64(64)]

    @T.prim_func(private=True)
    def fused_conv1d7_add6(p_lv505: T.handle, wnconv1d57: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32"), lv511: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv505 = T.match_buffer(p_lv505, (batch_size, T.int64(384), T.int64(82)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(384), T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(384)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(2688)), scope="shared")
        wnconv1d57_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(2688)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(12), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(2), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(336)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(2688), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv505[v1 // T.int64(64), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(64)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv505[v1 // T.int64(64), v2 // T.int64(7), v2 % T.int64(7) * T.int64(3) + v1 % T.int64(64)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d57_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(2688), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d57[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d57_reindex_shared[v0, v1, v2])
                                                        wnconv1d57_reindex_shared[v0, v1, v2] = wnconv1d57[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(2688), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d57_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d57_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(64) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv511[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)])
                                            T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv511[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d8_add6(p_lv525: T.handle, wnconv1d59: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32"), lv531: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv525 = T.match_buffer(p_lv525, (batch_size, T.int64(384), T.int64(118)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(384), T.int64(64)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(384)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(64), T.int64(2688)), scope="shared")
        wnconv1d59_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(384), T.int64(2688)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(12), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(2), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(336)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(2688), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv525[v1 // T.int64(64), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(64)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv525[v1 // T.int64(64), v2 // T.int64(7), v2 % T.int64(7) * T.int64(9) + v1 % T.int64(64)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d59_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(2688), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d59[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d59_reindex_shared[v0, v1, v2])
                                                        wnconv1d59_reindex_shared[v0, v1, v2] = wnconv1d59[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(2688), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d59_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d59_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(64), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(384), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(64) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv531[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)])
                                            T_add_intermediate[v1 // T.int64(64), v2, v1 % T.int64(64)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv531[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d9_add10(p_lv551: T.handle, wnconv1d61: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32"), lv557: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv551 = T.match_buffer(p_lv551, (batch_size, T.int64(192), T.int64(262)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(192), T.int64(256)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_local = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(192)), scope="local")
        pad_temp_reindex_shared = T.alloc_buffer((T.int64(1), batch_size * T.int64(256), T.int64(1344)), scope="shared")
        wnconv1d61_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(192), T.int64(1344)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(6), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding(batch_size * T.int64(8), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(168)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1344), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv551[v1 // T.int64(256), v2 // T.int64(7), v1 % T.int64(256) + v2 % T.int64(7)])
                                                        T.writes(pad_temp_reindex_shared[v0, v1, v2])
                                                        pad_temp_reindex_shared[v0, v1, v2] = lv551[v1 // T.int64(256), v2 // T.int64(7), v1 % T.int64(256) + v2 % T.int64(7)]
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d61_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(1344), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d61[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d61_reindex_shared[v0, v1, v2])
                                                        wnconv1d61_reindex_shared[v0, v1, v2] = wnconv1d61[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(1344), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2], pad_temp_reindex_shared[T.int64(0), v1, v3], wnconv1d61_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_local[T.int64(0), v1, v2] + pad_temp_reindex_shared[T.int64(0), v1, v3] * wnconv1d61_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial(batch_size * T.int64(256), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(192), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where((ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1) // T.int64(256) < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_local[v0, v1, v2], lv557[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)])
                                            T_add_intermediate[v1 // T.int64(256), v2, v1 % T.int64(256)] = conv1d_ncw_intermediate_reindex_local[v0, v1, v2] + lv557[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_conv1d_add(p_lv403: T.handle, wnconv1d48: T.Buffer((T.int64(1536), T.int64(1024), T.int64(7)), "float32"), lv409: T.Buffer((T.int64(1), T.int64(1536), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv403 = T.match_buffer(p_lv403, (batch_size, T.int64(1024), T.int64(7)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1536), T.int64(1)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(1536)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(7168)), scope="shared")
        wnconv1d48_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(1536), T.int64(7168)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(48), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(1536), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(896)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(7168), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(lv403[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size, lv403[v1, v2 // T.int64(7), v2 % T.int64(7)], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d48_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(1536), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(7168), ax3_0 * T.int64(8) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d48[v1, v2 // T.int64(7), v2 % T.int64(7)])
                                                        T.writes(wnconv1d48_reindex_shared[v0, v1, v2])
                                                        wnconv1d48_reindex_shared[v0, v1, v2] = wnconv1d48[v1, v2 // T.int64(7), v2 % T.int64(7)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(1536), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(7168), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d48_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d48_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(1536), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1 < batch_size)
                                            T.reads(conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv409[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate[v1, v2, T.int64(0)])
                                            T_add_intermediate[v1, v2, T.int64(0)] = conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv409[T.int64(0), v2, T.int64(0)]

    @T.prim_func(private=True)
    def fused_expand_dims1_add5(decoder_model_layers_2_block_layers_1_bias1: T.Buffer((T.int64(384),), "float32"), p_lv481: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv481 = T.match_buffer(p_lv481, (batch_size, T.int64(384), T.int64(64)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(384), T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * T.int64(24), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_add"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(24576))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(24576) // T.int64(64))
                    v2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(64))
                    T.reads(lv481[v0, v1, v2], decoder_model_layers_2_block_layers_1_bias1[v1])
                    T.writes(T_add_intermediate[v0, v1, v2])
                    T_add_intermediate[v0, v1, v2] = lv481[v0, v1, v2] + decoder_model_layers_2_block_layers_1_bias1[v1]

    @T.prim_func(private=True)
    def fused_expand_dims2_add9(decoder_model_layers_3_block_layers_1_bias1: T.Buffer((T.int64(192),), "float32"), p_lv547: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv547 = T.match_buffer(p_lv547, (batch_size, T.int64(192), T.int64(256)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(192), T.int64(256)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_add"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(49152))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(49152) // T.int64(256))
                    v2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(256))
                    T.reads(lv547[v0, v1, v2], decoder_model_layers_3_block_layers_1_bias1[v1])
                    T.writes(T_add_intermediate[v0, v1, v2])
                    T_add_intermediate[v0, v1, v2] = lv547[v0, v1, v2] + decoder_model_layers_3_block_layers_1_bias1[v1]

    @T.prim_func(private=True)
    def fused_expand_dims3_add13(decoder_model_layers_4_block_layers_1_bias1: T.Buffer((T.int64(96),), "float32"), p_lv613: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv613 = T.match_buffer(p_lv613, (batch_size, T.int64(96), T.int64(512)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(96), T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_add"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(49152))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(49152) // T.int64(512))
                    v2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(512))
                    T.reads(lv613[v0, v1, v2], decoder_model_layers_4_block_layers_1_bias1[v1])
                    T.writes(T_add_intermediate[v0, v1, v2])
                    T_add_intermediate[v0, v1, v2] = lv613[v0, v1, v2] + decoder_model_layers_4_block_layers_1_bias1[v1]

    @T.prim_func(private=True)
    def fused_expand_dims_add1(decoder_model_layers_1_block_layers_1_bias1: T.Buffer((T.int64(768),), "float32"), p_lv415: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv415 = T.match_buffer(p_lv415, (batch_size, T.int64(768), T.int64(8)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(768), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * T.int64(6), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_add"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(6144))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(6144) // T.int64(8))
                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(8))
                    T.reads(lv415[v0, v1, v2], decoder_model_layers_1_block_layers_1_bias1[v1])
                    T.writes(T_add_intermediate[v0, v1, v2])
                    T_add_intermediate[v0, v1, v2] = lv415[v0, v1, v2] + decoder_model_layers_1_block_layers_1_bias1[v1]

    @T.prim_func(private=True)
    def fused_matmul_multiply29_subtract_add32(p_divide: T.handle, permute_dims1: T.Buffer((T.int64(8), T.int64(1024)), "float32"), p_sum2: T.handle, permute_dims2: T.Buffer((T.int64(1), T.int64(1024)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        divide = T.match_buffer(p_divide, (batch_size, T.int64(8)))
        sum2 = T.match_buffer(p_sum2, (batch_size, T.int64(1)))
        T_add_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1024)))
        # with T.block("root"):
        matmul_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(1024)), scope="local")
        divide_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(8)), scope="shared")
        permute_dims1_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(8)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("matmul_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(matmul_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            matmul_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(1)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("divide_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(divide[v1, v2])
                                                        T.writes(divide_reindex_pad_shared[v0, v1, v2])
                                                        divide_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size, divide[v1, v2], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("permute_dims1_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(permute_dims1[v2, v1])
                                                        T.writes(permute_dims1_reindex_shared[v0, v1, v2])
                                                        permute_dims1_reindex_shared[v0, v1, v2] = permute_dims1[v2, v1]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("matmul_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(8), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(matmul_intermediate_reindex_pad_local[T.int64(0), v1, v2], divide_reindex_pad_shared[T.int64(0), v1, v3], permute_dims1_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(matmul_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                matmul_intermediate_reindex_pad_local[T.int64(0), v1, v2] = matmul_intermediate_reindex_pad_local[T.int64(0), v1, v2] + divide_reindex_pad_shared[T.int64(0), v1, v3] * permute_dims1_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("matmul_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1 < batch_size)
                                            T.reads(sum2[v1, T.int64(0)], matmul_intermediate_reindex_pad_local[v0, v1, v2], permute_dims2[T.int64(0), v2])
                                            T.writes(T_add_intermediate[v1, v2])
                                            T_add_intermediate[v1, v2] = sum2[v1, T.int64(0)] - matmul_intermediate_reindex_pad_local[v0, v1, v2] * T.float32(2.0) + permute_dims2[T.int64(0), v2]

    @T.prim_func(private=True)
    def fused_tir_sqrt10_divide26_multiply26(lv280: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_4_block_layers_4_weight_v: T.Buffer((T.int64(1024), T.int64(512), T.int64(16)), "float32"), encoder_block_layers_4_block_layers_4_weight_g: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1024), T.int64(512), T.int64(16)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8192), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(8192))
                    v1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(8192) // T.int64(16))
                    v2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(16))
                    T.reads(encoder_block_layers_4_block_layers_4_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_4_block_layers_4_weight_v[v0, v1, v2], lv280[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_layers_4_block_layers_4_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_4_block_layers_4_weight_v[v0, v1, v2] / T.sqrt(lv280[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt10_divide27_multiply27(lv289: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_6_weight_v: T.Buffer((T.int64(1024), T.int64(1024), T.int64(3)), "float32"), encoder_block_layers_6_weight_g: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1024), T.int64(1024), T.int64(3)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(3072), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(3072))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(3072) // T.int64(3))
                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(3))
                    T.reads(encoder_block_layers_6_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_6_weight_v[v0, v1, v2], lv289[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_layers_6_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_6_weight_v[v0, v1, v2] / T.sqrt(lv289[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt10_divide31_multiply30(lv301: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32"), quantizer_quantizers_0_out_proj_weight_v: T.Buffer((T.int64(1024), T.int64(8), T.int64(1)), "float32"), quantizer_quantizers_0_out_proj_weight_g: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1024), T.int64(8), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(8), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(1024), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.reads(quantizer_quantizers_0_out_proj_weight_g[v0, T.int64(0), T.int64(0)], quantizer_quantizers_0_out_proj_weight_v[v0, v1, T.int64(0)], lv301[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = quantizer_quantizers_0_out_proj_weight_g[v0, T.int64(0), T.int64(0)] * (quantizer_quantizers_0_out_proj_weight_v[v0, v1, T.int64(0)] / T.sqrt(lv301[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt11_divide28_multiply28(lv295: T.Buffer((T.int64(8), T.int64(1), T.int64(1)), "float32"), quantizer_quantizers_0_in_proj_weight_v: T.Buffer((T.int64(8), T.int64(1024), T.int64(1)), "float32"), quantizer_quantizers_0_in_proj_weight_g: T.Buffer((T.int64(8), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(8), T.int64(1024), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(8), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(1024))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(1024))
                    T.reads(quantizer_quantizers_0_in_proj_weight_g[v0, T.int64(0), T.int64(0)], quantizer_quantizers_0_in_proj_weight_v[v0, v1, T.int64(0)], lv295[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = quantizer_quantizers_0_in_proj_weight_g[v0, T.int64(0), T.int64(0)] * (quantizer_quantizers_0_in_proj_weight_v[v0, v1, T.int64(0)] / T.sqrt(lv295[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt1_divide2_multiply2(lv421: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_v1: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32"), decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_g1: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(4032), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(5376))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(5376) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_g1[v0, T.int64(0), T.int64(0)], decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_v1[v0, v1, v2], lv421[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_g1[v0, T.int64(0), T.int64(0)] * (decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_v1[v0, v1, v2] / T.sqrt(lv421[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt1_divide3_multiply3(lv430: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_v1: T.Buffer((T.int64(768), T.int64(768), T.int64(1)), "float32"), decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_g1: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(768), T.int64(768), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(576), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(768), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(768))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(768))
                    T.reads(decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_g1[v0, T.int64(0), T.int64(0)], decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_v1[v0, v1, T.int64(0)], lv430[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_g1[v0, T.int64(0), T.int64(0)] * (decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_v1[v0, v1, T.int64(0)] / T.sqrt(lv430[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt1_divide4_multiply4(lv478: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_2_block_layers_1_weight_v1: T.Buffer((T.int64(768), T.int64(384), T.int64(16)), "float32"), decoder_model_layers_2_block_layers_1_weight_g1: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(768), T.int64(384), T.int64(16)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(4608), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(6144))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(6144) // T.int64(16))
                    v2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(16))
                    T.reads(decoder_model_layers_2_block_layers_1_weight_g1[v0, T.int64(0), T.int64(0)], decoder_model_layers_2_block_layers_1_weight_v1[v0, v1, v2], lv478[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_layers_2_block_layers_1_weight_g1[v0, T.int64(0), T.int64(0)] * (decoder_model_layers_2_block_layers_1_weight_v1[v0, v1, v2] / T.sqrt(lv478[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt2_divide5_multiply5(lv487: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_v1: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32"), decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_g1: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(1008), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(2688))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(2688) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_g1[v0, T.int64(0), T.int64(0)], decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_v1[v0, v1, v2], lv487[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_g1[v0, T.int64(0), T.int64(0)] * (decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_v1[v0, v1, v2] / T.sqrt(lv487[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt2_divide6_multiply6(lv496: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_v1: T.Buffer((T.int64(384), T.int64(384), T.int64(1)), "float32"), decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_g1: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(384), T.int64(384), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(144), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(384), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(384))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(384))
                    T.reads(decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_g1[v0, T.int64(0), T.int64(0)], decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_v1[v0, v1, T.int64(0)], lv496[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_g1[v0, T.int64(0), T.int64(0)] * (decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_v1[v0, v1, T.int64(0)] / T.sqrt(lv496[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt2_divide7_multiply7(lv544: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_3_block_layers_1_weight_v1: T.Buffer((T.int64(384), T.int64(192), T.int64(8)), "float32"), decoder_model_layers_3_block_layers_1_weight_g1: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(384), T.int64(192), T.int64(8)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(576), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(1536))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(1536) // T.int64(8))
                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(8))
                    T.reads(decoder_model_layers_3_block_layers_1_weight_g1[v0, T.int64(0), T.int64(0)], decoder_model_layers_3_block_layers_1_weight_v1[v0, v1, v2], lv544[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_layers_3_block_layers_1_weight_g1[v0, T.int64(0), T.int64(0)] * (decoder_model_layers_3_block_layers_1_weight_v1[v0, v1, v2] / T.sqrt(lv544[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt3_divide10_multiply10(lv610: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_4_block_layers_1_weight_v1: T.Buffer((T.int64(192), T.int64(96), T.int64(4)), "float32"), decoder_model_layers_4_block_layers_1_weight_g1: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(192), T.int64(96), T.int64(4)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(72), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(384))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(384) // T.int64(4))
                    v2 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(4))
                    T.reads(decoder_model_layers_4_block_layers_1_weight_g1[v0, T.int64(0), T.int64(0)], decoder_model_layers_4_block_layers_1_weight_v1[v0, v1, v2], lv610[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_layers_4_block_layers_1_weight_g1[v0, T.int64(0), T.int64(0)] * (decoder_model_layers_4_block_layers_1_weight_v1[v0, v1, v2] / T.sqrt(lv610[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt3_divide8_multiply8(lv553: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_v1: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32"), decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_g1: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(252), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(1344))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(1344) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_g1[v0, T.int64(0), T.int64(0)], decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_v1[v0, v1, v2], lv553[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_g1[v0, T.int64(0), T.int64(0)] * (decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_v1[v0, v1, v2] / T.sqrt(lv553[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt3_divide9_multiply9(lv562: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_v1: T.Buffer((T.int64(192), T.int64(192), T.int64(1)), "float32"), decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_g1: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(192), T.int64(192), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(36), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(192), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(192))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(192))
                    T.reads(decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_g1[v0, T.int64(0), T.int64(0)], decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_v1[v0, v1, T.int64(0)], lv562[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_g1[v0, T.int64(0), T.int64(0)] * (decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_v1[v0, v1, T.int64(0)] / T.sqrt(lv562[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt4_divide11_multiply11(lv619: T.Buffer((T.int64(96), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_v1: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32"), decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_g1: T.Buffer((T.int64(96), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(63), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(672))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(672) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_g1[v0, T.int64(0), T.int64(0)], decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_v1[v0, v1, v2], lv619[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_g1[v0, T.int64(0), T.int64(0)] * (decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_v1[v0, v1, v2] / T.sqrt(lv619[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt4_divide12_multiply12(lv628: T.Buffer((T.int64(96), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_v1: T.Buffer((T.int64(96), T.int64(96), T.int64(1)), "float32"), decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_g1: T.Buffer((T.int64(96), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(96), T.int64(96), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(9), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(96), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(96))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(96))
                    T.reads(decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_g1[v0, T.int64(0), T.int64(0)], decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_v1[v0, v1, T.int64(0)], lv628[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_g1[v0, T.int64(0), T.int64(0)] * (decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_v1[v0, v1, T.int64(0)] / T.sqrt(lv628[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt5_divide13_multiply13(lv678: T.Buffer((T.int64(1), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_6_weight_v1: T.Buffer((T.int64(1), T.int64(96), T.int64(7)), "float32"), decoder_model_layers_6_weight_g1: T.Buffer((T.int64(1), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1), T.int64(96), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(96), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(7))
                    v1 = T.axis.spatial(T.int64(7), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(7))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(672))
                    T.reads(decoder_model_layers_6_weight_g1[T.int64(0), T.int64(0), T.int64(0)], decoder_model_layers_6_weight_v1[T.int64(0), v0, v1], lv678[T.int64(0), T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[T.int64(0), v0, v1])
                    T_multiply_intermediate[T.int64(0), v0, v1] = decoder_model_layers_6_weight_g1[T.int64(0), T.int64(0), T.int64(0)] * (decoder_model_layers_6_weight_v1[T.int64(0), v0, v1] / T.sqrt(lv678[T.int64(0), T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt6_divide14_multiply14(lv4: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_0_weight_v: T.Buffer((T.int64(64), T.int64(1), T.int64(7)), "float32"), encoder_block_layers_0_weight_g: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(64), T.int64(1), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(64), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(7))
                    v1 = T.axis.spatial(T.int64(7), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(7))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < T.int64(448))
                    T.reads(encoder_block_layers_0_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_0_weight_v[v0, T.int64(0), v1], lv4[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, T.int64(0), v1])
                    T_multiply_intermediate[v0, T.int64(0), v1] = encoder_block_layers_0_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_0_weight_v[v0, T.int64(0), v1] / T.sqrt(lv4[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt6_divide15_multiply15(lv14: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_v: T.Buffer((T.int64(64), T.int64(64), T.int64(7)), "float32"), encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_g: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(64), T.int64(64), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(28), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(448))
                    v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(448) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_v[v0, v1, v2], lv14[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_v[v0, v1, v2] / T.sqrt(lv14[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt6_divide16_multiply16(lv23: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_v: T.Buffer((T.int64(64), T.int64(64), T.int64(1)), "float32"), encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_g: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(64), T.int64(64), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(4), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(64), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(64))
                    v1 = T.axis.spatial(T.int64(64), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(64))
                    T.reads(encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_v[v0, v1, T.int64(0)], lv23[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_v[v0, v1, T.int64(0)] / T.sqrt(lv23[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt7_divide17_multiply17(lv73: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_1_block_layers_4_weight_v: T.Buffer((T.int64(128), T.int64(64), T.int64(4)), "float32"), encoder_block_layers_1_block_layers_4_weight_g: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(128), T.int64(64), T.int64(4)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(32), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(256))
                    v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(256) // T.int64(4))
                    v2 = T.axis.spatial(T.int64(4), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(4))
                    T.reads(encoder_block_layers_1_block_layers_4_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_1_block_layers_4_weight_v[v0, v1, v2], lv73[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_layers_1_block_layers_4_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_1_block_layers_4_weight_v[v0, v1, v2] / T.sqrt(lv73[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt7_divide18_multiply18(lv83: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_v: T.Buffer((T.int64(128), T.int64(128), T.int64(7)), "float32"), encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_g: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(128), T.int64(128), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(112), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(896))
                    v1 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(896) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_v[v0, v1, v2], lv83[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_v[v0, v1, v2] / T.sqrt(lv83[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt7_divide19_multiply19(lv92: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_v: T.Buffer((T.int64(128), T.int64(128), T.int64(1)), "float32"), encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_g: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(128), T.int64(128), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(16), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(128), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(128))
                    v1 = T.axis.spatial(T.int64(128), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(128))
                    T.reads(encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_v[v0, v1, T.int64(0)], lv92[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_v[v0, v1, T.int64(0)] / T.sqrt(lv92[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt8_divide20_multiply20(lv142: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_2_block_layers_4_weight_v: T.Buffer((T.int64(256), T.int64(128), T.int64(8)), "float32"), encoder_block_layers_2_block_layers_4_weight_g: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(256), T.int64(128), T.int64(8)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(1024))
                    v1 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(1024) // T.int64(8))
                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(8))
                    T.reads(encoder_block_layers_2_block_layers_4_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_2_block_layers_4_weight_v[v0, v1, v2], lv142[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_layers_2_block_layers_4_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_2_block_layers_4_weight_v[v0, v1, v2] / T.sqrt(lv142[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt8_divide21_multiply21(lv152: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_v: T.Buffer((T.int64(256), T.int64(256), T.int64(7)), "float32"), encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_g: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(256), T.int64(256), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(448), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(1792))
                    v1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(1792) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_v[v0, v1, v2], lv152[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_v[v0, v1, v2] / T.sqrt(lv152[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt8_divide22_multiply22(lv161: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_v: T.Buffer((T.int64(256), T.int64(256), T.int64(1)), "float32"), encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_g: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(256), T.int64(256), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(256), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(256))
                    v1 = T.axis.spatial(T.int64(256), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(256))
                    T.reads(encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_v[v0, v1, T.int64(0)], lv161[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_v[v0, v1, T.int64(0)] / T.sqrt(lv161[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt9_divide23_multiply23(lv211: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_3_block_layers_4_weight_v: T.Buffer((T.int64(512), T.int64(256), T.int64(16)), "float32"), encoder_block_layers_3_block_layers_4_weight_g: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(512), T.int64(256), T.int64(16)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(2048), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(4096))
                    v1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(4096) // T.int64(16))
                    v2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(16))
                    T.reads(encoder_block_layers_3_block_layers_4_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_3_block_layers_4_weight_v[v0, v1, v2], lv211[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_layers_3_block_layers_4_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_3_block_layers_4_weight_v[v0, v1, v2] / T.sqrt(lv211[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt9_divide24_multiply24(lv221: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_v: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), "float32"), encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_g: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(1792), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(3584))
                    v1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(3584) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_v[v0, v1, v2], lv221[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_v[v0, v1, v2] / T.sqrt(lv221[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt9_divide25_multiply25(lv230: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32"), encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_v: T.Buffer((T.int64(512), T.int64(512), T.int64(1)), "float32"), encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_g: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(512), T.int64(512), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(512), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(512))
                    v1 = T.axis.spatial(T.int64(512), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(512))
                    T.reads(encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_g[v0, T.int64(0), T.int64(0)], encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_v[v0, v1, T.int64(0)], lv230[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, T.int64(0)])
                    T_multiply_intermediate[v0, v1, T.int64(0)] = encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_g[v0, T.int64(0), T.int64(0)] * (encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_v[v0, v1, T.int64(0)] / T.sqrt(lv230[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt_divide1_multiply1(lv412: T.Buffer((T.int64(1536), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_1_block_layers_1_weight_v1: T.Buffer((T.int64(1536), T.int64(768), T.int64(16)), "float32"), decoder_model_layers_1_block_layers_1_weight_g1: T.Buffer((T.int64(1536), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1536), T.int64(768), T.int64(16)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(18432), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(1536), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(12288))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(12288) // T.int64(16))
                    v2 = T.axis.spatial(T.int64(16), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(16))
                    T.reads(decoder_model_layers_1_block_layers_1_weight_g1[v0, T.int64(0), T.int64(0)], decoder_model_layers_1_block_layers_1_weight_v1[v0, v1, v2], lv412[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_layers_1_block_layers_1_weight_g1[v0, T.int64(0), T.int64(0)] * (decoder_model_layers_1_block_layers_1_weight_v1[v0, v1, v2] / T.sqrt(lv412[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_sqrt_divide_multiply(lv405: T.Buffer((T.int64(1536), T.int64(1), T.int64(1)), "float32"), decoder_model_layers_0_weight_v1: T.Buffer((T.int64(1536), T.int64(1024), T.int64(7)), "float32"), decoder_model_layers_0_weight_g1: T.Buffer((T.int64(1536), T.int64(1), T.int64(1)), "float32"), T_multiply_intermediate: T.Buffer((T.int64(1536), T.int64(1024), T.int64(7)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(10752), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_multiply"):
                    v0 = T.axis.spatial(T.int64(1536), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(7168))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7168) // T.int64(7))
                    v2 = T.axis.spatial(T.int64(7), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(7))
                    T.reads(decoder_model_layers_0_weight_g1[v0, T.int64(0), T.int64(0)], decoder_model_layers_0_weight_v1[v0, v1, v2], lv405[v0, T.int64(0), T.int64(0)])
                    T.writes(T_multiply_intermediate[v0, v1, v2])
                    T_multiply_intermediate[v0, v1, v2] = decoder_model_layers_0_weight_g1[v0, T.int64(0), T.int64(0)] * (decoder_model_layers_0_weight_v1[v0, v1, v2] / T.sqrt(lv405[v0, T.int64(0), T.int64(0)]))

    @T.prim_func(private=True)
    def fused_tir_square10_sum10(decoder_model_layers_4_block_layers_1_weight_v1: T.Buffer((T.int64(192), T.int64(96), T.int64(4)), "float32"), lv609_red_intermediate: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv609_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(192), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(192), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv609_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv609_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv609_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv609_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(384))
                        T.reads(lv609_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_layers_4_block_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(4), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(4)])
                        T.writes(lv609_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv609_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv609_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_layers_4_block_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(4), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(4)] * decoder_model_layers_4_block_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(4), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(4)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv609_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv609_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv609_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv609_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv609_red_intermediate[v0, T.int64(0), T.int64(0)] = lv609_red_intermediate[v0, T.int64(0), T.int64(0)] + lv609_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square11_sum11(decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_v1: T.Buffer((T.int64(96), T.int64(96), T.int64(7)), "float32"), lv618_red_intermediate: T.Buffer((T.int64(96), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv618_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(96), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(96), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv618_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv618_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv618_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(3), 1):
                    with T.block("lv618_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(672))
                        T.reads(lv618_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv618_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv618_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv618_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv618_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv618_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv618_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv618_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv618_red_intermediate[v0, T.int64(0), T.int64(0)] = lv618_red_intermediate[v0, T.int64(0), T.int64(0)] + lv618_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square12_sum12(decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_v1: T.Buffer((T.int64(96), T.int64(96), T.int64(1)), "float32"), lv627_red_intermediate: T.Buffer((T.int64(96), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv627_red_intermediate_rf_local = T.alloc_buffer((T.int64(64), T.int64(96), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(96), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv627_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv627_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv627_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv627_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.where(ax1_fused_0 * T.int64(64) + ax1_fused_1 < T.int64(96))
                        T.reads(lv627_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_v1[v0, vax1_fused_0 * T.int64(64) + vax1_fused_1, T.int64(0)])
                        T.writes(lv627_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv627_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv627_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_v1[v0, vax1_fused_0 * T.int64(64) + vax1_fused_1, T.int64(0)] * decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_v1[v0, vax1_fused_0 * T.int64(64) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("lv627_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv627_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv627_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv627_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv627_red_intermediate[v0, T.int64(0), T.int64(0)] = lv627_red_intermediate[v0, T.int64(0), T.int64(0)] + lv627_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square13_sum13(decoder_model_layers_6_weight_v1: T.Buffer((T.int64(1), T.int64(96), T.int64(7)), "float32"), lv677_red_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv677_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(1), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv677_red_rf_init"):
                    vax1_ax2_fused_1 = T.axis.spatial(T.int64(256), ax1_ax2_fused_1)
                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                    T.reads()
                    T.writes(lv677_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)])
                    lv677_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(3), 1):
                    with T.block("lv677_red_rf_update"):
                        vax1_ax2_fused_1 = T.axis.spatial(T.int64(256), ax1_ax2_fused_1)
                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                        vax1_ax2_fused_0 = T.axis.reduce(T.int64(3), ax1_ax2_fused_0)
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(672))
                        T.reads(lv677_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)], decoder_model_layers_6_weight_v1[T.int64(0), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv677_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)])
                        lv677_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)] = lv677_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)] + decoder_model_layers_6_weight_v1[T.int64(0), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * decoder_model_layers_6_weight_v1[T.int64(0), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv677_red"):
                        vax1_ax2_fused_1 = T.axis.reduce(T.int64(256), ax0)
                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                        T.reads(lv677_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)])
                        T.writes(lv677_red_intermediate[T.int64(0), T.int64(0), T.int64(0)])
                        with T.init():
                            lv677_red_intermediate[T.int64(0), T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv677_red_intermediate[T.int64(0), T.int64(0), T.int64(0)] = lv677_red_intermediate[T.int64(0), T.int64(0), T.int64(0)] + lv677_red_intermediate_rf_local[vax1_ax2_fused_1, T.int64(0), T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square14_sum14(encoder_block_layers_0_weight_v: T.Buffer((T.int64(64), T.int64(1), T.int64(7)), "float32"), lv3_red_intermediate: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv3_red_intermediate_rf_local = T.alloc_buffer((T.int64(4), T.int64(64), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(4), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv3_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv3_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv3_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv3_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.where(ax1_fused_0 * T.int64(4) + ax1_fused_1 < T.int64(7))
                        T.reads(lv3_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_0_weight_v[v0, T.int64(0), vax1_fused_0 * T.int64(4) + vax1_fused_1])
                        T.writes(lv3_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv3_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv3_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_0_weight_v[v0, T.int64(0), vax1_fused_0 * T.int64(4) + vax1_fused_1] * encoder_block_layers_0_weight_v[v0, T.int64(0), vax1_fused_0 * T.int64(4) + vax1_fused_1]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(4), thread="threadIdx.x"):
                    with T.block("lv3_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv3_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv3_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv3_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv3_red_intermediate[v0, T.int64(0), T.int64(0)] = lv3_red_intermediate[v0, T.int64(0), T.int64(0)] + lv3_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square15_sum15(encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_v: T.Buffer((T.int64(64), T.int64(64), T.int64(7)), "float32"), lv13_red_intermediate: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv13_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(64), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv13_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv13_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv13_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv13_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(448))
                        T.reads(lv13_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv13_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv13_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv13_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv13_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv13_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv13_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv13_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv13_red_intermediate[v0, T.int64(0), T.int64(0)] = lv13_red_intermediate[v0, T.int64(0), T.int64(0)] + lv13_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square16_sum16(encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_v: T.Buffer((T.int64(64), T.int64(64), T.int64(1)), "float32"), lv22_red_intermediate: T.Buffer((T.int64(64), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv22_red_intermediate_rf_local = T.alloc_buffer((T.int64(64), T.int64(64), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv22_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv22_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv22_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("lv22_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv22_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_v[v0, vax1_fused_0 * T.int64(64) + vax1_fused_1, T.int64(0)])
                        T.writes(lv22_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv22_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv22_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_v[v0, vax1_fused_0 * T.int64(64) + vax1_fused_1, T.int64(0)] * encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_v[v0, vax1_fused_0 * T.int64(64) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    with T.block("lv22_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv22_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv22_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv22_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv22_red_intermediate[v0, T.int64(0), T.int64(0)] = lv22_red_intermediate[v0, T.int64(0), T.int64(0)] + lv22_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square17_sum17(encoder_block_layers_1_block_layers_4_weight_v: T.Buffer((T.int64(128), T.int64(64), T.int64(4)), "float32"), lv72_red_intermediate: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv72_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(128), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv72_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv72_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv72_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("lv72_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv72_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_1_block_layers_4_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(4), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(4)])
                        T.writes(lv72_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv72_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv72_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_1_block_layers_4_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(4), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(4)] * encoder_block_layers_1_block_layers_4_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(4), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(4)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv72_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv72_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv72_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv72_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv72_red_intermediate[v0, T.int64(0), T.int64(0)] = lv72_red_intermediate[v0, T.int64(0), T.int64(0)] + lv72_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square18_sum18(encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_v: T.Buffer((T.int64(128), T.int64(128), T.int64(7)), "float32"), lv82_red_intermediate: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv82_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(128), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv82_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv82_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv82_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(4), 1):
                    with T.block("lv82_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(896))
                        T.reads(lv82_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv82_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv82_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv82_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv82_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv82_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv82_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv82_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv82_red_intermediate[v0, T.int64(0), T.int64(0)] = lv82_red_intermediate[v0, T.int64(0), T.int64(0)] + lv82_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square19_sum19(encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_v: T.Buffer((T.int64(128), T.int64(128), T.int64(1)), "float32"), lv91_red_intermediate: T.Buffer((T.int64(128), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv91_red_intermediate_rf_local = T.alloc_buffer((T.int64(128), T.int64(128), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv91_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv91_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv91_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("lv91_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv91_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_v[v0, vax1_fused_0 * T.int64(128) + vax1_fused_1, T.int64(0)])
                        T.writes(lv91_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv91_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv91_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_v[v0, vax1_fused_0 * T.int64(128) + vax1_fused_1, T.int64(0)] * encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_v[v0, vax1_fused_0 * T.int64(128) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("lv91_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv91_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv91_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv91_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv91_red_intermediate[v0, T.int64(0), T.int64(0)] = lv91_red_intermediate[v0, T.int64(0), T.int64(0)] + lv91_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square1_sum1(decoder_model_layers_1_block_layers_1_weight_v1: T.Buffer((T.int64(1536), T.int64(768), T.int64(16)), "float32"), lv411_red_intermediate: T.Buffer((T.int64(1536), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv411_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(1536), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1536), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv411_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv411_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv411_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(48), 1):
                    with T.block("lv411_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv411_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_layers_1_block_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)])
                        T.writes(lv411_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv411_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv411_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_layers_1_block_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)] * decoder_model_layers_1_block_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv411_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv411_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv411_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv411_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv411_red_intermediate[v0, T.int64(0), T.int64(0)] = lv411_red_intermediate[v0, T.int64(0), T.int64(0)] + lv411_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square20_sum20(encoder_block_layers_2_block_layers_4_weight_v: T.Buffer((T.int64(256), T.int64(128), T.int64(8)), "float32"), lv141_red_intermediate: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv141_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv141_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv141_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv141_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(4), 1):
                    with T.block("lv141_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv141_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_2_block_layers_4_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(8), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(8)])
                        T.writes(lv141_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv141_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv141_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_2_block_layers_4_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(8), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(8)] * encoder_block_layers_2_block_layers_4_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(8), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(8)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv141_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv141_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv141_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv141_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv141_red_intermediate[v0, T.int64(0), T.int64(0)] = lv141_red_intermediate[v0, T.int64(0), T.int64(0)] + lv141_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square21_sum21(encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_v: T.Buffer((T.int64(256), T.int64(256), T.int64(7)), "float32"), lv151_red_intermediate: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv151_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv151_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv151_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv151_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(7), 1):
                    with T.block("lv151_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv151_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv151_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv151_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv151_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv151_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv151_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv151_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv151_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv151_red_intermediate[v0, T.int64(0), T.int64(0)] = lv151_red_intermediate[v0, T.int64(0), T.int64(0)] + lv151_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square22_sum22(encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_v: T.Buffer((T.int64(256), T.int64(256), T.int64(1)), "float32"), lv160_red_intermediate: T.Buffer((T.int64(256), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv160_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(256), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(256), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv160_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv160_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv160_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("lv160_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv160_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_v[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)])
                        T.writes(lv160_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv160_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv160_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_v[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)] * encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_v[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv160_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv160_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv160_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv160_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv160_red_intermediate[v0, T.int64(0), T.int64(0)] = lv160_red_intermediate[v0, T.int64(0), T.int64(0)] + lv160_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square23_sum23(encoder_block_layers_3_block_layers_4_weight_v: T.Buffer((T.int64(512), T.int64(256), T.int64(16)), "float32"), lv210_red_intermediate: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv210_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(512), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv210_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv210_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv210_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(16), 1):
                    with T.block("lv210_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv210_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_3_block_layers_4_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)])
                        T.writes(lv210_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv210_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv210_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_3_block_layers_4_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)] * encoder_block_layers_3_block_layers_4_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv210_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv210_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv210_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv210_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv210_red_intermediate[v0, T.int64(0), T.int64(0)] = lv210_red_intermediate[v0, T.int64(0), T.int64(0)] + lv210_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square24_sum24(encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_v: T.Buffer((T.int64(512), T.int64(512), T.int64(7)), "float32"), lv220_red_intermediate: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv220_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(512), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv220_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv220_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv220_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(14), 1):
                    with T.block("lv220_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv220_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv220_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv220_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv220_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv220_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv220_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv220_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv220_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv220_red_intermediate[v0, T.int64(0), T.int64(0)] = lv220_red_intermediate[v0, T.int64(0), T.int64(0)] + lv220_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square25_sum25(encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_v: T.Buffer((T.int64(512), T.int64(512), T.int64(1)), "float32"), lv229_red_intermediate: T.Buffer((T.int64(512), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv229_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(512), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(512), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv229_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv229_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv229_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv229_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv229_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_v[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)])
                        T.writes(lv229_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv229_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv229_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_v[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)] * encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_v[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv229_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv229_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv229_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv229_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv229_red_intermediate[v0, T.int64(0), T.int64(0)] = lv229_red_intermediate[v0, T.int64(0), T.int64(0)] + lv229_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square26_sum26(encoder_block_layers_4_block_layers_4_weight_v: T.Buffer((T.int64(1024), T.int64(512), T.int64(16)), "float32"), lv279_red_intermediate: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv279_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv279_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv279_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv279_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(32), 1):
                    with T.block("lv279_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv279_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_4_block_layers_4_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)])
                        T.writes(lv279_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv279_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv279_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_4_block_layers_4_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)] * encoder_block_layers_4_block_layers_4_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv279_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv279_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv279_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv279_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv279_red_intermediate[v0, T.int64(0), T.int64(0)] = lv279_red_intermediate[v0, T.int64(0), T.int64(0)] + lv279_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square27_sum27(encoder_block_layers_6_weight_v: T.Buffer((T.int64(1024), T.int64(1024), T.int64(3)), "float32"), lv288_red_intermediate: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv288_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(1024), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv288_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv288_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv288_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(12), 1):
                    with T.block("lv288_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv288_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], encoder_block_layers_6_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(3), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(3)])
                        T.writes(lv288_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv288_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv288_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + encoder_block_layers_6_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(3), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(3)] * encoder_block_layers_6_weight_v[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(3), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(3)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv288_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv288_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv288_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv288_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv288_red_intermediate[v0, T.int64(0), T.int64(0)] = lv288_red_intermediate[v0, T.int64(0), T.int64(0)] + lv288_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square28_sum28(quantizer_quantizers_0_in_proj_weight_v: T.Buffer((T.int64(8), T.int64(1024), T.int64(1)), "float32"), lv294_red_intermediate: T.Buffer((T.int64(8), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv294_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(8), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(8), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv294_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv294_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv294_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(4), 1):
                    with T.block("lv294_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv294_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], quantizer_quantizers_0_in_proj_weight_v[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)])
                        T.writes(lv294_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv294_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv294_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + quantizer_quantizers_0_in_proj_weight_v[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)] * quantizer_quantizers_0_in_proj_weight_v[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv294_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv294_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv294_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv294_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv294_red_intermediate[v0, T.int64(0), T.int64(0)] = lv294_red_intermediate[v0, T.int64(0), T.int64(0)] + lv294_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square29_sum29(p_reshape58: T.handle, p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape58 = T.match_buffer(p_reshape58, (batch_size, T.int64(8)))
        square_red_intermediate = T.match_buffer(p_output0, (batch_size, T.int64(1)))
        # with T.block("root"):
        square_red_intermediate_rf_local = T.alloc_buffer((T.int64(8), batch_size, T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(batch_size, thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("square_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(square_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                    square_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("square_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(square_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)], reshape58[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1])
                        T.writes(square_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        square_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = square_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] + reshape58[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1] * reshape58[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("square_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(square_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        T.writes(square_red_intermediate[v0, T.int64(0)])
                        with T.init():
                            square_red_intermediate[v0, T.int64(0)] = T.float32(0.0)
                        square_red_intermediate[v0, T.int64(0)] = square_red_intermediate[v0, T.int64(0)] + square_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square2_sum2(decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_v1: T.Buffer((T.int64(768), T.int64(768), T.int64(7)), "float32"), lv420_red_intermediate: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv420_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(768), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(768), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv420_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv420_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv420_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(21), 1):
                    with T.block("lv420_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv420_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv420_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv420_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv420_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv420_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv420_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv420_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv420_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv420_red_intermediate[v0, T.int64(0), T.int64(0)] = lv420_red_intermediate[v0, T.int64(0), T.int64(0)] + lv420_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square30_sum30(quantizer_quantizers_0_codebook_weight: T.Buffer((T.int64(1024), T.int64(8)), "float32"), square1_red_intermediate: T.Buffer((T.int64(1024), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        square1_red_intermediate_rf_local = T.alloc_buffer((T.int64(8), T.int64(1024), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("square1_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(square1_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                    square1_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("square1_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(square1_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)], quantizer_quantizers_0_codebook_weight[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1])
                        T.writes(square1_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        square1_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] = square1_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)] + quantizer_quantizers_0_codebook_weight[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1] * quantizer_quantizers_0_codebook_weight[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("square1_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(square1_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)])
                        T.writes(square1_red_intermediate[v0, T.int64(0)])
                        with T.init():
                            square1_red_intermediate[v0, T.int64(0)] = T.float32(0.0)
                        square1_red_intermediate[v0, T.int64(0)] = square1_red_intermediate[v0, T.int64(0)] + square1_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square31_sum31(quantizer_quantizers_0_out_proj_weight_v: T.Buffer((T.int64(1024), T.int64(8), T.int64(1)), "float32"), lv300_red_intermediate: T.Buffer((T.int64(1024), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv300_red_intermediate_rf_local = T.alloc_buffer((T.int64(8), T.int64(1024), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1024), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv300_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv300_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv300_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(1), 1):
                    with T.block("lv300_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv300_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], quantizer_quantizers_0_out_proj_weight_v[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1, T.int64(0)])
                        T.writes(lv300_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv300_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv300_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + quantizer_quantizers_0_out_proj_weight_v[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1, T.int64(0)] * quantizer_quantizers_0_out_proj_weight_v[v0, vax1_fused_0 * T.int64(8) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                    with T.block("lv300_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv300_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv300_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv300_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv300_red_intermediate[v0, T.int64(0), T.int64(0)] = lv300_red_intermediate[v0, T.int64(0), T.int64(0)] + lv300_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square3_sum3(decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_v1: T.Buffer((T.int64(768), T.int64(768), T.int64(1)), "float32"), lv429_red_intermediate: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv429_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(768), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(768), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv429_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv429_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv429_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(3), 1):
                    with T.block("lv429_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.reads(lv429_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)])
                        T.writes(lv429_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv429_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv429_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)] * decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv429_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv429_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv429_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv429_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv429_red_intermediate[v0, T.int64(0), T.int64(0)] = lv429_red_intermediate[v0, T.int64(0), T.int64(0)] + lv429_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square4_sum4(decoder_model_layers_2_block_layers_1_weight_v1: T.Buffer((T.int64(768), T.int64(384), T.int64(16)), "float32"), lv477_red_intermediate: T.Buffer((T.int64(768), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv477_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(768), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(768), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv477_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv477_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv477_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(24), 1):
                    with T.block("lv477_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv477_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_layers_2_block_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)])
                        T.writes(lv477_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv477_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv477_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_layers_2_block_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)] * decoder_model_layers_2_block_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(16), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(16)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv477_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv477_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv477_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv477_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv477_red_intermediate[v0, T.int64(0), T.int64(0)] = lv477_red_intermediate[v0, T.int64(0), T.int64(0)] + lv477_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square5_sum5(decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_v1: T.Buffer((T.int64(384), T.int64(384), T.int64(7)), "float32"), lv486_red_intermediate: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv486_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(384), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(384), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv486_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv486_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv486_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(11), 1):
                    with T.block("lv486_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(2688))
                        T.reads(lv486_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv486_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv486_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv486_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv486_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv486_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv486_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv486_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv486_red_intermediate[v0, T.int64(0), T.int64(0)] = lv486_red_intermediate[v0, T.int64(0), T.int64(0)] + lv486_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square6_sum6(decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_v1: T.Buffer((T.int64(384), T.int64(384), T.int64(1)), "float32"), lv495_red_intermediate: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv495_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(384), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(384), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv495_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv495_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv495_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv495_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.where(ax1_fused_0 * T.int64(256) + ax1_fused_1 < T.int64(384))
                        T.reads(lv495_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)])
                        T.writes(lv495_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv495_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv495_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)] * decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_v1[v0, vax1_fused_0 * T.int64(256) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv495_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv495_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv495_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv495_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv495_red_intermediate[v0, T.int64(0), T.int64(0)] = lv495_red_intermediate[v0, T.int64(0), T.int64(0)] + lv495_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square7_sum7(decoder_model_layers_3_block_layers_1_weight_v1: T.Buffer((T.int64(384), T.int64(192), T.int64(8)), "float32"), lv543_red_intermediate: T.Buffer((T.int64(384), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv543_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(384), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(384), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv543_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv543_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv543_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(6), 1):
                    with T.block("lv543_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv543_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_layers_3_block_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(8), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(8)])
                        T.writes(lv543_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv543_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv543_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_layers_3_block_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(8), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(8)] * decoder_model_layers_3_block_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(8), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(8)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv543_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv543_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv543_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv543_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv543_red_intermediate[v0, T.int64(0), T.int64(0)] = lv543_red_intermediate[v0, T.int64(0), T.int64(0)] + lv543_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square8_sum8(decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_v1: T.Buffer((T.int64(192), T.int64(192), T.int64(7)), "float32"), lv552_red_intermediate: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv552_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(192), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(192), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv552_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv552_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv552_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(6), 1):
                    with T.block("lv552_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.where(ax1_ax2_fused_0 * T.int64(256) + ax1_ax2_fused_1 < T.int64(1344))
                        T.reads(lv552_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv552_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv552_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv552_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv552_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv552_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv552_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv552_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv552_red_intermediate[v0, T.int64(0), T.int64(0)] = lv552_red_intermediate[v0, T.int64(0), T.int64(0)] + lv552_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square9_sum9(decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_v1: T.Buffer((T.int64(192), T.int64(192), T.int64(1)), "float32"), lv561_red_intermediate: T.Buffer((T.int64(192), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv561_red_intermediate_rf_local = T.alloc_buffer((T.int64(128), T.int64(192), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(192), thread="blockIdx.x"):
            for ax1_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv561_red_rf_init"):
                    vax1_fused_1, v0 = T.axis.remap("SS", [ax1_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv561_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                    lv561_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_fused_0, u in T.grid(T.int64(2), 1):
                    with T.block("lv561_red_rf_update"):
                        vax1_fused_1, v0, vax1_fused_0 = T.axis.remap("SSR", [ax1_fused_1, ax0_fused, ax1_fused_0])
                        T.where(ax1_fused_0 * T.int64(128) + ax1_fused_1 < T.int64(192))
                        T.reads(lv561_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_v1[v0, vax1_fused_0 * T.int64(128) + vax1_fused_1, T.int64(0)])
                        T.writes(lv561_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        lv561_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] = lv561_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_v1[v0, vax1_fused_0 * T.int64(128) + vax1_fused_1, T.int64(0)] * decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_v1[v0, vax1_fused_0 * T.int64(128) + vax1_fused_1, T.int64(0)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    with T.block("lv561_red"):
                        vax1_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv561_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv561_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv561_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv561_red_intermediate[v0, T.int64(0), T.int64(0)] = lv561_red_intermediate[v0, T.int64(0), T.int64(0)] + lv561_red_intermediate_rf_local[vax1_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_tir_square_sum(decoder_model_layers_0_weight_v1: T.Buffer((T.int64(1536), T.int64(1024), T.int64(7)), "float32"), lv404_red_intermediate: T.Buffer((T.int64(1536), T.int64(1), T.int64(1)), "float32")):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        lv404_red_intermediate_rf_local = T.alloc_buffer((T.int64(256), T.int64(1536), T.int64(1), T.int64(1)), scope="local")
        for ax0_fused in T.thread_binding(T.int64(1536), thread="blockIdx.x"):
            for ax1_ax2_fused_1 in T.thread_binding(T.int64(256), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                with T.block("lv404_red_rf_init"):
                    vax1_ax2_fused_1, v0 = T.axis.remap("SS", [ax1_ax2_fused_1, ax0_fused])
                    T.reads()
                    T.writes(lv404_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                    lv404_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                for ax1_ax2_fused_0, u in T.grid(T.int64(28), 1):
                    with T.block("lv404_red_rf_update"):
                        vax1_ax2_fused_1, v0, vax1_ax2_fused_0 = T.axis.remap("SSR", [ax1_ax2_fused_1, ax0_fused, ax1_ax2_fused_0])
                        T.reads(lv404_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)], decoder_model_layers_0_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)])
                        T.writes(lv404_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        lv404_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] = lv404_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)] + decoder_model_layers_0_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)] * decoder_model_layers_0_weight_v1[v0, (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) // T.int64(7), (vax1_ax2_fused_0 * T.int64(256) + vax1_ax2_fused_1) % T.int64(7)]
            for ax1_fused in range(T.int64(1)):
                for ax0 in T.thread_binding(T.int64(256), thread="threadIdx.x"):
                    with T.block("lv404_red"):
                        vax1_ax2_fused_1, v0 = T.axis.remap("RS", [ax0, ax0_fused])
                        T.reads(lv404_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)])
                        T.writes(lv404_red_intermediate[v0, T.int64(0), T.int64(0)])
                        with T.init():
                            lv404_red_intermediate[v0, T.int64(0), T.int64(0)] = T.float32(0.0)
                        lv404_red_intermediate[v0, T.int64(0), T.int64(0)] = lv404_red_intermediate[v0, T.int64(0), T.int64(0)] + lv404_red_intermediate_rf_local[vax1_ax2_fused_1, v0, T.int64(0), T.int64(0)]

    @T.prim_func(private=True)
    def fused_zeros_add33_add33_add33_add33_add33_add33_add33_add33_conv1d41_add30_add33(p_conv1d31: T.handle, p_conv1d33: T.handle, p_conv1d35: T.handle, p_conv1d37: T.handle, p_conv1d39: T.handle, p_conv1d41: T.handle, p_conv1d43: T.handle, p_conv1d45: T.handle, p_permute_dims35: T.handle, wnconv1d47: T.Buffer((T.int64(1024), T.int64(8), T.int64(1)), "float32"), lv401: T.Buffer((T.int64(1), T.int64(1024), T.int64(1)), "float32"), p_output0: T.handle):
        T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d31 = T.match_buffer(p_conv1d31, (batch_size, T.int64(1024), T.int64(1)))
        conv1d33 = T.match_buffer(p_conv1d33, (batch_size, T.int64(1024), T.int64(1)))
        conv1d35 = T.match_buffer(p_conv1d35, (batch_size, T.int64(1024), T.int64(1)))
        conv1d37 = T.match_buffer(p_conv1d37, (batch_size, T.int64(1024), T.int64(1)))
        conv1d39 = T.match_buffer(p_conv1d39, (batch_size, T.int64(1024), T.int64(1)))
        conv1d41 = T.match_buffer(p_conv1d41, (batch_size, T.int64(1024), T.int64(1)))
        conv1d43 = T.match_buffer(p_conv1d43, (batch_size, T.int64(1024), T.int64(1)))
        conv1d45 = T.match_buffer(p_conv1d45, (batch_size, T.int64(1024), T.int64(1)))
        permute_dims35 = T.match_buffer(p_permute_dims35, (batch_size, T.int64(8), T.int64(1)))
        T_add_intermediate_1_2_3_4_5_6_7_8_9 = T.match_buffer(p_output0, (batch_size, T.int64(1024), T.int64(1)))
        # with T.block("root"):
        conv1d_ncw_intermediate_reindex_pad_local = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(1024)), scope="local")
        pad_temp_reindex_pad_shared = T.alloc_buffer((T.int64(1), (batch_size + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(8)), scope="shared")
        wnconv1d47_reindex_shared = T.alloc_buffer((T.int64(1), T.int64(1024), T.int64(8)), scope="shared")
        for ax0_ax2_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.y"):
            for ax1_0 in T.thread_binding((batch_size + T.int64(31)) // T.int64(32), thread="blockIdx.x"):
                for ax2_1 in T.thread_binding(T.int64(1), thread="vthread.y"):
                    for ax1_1 in T.thread_binding(T.int64(1), thread="vthread.x"):
                        for ax2_2 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                            for ax1_2 in T.thread_binding(T.int64(8), thread="threadIdx.x", annotations={"pragma_auto_unroll_max_step": 256, "pragma_unroll_explicit": 1}):
                                for ax1_3_init, ax2_3_0_init in T.grid(T.int64(4), T.int64(4)):
                                    for ax2_3_1_init in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_init"):
                                            v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3_init)
                                            v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0_init + ax2_3_1_init)
                                            T.reads()
                                            T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                            conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = T.float32(0.0)
                                for ax3_0 in range(T.int64(1)):
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("pad_temp_reindex_pad_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(permute_dims35[v1, v2, T.Add(T.int64(0), T.int64(0))])
                                                        T.writes(pad_temp_reindex_pad_shared[v0, v1, v2])
                                                        pad_temp_reindex_pad_shared[v0, v1, v2] = T.if_then_else(v1 < batch_size, permute_dims35[v1, v2, T.Add(T.int64(0), T.int64(0))], T.float32(0.0))
                                    for ax0_ax1_ax2_fused_0 in T.thread_binding(T.int64(8), thread="threadIdx.y"):
                                        for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(8), thread="threadIdx.x"):
                                            for ax0_ax1_ax2_fused_2 in range(T.int64(4)):
                                                for ax0_ax1_ax2_fused_3 in T.vectorized(T.int64(1)):
                                                    with T.block("wnconv1d47_reindex_shared"):
                                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                        v1 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) // T.int64(8))
                                                        v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(32) + ax0_ax1_ax2_fused_1 * T.int64(4) + ax0_ax1_ax2_fused_2 + ax0_ax1_ax2_fused_3) % T.int64(8))
                                                        T.reads(wnconv1d47[v1, v2, T.int64(0)])
                                                        T.writes(wnconv1d47_reindex_shared[v0, v1, v2])
                                                        wnconv1d47_reindex_shared[v0, v1, v2] = wnconv1d47[v1, v2, T.int64(0)]
                                    for ax3_1, ax1_3, ax2_3_0 in T.grid(T.int64(8), T.int64(4), T.int64(4)):
                                        for ax2_3_1 in T.vectorized(T.int64(1)):
                                            with T.block("conv1d_ncw_update"):
                                                v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                                v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_1 * T.int64(32) + ax1_2 * T.int64(4) + ax1_3)
                                                v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_1 * T.int64(32) + ax2_2 * T.int64(4) + ax2_3_0 + ax2_3_1)
                                                v3 = T.axis.reduce(T.int64(8), ax3_0 * T.int64(8) + ax3_1)
                                                T.reads(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2], pad_temp_reindex_pad_shared[T.int64(0), v1, v3], wnconv1d47_reindex_shared[T.int64(0), v2, v3])
                                                T.writes(conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2])
                                                conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] = conv1d_ncw_intermediate_reindex_pad_local[T.int64(0), v1, v2] + pad_temp_reindex_pad_shared[T.int64(0), v1, v3] * wnconv1d47_reindex_shared[T.int64(0), v2, v3]
                                for ax0, ax1, ax2_0 in T.grid(T.int64(1), T.int64(4), T.int64(4)):
                                    for ax2_1_1 in T.vectorized(T.int64(1)):
                                        with T.block("conv1d_ncw_intermediate_reindex_pad_local"):
                                            v0 = T.axis.spatial(T.int64(1), ax0)
                                            v1 = T.axis.spatial((batch_size + T.int64(31)) // T.int64(32) * T.int64(32), ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1)
                                            v2 = T.axis.spatial(T.int64(1024), ax0_ax2_0_fused * T.int64(32) + ax2_2 * T.int64(4) + ax2_0 + ax2_1_1)
                                            T.where(ax1_0 * T.int64(32) + ax1_2 * T.int64(4) + ax1 < batch_size)
                                            T.reads(conv1d31[v1, v2, T.int64(0)], conv1d33[v1, v2, T.int64(0)], conv1d35[v1, v2, T.int64(0)], conv1d37[v1, v2, T.int64(0)], conv1d39[v1, v2, T.int64(0)], conv1d41[v1, v2, T.int64(0)], conv1d43[v1, v2, T.int64(0)], conv1d45[v1, v2, T.int64(0)], conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2], lv401[T.int64(0), v2, T.int64(0)])
                                            T.writes(T_add_intermediate_1_2_3_4_5_6_7_8_9[v1, v2, T.int64(0)])
                                            T_add_intermediate_1_2_3_4_5_6_7_8_9[v1, v2, T.int64(0)] = T.Add(T.float32(0.0), conv1d31[v1, v2, T.int64(0)]) + conv1d33[v1, v2, T.int64(0)] + conv1d35[v1, v2, T.int64(0)] + conv1d37[v1, v2, T.int64(0)] + conv1d39[v1, v2, T.int64(0)] + conv1d41[v1, v2, T.int64(0)] + conv1d43[v1, v2, T.int64(0)] + conv1d45[v1, v2, T.int64(0)] + (conv1d_ncw_intermediate_reindex_pad_local[v0, v1, v2] + lv401[T.int64(0), v2, T.int64(0)])

    @T.prim_func(private=True)
    def reshape(decoder_model_layers_0_bias1: T.Buffer((T.int64(1536),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(1536), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(2), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(1536), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(1536))
                    T.reads(decoder_model_layers_0_bias1[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = decoder_model_layers_0_bias1[v0]

    @T.prim_func(private=True)
    def reshape1(var_conv1d48: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d48 = T.match_buffer(var_conv1d48, (batch_size, T.int64(1536), T.int64(1)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(1536), T.int64(1)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * T.int64(1536) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(1536))
                    v1 = T.axis.spatial(T.int64(1536), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(1536))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * T.int64(1536))
                    T.reads(conv1d48[v0, v1, T.int64(0)])
                    T.writes(T_reshape[v0, v1, T.int64(0)])
                    T_reshape[v0, v1, T.int64(0)] = conv1d48[v0, v1, T.int64(0)]

    @T.prim_func(private=True)
    def reshape11(encoder_block_layers_0_bias: T.Buffer((T.int64(64),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(64), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(64))
                    T.reads(encoder_block_layers_0_bias[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = encoder_block_layers_0_bias[v0]

    @T.prim_func(private=True)
    def reshape12(var_lv9: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv9 = T.match_buffer(var_lv9, (batch_size, T.int64(64), T.int64(512)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(64), T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * T.int64(32), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(32768))
                    v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(32768) // T.int64(512))
                    v2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(512))
                    T.reads(lv9[v0, v1, v2])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = lv9[v0, v1, v2]

    @T.prim_func(private=True)
    def reshape13(encoder_block_layers_1_block_layers_4_bias: T.Buffer((T.int64(128),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(128), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(128))
                    T.reads(encoder_block_layers_1_block_layers_4_bias[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = encoder_block_layers_1_block_layers_4_bias[v0]

    @T.prim_func(private=True)
    def reshape14(var_lv78: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv78 = T.match_buffer(var_lv78, (batch_size, T.int64(128), T.int64(256)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(128), T.int64(256)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * T.int64(32), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(32768))
                    v1 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(32768) // T.int64(256))
                    v2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(256))
                    T.reads(lv78[v0, v1, v2])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = lv78[v0, v1, v2]

    @T.prim_func(private=True)
    def reshape15(encoder_block_layers_2_block_layers_4_bias: T.Buffer((T.int64(256),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(256), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(256))
                    T.reads(encoder_block_layers_2_block_layers_4_bias[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = encoder_block_layers_2_block_layers_4_bias[v0]

    @T.prim_func(private=True)
    def reshape16(var_lv147: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv147 = T.match_buffer(var_lv147, (batch_size, T.int64(256), T.int64(64)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(256), T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * T.int64(16), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(16384))
                    v1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(16384) // T.int64(64))
                    v2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(64))
                    T.reads(lv147[v0, v1, v2])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = lv147[v0, v1, v2]

    @T.prim_func(private=True)
    def reshape17(encoder_block_layers_3_block_layers_4_bias: T.Buffer((T.int64(512),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(512), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(512))
                    T.reads(encoder_block_layers_3_block_layers_4_bias[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = encoder_block_layers_3_block_layers_4_bias[v0]

    @T.prim_func(private=True)
    def reshape18(var_lv216: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv216 = T.match_buffer(var_lv216, (batch_size, T.int64(512), T.int64(8)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(512), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * T.int64(4), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(4096))
                    v1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(4096) // T.int64(8))
                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(8))
                    T.reads(lv216[v0, v1, v2])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = lv216[v0, v1, v2]

    @T.prim_func(private=True)
    def reshape19(encoder_block_layers_4_block_layers_4_bias: T.Buffer((T.int64(1024),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(1024), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(1024), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.reads(encoder_block_layers_4_block_layers_4_bias[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = encoder_block_layers_4_block_layers_4_bias[v0]

    @T.prim_func(private=True)
    def reshape2(var_lv416: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv416 = T.match_buffer(var_lv416, (batch_size, T.int64(768), T.int64(8)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(768), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * T.int64(6), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(6144))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(6144) // T.int64(8))
                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(8))
                    T.reads(lv416[v0, v1, v2])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = lv416[v0, v1, v2]

    @T.prim_func(private=True)
    def reshape20(var_conv1d28: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d28 = T.match_buffer(var_conv1d28, (batch_size, T.int64(1024), T.int64(1)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(1024), T.int64(1)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(batch_size, thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(1024))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(1024))
                    T.reads(conv1d28[v0, v1, T.int64(0)])
                    T.writes(T_reshape[v0, v1, T.int64(0)])
                    T_reshape[v0, v1, T.int64(0)] = conv1d28[v0, v1, T.int64(0)]

    @T.prim_func(private=True)
    def reshape21(quantizer_quantizers_0_in_proj_bias: T.Buffer((T.int64(8),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(8), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(8), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(8))
                    T.reads(quantizer_quantizers_0_in_proj_bias[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = quantizer_quantizers_0_in_proj_bias[v0]

    @T.prim_func(private=True)
    def reshape22(var_permute_dims: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        permute_dims = T.match_buffer(var_permute_dims, (batch_size, T.int64(1), T.int64(8)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * T.int64(8))
                    T.reads(permute_dims[v0, T.int64(0), v1])
                    T.writes(T_reshape[v0, v1])
                    T_reshape[v0, v1] = permute_dims[v0, T.int64(0), v1]

    @T.prim_func(private=True)
    def reshape23(var_take: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        take = T.match_buffer(var_take, (batch_size, T.int64(1)), "int32")
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(1)), "int32")
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding((batch_size + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < batch_size)
                    T.reads(take[v0, T.int64(0)])
                    T.writes(T_reshape[v0, T.int64(0)])
                    T_reshape[v0, T.int64(0)] = take[v0, T.int64(0)]

    @T.prim_func(private=True)
    def reshape24(var_reshape59: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape59 = T.match_buffer(var_reshape59, (batch_size, T.int64(1)), "int32")
        T_reshape = T.match_buffer(var_T_reshape, (batch_size,), "int32")
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding((batch_size + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < batch_size)
                    T.reads(reshape59[v0, T.int64(0)])
                    T.writes(T_reshape[v0])
                    T_reshape[v0] = reshape59[v0, T.int64(0)]

    @T.prim_func(private=True)
    def reshape25(var_take1: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        take1 = T.match_buffer(var_take1, (batch_size, T.int64(8)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(1), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * T.int64(8))
                    T.reads(take1[v0, v1])
                    T.writes(T_reshape[v0, T.int64(0), v1])
                    T_reshape[v0, T.int64(0), v1] = take1[v0, v1]

    @T.prim_func(private=True)
    def reshape3(decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_bias1: T.Buffer((T.int64(768),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(768), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(768))
                    T.reads(decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_bias1[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_bias1[v0]

    @T.prim_func(private=True)
    def reshape4(var_lv482: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv482 = T.match_buffer(var_lv482, (batch_size, T.int64(384), T.int64(64)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(384), T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * T.int64(24), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(24576))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(24576) // T.int64(64))
                    v2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(64))
                    T.reads(lv482[v0, v1, v2])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = lv482[v0, v1, v2]

    @T.prim_func(private=True)
    def reshape5(decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_bias1: T.Buffer((T.int64(384),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(384), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(384))
                    T.reads(decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_bias1[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_bias1[v0]

    @T.prim_func(private=True)
    def reshape6(var_lv548: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv548 = T.match_buffer(var_lv548, (batch_size, T.int64(192), T.int64(256)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(192), T.int64(256)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(49152))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(49152) // T.int64(256))
                    v2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(256))
                    T.reads(lv548[v0, v1, v2])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = lv548[v0, v1, v2]

    @T.prim_func(private=True)
    def reshape7(decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_bias1: T.Buffer((T.int64(192),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(192), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(192))
                    T.reads(decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_bias1[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_bias1[v0]

    @T.prim_func(private=True)
    def reshape8(var_lv614: T.handle, var_T_reshape: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        lv614 = T.match_buffer(var_lv614, (batch_size, T.int64(96), T.int64(512)))
        T_reshape = T.match_buffer(var_T_reshape, (batch_size, T.int64(96), T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(49152))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(49152) // T.int64(512))
                    v2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(512))
                    T.reads(lv614[v0, v1, v2])
                    T.writes(T_reshape[v0, v1, v2])
                    T_reshape[v0, v1, v2] = lv614[v0, v1, v2]

    @T.prim_func(private=True)
    def reshape9(decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_bias1: T.Buffer((T.int64(96),), "float32"), T_reshape: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_reshape"):
                    v0 = T.axis.spatial(T.int64(96), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < T.int64(96))
                    T.reads(decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_bias1[v0])
                    T.writes(T_reshape[T.int64(0), v0, T.int64(0)])
                    T_reshape[T.int64(0), v0, T.int64(0)] = decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_bias1[v0]

    @T.prim_func(private=True)
    def snake(var_reshape: T.handle, encoder_block_layers_1_block_layers_0_block_branches_0_layers_0_alpha: T.Buffer((T.int64(1), T.int64(64), T.int64(1)), "float32"), var_snake_compute: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape = T.match_buffer(var_reshape, (batch_size, T.int64(64), T.int64(512)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(64), T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * T.int64(32), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(32768))
                    v1 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(32768) // T.int64(512))
                    v2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(512))
                    T.reads(reshape[v0, v1, v2], encoder_block_layers_1_block_layers_0_block_branches_0_layers_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape[v0, v1, v2] + T.float32(1.0) / (encoder_block_layers_1_block_layers_0_block_branches_0_layers_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(encoder_block_layers_1_block_layers_0_block_branches_0_layers_0_alpha[T.int64(0), v1, T.int64(0)] * reshape[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake1(var_reshape14: T.handle, encoder_block_layers_2_block_layers_0_block_branches_0_layers_0_alpha: T.Buffer((T.int64(1), T.int64(128), T.int64(1)), "float32"), var_snake_compute: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape14 = T.match_buffer(var_reshape14, (batch_size, T.int64(128), T.int64(256)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(128), T.int64(256)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * T.int64(32), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(32768))
                    v1 = T.axis.spatial(T.int64(128), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(32768) // T.int64(256))
                    v2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(256))
                    T.reads(reshape14[v0, v1, v2], encoder_block_layers_2_block_layers_0_block_branches_0_layers_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape14[v0, v1, v2] + T.float32(1.0) / (encoder_block_layers_2_block_layers_0_block_branches_0_layers_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(encoder_block_layers_2_block_layers_0_block_branches_0_layers_0_alpha[T.int64(0), v1, T.int64(0)] * reshape14[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake2(var_reshape28: T.handle, encoder_block_layers_3_block_layers_0_block_branches_0_layers_0_alpha: T.Buffer((T.int64(1), T.int64(256), T.int64(1)), "float32"), var_snake_compute: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape28 = T.match_buffer(var_reshape28, (batch_size, T.int64(256), T.int64(64)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(256), T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * T.int64(16), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(16384))
                    v1 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(16384) // T.int64(64))
                    v2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(64))
                    T.reads(reshape28[v0, v1, v2], encoder_block_layers_3_block_layers_0_block_branches_0_layers_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape28[v0, v1, v2] + T.float32(1.0) / (encoder_block_layers_3_block_layers_0_block_branches_0_layers_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(encoder_block_layers_3_block_layers_0_block_branches_0_layers_0_alpha[T.int64(0), v1, T.int64(0)] * reshape28[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake3(var_reshape42: T.handle, encoder_block_layers_4_block_layers_0_block_branches_0_layers_0_alpha: T.Buffer((T.int64(1), T.int64(512), T.int64(1)), "float32"), var_snake_compute: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape42 = T.match_buffer(var_reshape42, (batch_size, T.int64(512), T.int64(8)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(512), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * T.int64(4), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(4096))
                    v1 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(4096) // T.int64(8))
                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(8))
                    T.reads(reshape42[v0, v1, v2], encoder_block_layers_4_block_layers_0_block_branches_0_layers_0_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape42[v0, v1, v2] + T.float32(1.0) / (encoder_block_layers_4_block_layers_0_block_branches_0_layers_0_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(encoder_block_layers_4_block_layers_0_block_branches_0_layers_0_alpha[T.int64(0), v1, T.int64(0)] * reshape42[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake4(var_reshape56: T.handle, encoder_block_layers_5_alpha: T.Buffer((T.int64(1), T.int64(1024), T.int64(1)), "float32"), var_snake_compute: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape56 = T.match_buffer(var_reshape56, (batch_size, T.int64(1024), T.int64(1)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(1024), T.int64(1)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(batch_size_1, thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(1024))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(1024))
                    T.reads(reshape56[v0, v1, T.int64(0)], encoder_block_layers_5_alpha[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, T.int64(0)])
                    snake_compute[v0, v1, T.int64(0)] = reshape56[v0, v1, T.int64(0)] + T.float32(1.0) / (encoder_block_layers_5_alpha[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(encoder_block_layers_5_alpha[T.int64(0), v1, T.int64(0)] * reshape56[v0, v1, T.int64(0)]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake5(var_reshape94: T.handle, decoder_model_layers_1_block_layers_0_alpha1: T.Buffer((T.int64(1), T.int64(1536), T.int64(1)), "float32"), var_snake_compute: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape94 = T.match_buffer(var_reshape94, (batch_size, T.int64(1536), T.int64(1)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(1536), T.int64(1)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size_1 * T.int64(1536) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(1536))
                    v1 = T.axis.spatial(T.int64(1536), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(1536))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size_1 * T.int64(1536))
                    T.reads(reshape94[v0, v1, T.int64(0)], decoder_model_layers_1_block_layers_0_alpha1[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, T.int64(0)])
                    snake_compute[v0, v1, T.int64(0)] = reshape94[v0, v1, T.int64(0)] + T.float32(1.0) / (decoder_model_layers_1_block_layers_0_alpha1[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_layers_1_block_layers_0_alpha1[T.int64(0), v1, T.int64(0)] * reshape94[v0, v1, T.int64(0)]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake6(var_reshape96: T.handle, decoder_model_layers_1_block_layers_2_block_branches_0_layers_0_alpha1: T.Buffer((T.int64(1), T.int64(768), T.int64(1)), "float32"), var_snake_compute: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape96 = T.match_buffer(var_reshape96, (batch_size, T.int64(768), T.int64(8)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(768), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * T.int64(6), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(6144))
                    v1 = T.axis.spatial(T.int64(768), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(6144) // T.int64(8))
                    v2 = T.axis.spatial(T.int64(8), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(8))
                    T.reads(reshape96[v0, v1, v2], decoder_model_layers_1_block_layers_2_block_branches_0_layers_0_alpha1[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape96[v0, v1, v2] + T.float32(1.0) / (decoder_model_layers_1_block_layers_2_block_branches_0_layers_0_alpha1[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_layers_1_block_layers_2_block_branches_0_layers_0_alpha1[T.int64(0), v1, T.int64(0)] * reshape96[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake7(var_reshape110: T.handle, decoder_model_layers_2_block_layers_2_block_branches_0_layers_0_alpha1: T.Buffer((T.int64(1), T.int64(384), T.int64(1)), "float32"), var_snake_compute: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape110 = T.match_buffer(var_reshape110, (batch_size, T.int64(384), T.int64(64)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(384), T.int64(64)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * T.int64(24), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(24576))
                    v1 = T.axis.spatial(T.int64(384), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(24576) // T.int64(64))
                    v2 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(64))
                    T.reads(reshape110[v0, v1, v2], decoder_model_layers_2_block_layers_2_block_branches_0_layers_0_alpha1[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape110[v0, v1, v2] + T.float32(1.0) / (decoder_model_layers_2_block_layers_2_block_branches_0_layers_0_alpha1[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_layers_2_block_layers_2_block_branches_0_layers_0_alpha1[T.int64(0), v1, T.int64(0)] * reshape110[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake8(var_reshape124: T.handle, decoder_model_layers_3_block_layers_2_block_branches_0_layers_0_alpha1: T.Buffer((T.int64(1), T.int64(192), T.int64(1)), "float32"), var_snake_compute: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape124 = T.match_buffer(var_reshape124, (batch_size, T.int64(192), T.int64(256)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(192), T.int64(256)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(49152))
                    v1 = T.axis.spatial(T.int64(192), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(49152) // T.int64(256))
                    v2 = T.axis.spatial(T.int64(256), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(256))
                    T.reads(reshape124[v0, v1, v2], decoder_model_layers_3_block_layers_2_block_branches_0_layers_0_alpha1[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape124[v0, v1, v2] + T.float32(1.0) / (decoder_model_layers_3_block_layers_2_block_branches_0_layers_0_alpha1[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_layers_3_block_layers_2_block_branches_0_layers_0_alpha1[T.int64(0), v1, T.int64(0)] * reshape124[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def snake9(var_reshape138: T.handle, decoder_model_layers_4_block_layers_2_block_branches_0_layers_0_alpha1: T.Buffer((T.int64(1), T.int64(96), T.int64(1)), "float32"), var_snake_compute: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape138 = T.match_buffer(var_reshape138, (batch_size, T.int64(96), T.int64(512)))
        batch_size_1 = T.int64()
        snake_compute = T.match_buffer(var_snake_compute, (batch_size_1, T.int64(96), T.int64(512)))
        # with T.block("root"):
        for ax0_ax1_ax2_fused_0 in T.thread_binding(batch_size_1 * T.int64(48), thread="blockIdx.x"):
            for ax0_ax1_ax2_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("snake_compute"):
                    v0 = T.axis.spatial(batch_size_1, (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) // T.int64(49152))
                    v1 = T.axis.spatial(T.int64(96), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(49152) // T.int64(512))
                    v2 = T.axis.spatial(T.int64(512), (ax0_ax1_ax2_fused_0 * T.int64(1024) + ax0_ax1_ax2_fused_1) % T.int64(512))
                    T.reads(reshape138[v0, v1, v2], decoder_model_layers_4_block_layers_2_block_branches_0_layers_0_alpha1[T.int64(0), v1, T.int64(0)])
                    T.writes(snake_compute[v0, v1, v2])
                    snake_compute[v0, v1, v2] = reshape138[v0, v1, v2] + T.float32(1.0) / (decoder_model_layers_4_block_layers_2_block_branches_0_layers_0_alpha1[T.int64(0), v1, T.int64(0)] + T.float32(1.0000000000000001e-09)) * T.pow(T.sin(decoder_model_layers_4_block_layers_2_block_branches_0_layers_0_alpha1[T.int64(0), v1, T.int64(0)] * reshape138[v0, v1, v2]), T.float32(2.0))

    @T.prim_func(private=True)
    def subtract1(var_conv1d29: T.handle, var_conv1d31: T.handle, var_T_subtract: T.handle):
        T.func_attr({"op_pattern": 0, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d29 = T.match_buffer(var_conv1d29, (batch_size, T.int64(1024), T.int64(1)))
        conv1d31 = T.match_buffer(var_conv1d31, (batch_size, T.int64(1024), T.int64(1)))
        T_subtract = T.match_buffer(var_T_subtract, (batch_size, T.int64(1024), T.int64(1)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(batch_size, thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_subtract"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(1024))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(1024))
                    T.reads(conv1d29[v0, v1, T.int64(0)], conv1d31[v0, v1, T.int64(0)])
                    T.writes(T_subtract[v0, v1, T.int64(0)])
                    T_subtract[v0, v1, T.int64(0)] = conv1d29[v0, v1, T.int64(0)] - conv1d31[v0, v1, T.int64(0)]

    @T.prim_func(private=True)
    def take(var_argsort: T.handle, B: T.Buffer((T.int64(1),), "int32"), var_T_take: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        argsort = T.match_buffer(var_argsort, (batch_size, T.int64(1024)), "int32")
        T_take = T.match_buffer(var_T_take, (batch_size, T.int64(1)), "int32")
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding((batch_size + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_take"):
                    v0 = T.axis.spatial(batch_size, ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.where(ax0_fused_0 * T.int64(1024) + ax0_fused_1 < batch_size)
                    T.reads(argsort[v0, B[T.int64(0)]], B[T.int64(0)])
                    T.writes(T_take[v0, T.int64(0)])
                    T_take[v0, T.int64(0)] = argsort[v0, B[T.int64(0)]]

    @T.prim_func(private=True)
    def take1(quantizer_quantizers_0_codebook_weight: T.Buffer((T.int64(1024), T.int64(8)), "float32"), var_reshape60: T.handle, var_T_take: T.handle):
        T.func_attr({"op_pattern": 8, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape60 = T.match_buffer(var_reshape60, (batch_size,), "int32")
        T_take = T.match_buffer(var_T_take, (batch_size, T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_take"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * T.int64(8))
                    T.reads(quantizer_quantizers_0_codebook_weight[reshape60[v0], v1], reshape60[v0])
                    T.writes(T_take[v0, v1])
                    T_take[v0, v1] = quantizer_quantizers_0_codebook_weight[reshape60[v0], v1]

    @T.prim_func(private=True)
    def transpose(var_conv1d30: T.handle, var_T_transpose: T.handle):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        conv1d30 = T.match_buffer(var_conv1d30, (batch_size, T.int64(8), T.int64(1)))
        T_transpose = T.match_buffer(var_T_transpose, (batch_size, T.int64(1), T.int64(8)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_transpose"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * T.int64(8))
                    T.reads(conv1d30[v0, v1, T.int64(0)])
                    T.writes(T_transpose[v0, T.int64(0), v1])
                    T_transpose[v0, T.int64(0), v1] = conv1d30[v0, v1, T.int64(0)]

    @T.prim_func(private=True)
    def transpose1(divide1: T.Buffer((T.int64(1024), T.int64(8)), "float32"), T_transpose: T.Buffer((T.int64(8), T.int64(1024)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(8), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_transpose"):
                    v0 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(1024))
                    v1 = T.axis.spatial(T.int64(1024), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(1024))
                    T.reads(divide1[v1, v0])
                    T.writes(T_transpose[v0, v1])
                    T_transpose[v0, v1] = divide1[v1, v0]

    @T.prim_func(private=True)
    def transpose2(sum3: T.Buffer((T.int64(1024), T.int64(1)), "float32"), T_transpose: T.Buffer((T.int64(1), T.int64(1024)), "float32")):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        # with T.block("root"):
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
            for ax0_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_transpose"):
                    v0 = T.axis.spatial(T.int64(1024), ax0_fused_0 * T.int64(1024) + ax0_fused_1)
                    T.reads(sum3[v0, T.int64(0)])
                    T.writes(T_transpose[T.int64(0), v0])
                    T_transpose[T.int64(0), v0] = sum3[v0, T.int64(0)]

    @T.prim_func(private=True)
    def transpose3(var_reshape61: T.handle, var_T_transpose: T.handle):
        T.func_attr({"op_pattern": 2, "tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
        batch_size = T.int64()
        reshape61 = T.match_buffer(var_reshape61, (batch_size, T.int64(1), T.int64(8)))
        T_transpose = T.match_buffer(var_T_transpose, (batch_size, T.int64(8), T.int64(1)))
        # with T.block("root"):
        for ax0_ax1_fused_0 in T.thread_binding((batch_size * T.int64(8) + T.int64(1023)) // T.int64(1024), thread="blockIdx.x"):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread="threadIdx.x"):
                with T.block("T_transpose"):
                    v0 = T.axis.spatial(batch_size, (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(8))
                    v1 = T.axis.spatial(T.int64(8), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(8))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 < batch_size * T.int64(8))
                    T.reads(reshape61[v0, T.int64(0), v1])
                    T.writes(T_transpose[v0, v1, T.int64(0)])
                    T_transpose[v0, v1, T.int64(0)] = reshape61[v0, T.int64(0), v1]

    @R.function
    def _initialize_effect() -> R.Tuple(R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object):
        cls = Module
        with R.dataflow():
            _io: R.Object = R.null_value()
            encoder_block_layers_0_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(6), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_0_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(6), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_0_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_0_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(3), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(18), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_1_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_1_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(9), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(54), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_2_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_2_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(27), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_4_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(2), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_1_block_layers_4_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(1), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(6), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_0_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_0_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(3), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(18), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_1_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_1_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(9), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(54), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_2_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_2_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(27), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_4_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(4), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_2_block_layers_4_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(1), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(6), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_0_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_0_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(3), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(18), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_1_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_1_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(9), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(54), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_2_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_2_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(27), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_4_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(8), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_3_block_layers_4_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(5), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(6), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_0_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_0_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(3), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(18), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_1_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_1_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(9), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(54), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_2_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_2_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(27), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_4_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(8), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_4_block_layers_4_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(5), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_6_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(2), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            encoder_block_layers_6_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_0_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(6), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_0_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(8), R.prim_value(1), R.prim_value(32), R.prim_value(1), cls.cached_padding_1d_init, cls.cached_padding_transpose_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(6), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_2_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_2_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(3), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(18), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_3_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_3_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(9), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(54), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_4_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_1_block_layers_4_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(27), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(8), R.prim_value(1), R.prim_value(32), R.prim_value(1), cls.cached_padding_1d_init, cls.cached_padding_transpose_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(6), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_2_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_2_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(3), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(18), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_3_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_3_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(9), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(54), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_4_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_2_block_layers_4_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(27), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(4), R.prim_value(1), R.prim_value(32), R.prim_value(1), cls.cached_padding_1d_init, cls.cached_padding_transpose_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(6), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_2_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_2_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(3), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(18), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_3_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_3_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(9), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(54), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_4_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_3_block_layers_4_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(27), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(2), R.prim_value(1), R.prim_value(32), R.prim_value(1), cls.cached_padding_1d_init, cls.cached_padding_transpose_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(6), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_2_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_2_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(3), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(18), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_3_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_3_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(9), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(54), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_4_block_paddings_0_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_4_block_layers_4_block_paddings_1_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(27), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_6_cache_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(6), R.prim_value(0), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            decoder_model_layers_6_downsampling_delay_cache: R.Object = R.call_pure_packed("vm.builtin.cached_padding_1d_create", R.prim_value(0), R.prim_value(1), R.prim_value(32), R.prim_value(0), cls.cached_padding_1d_init, cls.cached_padding_1d_update, cls.cached_padding_1d_crop, sinfo_args=(R.Object,))
            gv: R.Tuple(R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object) = _io, encoder_block_layers_0_cache_cache, encoder_block_layers_0_downsampling_delay_cache, encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_cache_cache, encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_cache_cache, encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_1_block_layers_0_block_paddings_0_cache, encoder_block_layers_1_block_layers_0_block_paddings_1_cache, encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_cache_cache, encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_cache_cache, encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_1_block_layers_1_block_paddings_0_cache, encoder_block_layers_1_block_layers_1_block_paddings_1_cache, encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache, encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache, encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_1_block_layers_2_block_paddings_0_cache, encoder_block_layers_1_block_layers_2_block_paddings_1_cache, encoder_block_layers_1_block_layers_4_cache_cache, encoder_block_layers_1_block_layers_4_downsampling_delay_cache, encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_cache_cache, encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_cache_cache, encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_2_block_layers_0_block_paddings_0_cache, encoder_block_layers_2_block_layers_0_block_paddings_1_cache, encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_cache_cache, encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_cache_cache, encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_2_block_layers_1_block_paddings_0_cache, encoder_block_layers_2_block_layers_1_block_paddings_1_cache, encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache, encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache, encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_2_block_layers_2_block_paddings_0_cache, encoder_block_layers_2_block_layers_2_block_paddings_1_cache, encoder_block_layers_2_block_layers_4_cache_cache, encoder_block_layers_2_block_layers_4_downsampling_delay_cache, encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_cache_cache, encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_cache_cache, encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_3_block_layers_0_block_paddings_0_cache, encoder_block_layers_3_block_layers_0_block_paddings_1_cache, encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_cache_cache, encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_cache_cache, encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_3_block_layers_1_block_paddings_0_cache, encoder_block_layers_3_block_layers_1_block_paddings_1_cache, encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache, encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache, encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_3_block_layers_2_block_paddings_0_cache, encoder_block_layers_3_block_layers_2_block_paddings_1_cache, encoder_block_layers_3_block_layers_4_cache_cache, encoder_block_layers_3_block_layers_4_downsampling_delay_cache, encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_cache_cache, encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_cache_cache, encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_4_block_layers_0_block_paddings_0_cache, encoder_block_layers_4_block_layers_0_block_paddings_1_cache, encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_cache_cache, encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_cache_cache, encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_4_block_layers_1_block_paddings_0_cache, encoder_block_layers_4_block_layers_1_block_paddings_1_cache, encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache, encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache, encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_4_block_layers_2_block_paddings_0_cache, encoder_block_layers_4_block_layers_2_block_paddings_1_cache, encoder_block_layers_4_block_layers_4_cache_cache, encoder_block_layers_4_block_layers_4_downsampling_delay_cache, encoder_block_layers_6_cache_cache, encoder_block_layers_6_downsampling_delay_cache, decoder_model_layers_0_cache_cache, decoder_model_layers_0_downsampling_delay_cache, decoder_model_layers_1_block_layers_1_cache_cache, decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache, decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache, decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_1_block_layers_2_block_paddings_0_cache, decoder_model_layers_1_block_layers_2_block_paddings_1_cache, decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_cache_cache, decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_cache_cache, decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_1_block_layers_3_block_paddings_0_cache, decoder_model_layers_1_block_layers_3_block_paddings_1_cache, decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_cache_cache, decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_cache_cache, decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_1_block_layers_4_block_paddings_0_cache, decoder_model_layers_1_block_layers_4_block_paddings_1_cache, decoder_model_layers_2_block_layers_1_cache_cache, decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache, decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache, decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_2_block_layers_2_block_paddings_0_cache, decoder_model_layers_2_block_layers_2_block_paddings_1_cache, decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_cache_cache, decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_cache_cache, decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_2_block_layers_3_block_paddings_0_cache, decoder_model_layers_2_block_layers_3_block_paddings_1_cache, decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_cache_cache, decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_cache_cache, decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_2_block_layers_4_block_paddings_0_cache, decoder_model_layers_2_block_layers_4_block_paddings_1_cache, decoder_model_layers_3_block_layers_1_cache_cache, decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache, decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache, decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_3_block_layers_2_block_paddings_0_cache, decoder_model_layers_3_block_layers_2_block_paddings_1_cache, decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_cache_cache, decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_cache_cache, decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_3_block_layers_3_block_paddings_0_cache, decoder_model_layers_3_block_layers_3_block_paddings_1_cache, decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_cache_cache, decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_cache_cache, decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_3_block_layers_4_block_paddings_0_cache, decoder_model_layers_3_block_layers_4_block_paddings_1_cache, decoder_model_layers_4_block_layers_1_cache_cache, decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache, decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache, decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_4_block_layers_2_block_paddings_0_cache, decoder_model_layers_4_block_layers_2_block_paddings_1_cache, decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_cache_cache, decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_cache_cache, decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_4_block_layers_3_block_paddings_0_cache, decoder_model_layers_4_block_layers_3_block_paddings_1_cache, decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_cache_cache, decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_cache_cache, decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_4_block_layers_4_block_paddings_0_cache, decoder_model_layers_4_block_layers_4_block_paddings_1_cache, decoder_model_layers_6_cache_cache, decoder_model_layers_6_downsampling_delay_cache
            R.output(gv)
        return gv

    @R.function
    def decode(z: R.Tensor(("batch_size", 1024, 1), dtype="float32"), _io: R.Object, encoder_block_layers_0_cache_cache: R.Object, encoder_block_layers_0_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_0_block_paddings_0_cache: R.Object, encoder_block_layers_1_block_layers_0_block_paddings_1_cache: R.Object, encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_1_block_paddings_0_cache: R.Object, encoder_block_layers_1_block_layers_1_block_paddings_1_cache: R.Object, encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_2_block_paddings_0_cache: R.Object, encoder_block_layers_1_block_layers_2_block_paddings_1_cache: R.Object, encoder_block_layers_1_block_layers_4_cache_cache: R.Object, encoder_block_layers_1_block_layers_4_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_0_block_paddings_0_cache: R.Object, encoder_block_layers_2_block_layers_0_block_paddings_1_cache: R.Object, encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_1_block_paddings_0_cache: R.Object, encoder_block_layers_2_block_layers_1_block_paddings_1_cache: R.Object, encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_2_block_paddings_0_cache: R.Object, encoder_block_layers_2_block_layers_2_block_paddings_1_cache: R.Object, encoder_block_layers_2_block_layers_4_cache_cache: R.Object, encoder_block_layers_2_block_layers_4_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_0_block_paddings_0_cache: R.Object, encoder_block_layers_3_block_layers_0_block_paddings_1_cache: R.Object, encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_1_block_paddings_0_cache: R.Object, encoder_block_layers_3_block_layers_1_block_paddings_1_cache: R.Object, encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_2_block_paddings_0_cache: R.Object, encoder_block_layers_3_block_layers_2_block_paddings_1_cache: R.Object, encoder_block_layers_3_block_layers_4_cache_cache: R.Object, encoder_block_layers_3_block_layers_4_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_0_block_paddings_0_cache: R.Object, encoder_block_layers_4_block_layers_0_block_paddings_1_cache: R.Object, encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_1_block_paddings_0_cache: R.Object, encoder_block_layers_4_block_layers_1_block_paddings_1_cache: R.Object, encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_2_block_paddings_0_cache: R.Object, encoder_block_layers_4_block_layers_2_block_paddings_1_cache: R.Object, encoder_block_layers_4_block_layers_4_cache_cache: R.Object, encoder_block_layers_4_block_layers_4_downsampling_delay_cache: R.Object, encoder_block_layers_6_cache_cache: R.Object, encoder_block_layers_6_downsampling_delay_cache: R.Object, decoder_model_layers_0_cache_cache: R.Object, decoder_model_layers_0_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_1_cache_cache: R.Object, decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_2_block_paddings_0_cache: R.Object, decoder_model_layers_1_block_layers_2_block_paddings_1_cache: R.Object, decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_3_block_paddings_0_cache: R.Object, decoder_model_layers_1_block_layers_3_block_paddings_1_cache: R.Object, decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_4_block_paddings_0_cache: R.Object, decoder_model_layers_1_block_layers_4_block_paddings_1_cache: R.Object, decoder_model_layers_2_block_layers_1_cache_cache: R.Object, decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_2_block_layers_2_block_paddings_0_cache: R.Object, decoder_model_layers_2_block_layers_2_block_paddings_1_cache: R.Object, decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_2_block_layers_3_block_paddings_0_cache: R.Object, decoder_model_layers_2_block_layers_3_block_paddings_1_cache: R.Object, decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_2_block_layers_4_block_paddings_0_cache: R.Object, decoder_model_layers_2_block_layers_4_block_paddings_1_cache: R.Object, decoder_model_layers_3_block_layers_1_cache_cache: R.Object, decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_3_block_layers_2_block_paddings_0_cache: R.Object, decoder_model_layers_3_block_layers_2_block_paddings_1_cache: R.Object, decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_3_block_layers_3_block_paddings_0_cache: R.Object, decoder_model_layers_3_block_layers_3_block_paddings_1_cache: R.Object, decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_3_block_layers_4_block_paddings_0_cache: R.Object, decoder_model_layers_3_block_layers_4_block_paddings_1_cache: R.Object, decoder_model_layers_4_block_layers_1_cache_cache: R.Object, decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_4_block_layers_2_block_paddings_0_cache: R.Object, decoder_model_layers_4_block_layers_2_block_paddings_1_cache: R.Object, decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_4_block_layers_3_block_paddings_0_cache: R.Object, decoder_model_layers_4_block_layers_3_block_paddings_1_cache: R.Object, decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_4_block_layers_4_block_paddings_0_cache: R.Object, decoder_model_layers_4_block_layers_4_block_paddings_1_cache: R.Object, decoder_model_layers_6_cache_cache: R.Object, decoder_model_layers_6_downsampling_delay_cache: R.Object, packed_params: R.Tuple(R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 1, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 64, 4), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 128, 8), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 256, 16), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 512, 16), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1, 1024, 1), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 1024, 3), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((1536, 1, 1), dtype="float32"), R.Tensor((1536, 1024, 7), dtype="float32"), R.Tensor((1536,), dtype="float32"), R.Tensor((1, 1536, 1), dtype="float32"), R.Tensor((1536, 1, 1), dtype="float32"), R.Tensor((1536, 768, 16), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 384, 16), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 192, 8), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 96, 4), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((1, 1, 1), dtype="float32"), R.Tensor((1, 96, 7), dtype="float32"), R.Tensor((1,), dtype="float32"))) -> R.Tuple(R.Tensor(("batch_size", 1, 512), dtype="float32"), R.Tuple(R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object)):
        batch_size = T.int64()
        R.func_attr({"num_input": 166})
        cls = Module
        with R.dataflow():
            decoder_model_layers_0_weight_g1: R.Tensor((1536, 1, 1), dtype="float32") = packed_params[182]
            decoder_model_layers_0_weight_v1: R.Tensor((1536, 1024, 7), dtype="float32") = packed_params[183]
            decoder_model_layers_0_bias1: R.Tensor((1536,), dtype="float32") = packed_params[184]
            decoder_model_layers_1_block_layers_0_alpha1: R.Tensor((1, 1536, 1), dtype="float32") = packed_params[185]
            decoder_model_layers_1_block_layers_1_weight_g1: R.Tensor((1536, 1, 1), dtype="float32") = packed_params[186]
            decoder_model_layers_1_block_layers_1_weight_v1: R.Tensor((1536, 768, 16), dtype="float32") = packed_params[187]
            decoder_model_layers_1_block_layers_1_bias1: R.Tensor((768,), dtype="float32") = packed_params[188]
            decoder_model_layers_1_block_layers_2_block_branches_0_layers_0_alpha1: R.Tensor((1, 768, 1), dtype="float32") = packed_params[189]
            decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_g1: R.Tensor((768, 1, 1), dtype="float32") = packed_params[190]
            decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_v1: R.Tensor((768, 768, 7), dtype="float32") = packed_params[191]
            decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_bias1: R.Tensor((768,), dtype="float32") = packed_params[192]
            decoder_model_layers_1_block_layers_2_block_branches_0_layers_2_alpha1: R.Tensor((1, 768, 1), dtype="float32") = packed_params[193]
            decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_g1: R.Tensor((768, 1, 1), dtype="float32") = packed_params[194]
            decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_v1: R.Tensor((768, 768, 1), dtype="float32") = packed_params[195]
            decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_bias1: R.Tensor((768,), dtype="float32") = packed_params[196]
            decoder_model_layers_1_block_layers_3_block_branches_0_layers_0_alpha1: R.Tensor((1, 768, 1), dtype="float32") = packed_params[197]
            decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_weight_g1: R.Tensor((768, 1, 1), dtype="float32") = packed_params[198]
            decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_weight_v1: R.Tensor((768, 768, 7), dtype="float32") = packed_params[199]
            decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_bias1: R.Tensor((768,), dtype="float32") = packed_params[200]
            decoder_model_layers_1_block_layers_3_block_branches_0_layers_2_alpha1: R.Tensor((1, 768, 1), dtype="float32") = packed_params[201]
            decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_weight_g1: R.Tensor((768, 1, 1), dtype="float32") = packed_params[202]
            decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_weight_v1: R.Tensor((768, 768, 1), dtype="float32") = packed_params[203]
            decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_bias1: R.Tensor((768,), dtype="float32") = packed_params[204]
            decoder_model_layers_1_block_layers_4_block_branches_0_layers_0_alpha1: R.Tensor((1, 768, 1), dtype="float32") = packed_params[205]
            decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_weight_g1: R.Tensor((768, 1, 1), dtype="float32") = packed_params[206]
            decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_weight_v1: R.Tensor((768, 768, 7), dtype="float32") = packed_params[207]
            decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_bias1: R.Tensor((768,), dtype="float32") = packed_params[208]
            decoder_model_layers_1_block_layers_4_block_branches_0_layers_2_alpha1: R.Tensor((1, 768, 1), dtype="float32") = packed_params[209]
            decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_weight_g1: R.Tensor((768, 1, 1), dtype="float32") = packed_params[210]
            decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_weight_v1: R.Tensor((768, 768, 1), dtype="float32") = packed_params[211]
            decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_bias1: R.Tensor((768,), dtype="float32") = packed_params[212]
            decoder_model_layers_2_block_layers_0_alpha1: R.Tensor((1, 768, 1), dtype="float32") = packed_params[213]
            decoder_model_layers_2_block_layers_1_weight_g1: R.Tensor((768, 1, 1), dtype="float32") = packed_params[214]
            decoder_model_layers_2_block_layers_1_weight_v1: R.Tensor((768, 384, 16), dtype="float32") = packed_params[215]
            decoder_model_layers_2_block_layers_1_bias1: R.Tensor((384,), dtype="float32") = packed_params[216]
            decoder_model_layers_2_block_layers_2_block_branches_0_layers_0_alpha1: R.Tensor((1, 384, 1), dtype="float32") = packed_params[217]
            decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_g1: R.Tensor((384, 1, 1), dtype="float32") = packed_params[218]
            decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_v1: R.Tensor((384, 384, 7), dtype="float32") = packed_params[219]
            decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_bias1: R.Tensor((384,), dtype="float32") = packed_params[220]
            decoder_model_layers_2_block_layers_2_block_branches_0_layers_2_alpha1: R.Tensor((1, 384, 1), dtype="float32") = packed_params[221]
            decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_g1: R.Tensor((384, 1, 1), dtype="float32") = packed_params[222]
            decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_v1: R.Tensor((384, 384, 1), dtype="float32") = packed_params[223]
            decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_bias1: R.Tensor((384,), dtype="float32") = packed_params[224]
            decoder_model_layers_2_block_layers_3_block_branches_0_layers_0_alpha1: R.Tensor((1, 384, 1), dtype="float32") = packed_params[225]
            decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_weight_g1: R.Tensor((384, 1, 1), dtype="float32") = packed_params[226]
            decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_weight_v1: R.Tensor((384, 384, 7), dtype="float32") = packed_params[227]
            decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_bias1: R.Tensor((384,), dtype="float32") = packed_params[228]
            decoder_model_layers_2_block_layers_3_block_branches_0_layers_2_alpha1: R.Tensor((1, 384, 1), dtype="float32") = packed_params[229]
            decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_weight_g1: R.Tensor((384, 1, 1), dtype="float32") = packed_params[230]
            decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_weight_v1: R.Tensor((384, 384, 1), dtype="float32") = packed_params[231]
            decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_bias1: R.Tensor((384,), dtype="float32") = packed_params[232]
            decoder_model_layers_2_block_layers_4_block_branches_0_layers_0_alpha1: R.Tensor((1, 384, 1), dtype="float32") = packed_params[233]
            decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_weight_g1: R.Tensor((384, 1, 1), dtype="float32") = packed_params[234]
            decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_weight_v1: R.Tensor((384, 384, 7), dtype="float32") = packed_params[235]
            decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_bias1: R.Tensor((384,), dtype="float32") = packed_params[236]
            decoder_model_layers_2_block_layers_4_block_branches_0_layers_2_alpha1: R.Tensor((1, 384, 1), dtype="float32") = packed_params[237]
            decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_weight_g1: R.Tensor((384, 1, 1), dtype="float32") = packed_params[238]
            decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_weight_v1: R.Tensor((384, 384, 1), dtype="float32") = packed_params[239]
            decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_bias1: R.Tensor((384,), dtype="float32") = packed_params[240]
            decoder_model_layers_3_block_layers_0_alpha1: R.Tensor((1, 384, 1), dtype="float32") = packed_params[241]
            decoder_model_layers_3_block_layers_1_weight_g1: R.Tensor((384, 1, 1), dtype="float32") = packed_params[242]
            decoder_model_layers_3_block_layers_1_weight_v1: R.Tensor((384, 192, 8), dtype="float32") = packed_params[243]
            decoder_model_layers_3_block_layers_1_bias1: R.Tensor((192,), dtype="float32") = packed_params[244]
            decoder_model_layers_3_block_layers_2_block_branches_0_layers_0_alpha1: R.Tensor((1, 192, 1), dtype="float32") = packed_params[245]
            decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_g1: R.Tensor((192, 1, 1), dtype="float32") = packed_params[246]
            decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_v1: R.Tensor((192, 192, 7), dtype="float32") = packed_params[247]
            decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_bias1: R.Tensor((192,), dtype="float32") = packed_params[248]
            decoder_model_layers_3_block_layers_2_block_branches_0_layers_2_alpha1: R.Tensor((1, 192, 1), dtype="float32") = packed_params[249]
            decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_g1: R.Tensor((192, 1, 1), dtype="float32") = packed_params[250]
            decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_v1: R.Tensor((192, 192, 1), dtype="float32") = packed_params[251]
            decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_bias1: R.Tensor((192,), dtype="float32") = packed_params[252]
            decoder_model_layers_3_block_layers_3_block_branches_0_layers_0_alpha1: R.Tensor((1, 192, 1), dtype="float32") = packed_params[253]
            decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_weight_g1: R.Tensor((192, 1, 1), dtype="float32") = packed_params[254]
            decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_weight_v1: R.Tensor((192, 192, 7), dtype="float32") = packed_params[255]
            decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_bias1: R.Tensor((192,), dtype="float32") = packed_params[256]
            decoder_model_layers_3_block_layers_3_block_branches_0_layers_2_alpha1: R.Tensor((1, 192, 1), dtype="float32") = packed_params[257]
            decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_weight_g1: R.Tensor((192, 1, 1), dtype="float32") = packed_params[258]
            decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_weight_v1: R.Tensor((192, 192, 1), dtype="float32") = packed_params[259]
            decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_bias1: R.Tensor((192,), dtype="float32") = packed_params[260]
            decoder_model_layers_3_block_layers_4_block_branches_0_layers_0_alpha1: R.Tensor((1, 192, 1), dtype="float32") = packed_params[261]
            decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_weight_g1: R.Tensor((192, 1, 1), dtype="float32") = packed_params[262]
            decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_weight_v1: R.Tensor((192, 192, 7), dtype="float32") = packed_params[263]
            decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_bias1: R.Tensor((192,), dtype="float32") = packed_params[264]
            decoder_model_layers_3_block_layers_4_block_branches_0_layers_2_alpha1: R.Tensor((1, 192, 1), dtype="float32") = packed_params[265]
            decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_weight_g1: R.Tensor((192, 1, 1), dtype="float32") = packed_params[266]
            decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_weight_v1: R.Tensor((192, 192, 1), dtype="float32") = packed_params[267]
            decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_bias1: R.Tensor((192,), dtype="float32") = packed_params[268]
            decoder_model_layers_4_block_layers_0_alpha1: R.Tensor((1, 192, 1), dtype="float32") = packed_params[269]
            decoder_model_layers_4_block_layers_1_weight_g1: R.Tensor((192, 1, 1), dtype="float32") = packed_params[270]
            decoder_model_layers_4_block_layers_1_weight_v1: R.Tensor((192, 96, 4), dtype="float32") = packed_params[271]
            decoder_model_layers_4_block_layers_1_bias1: R.Tensor((96,), dtype="float32") = packed_params[272]
            decoder_model_layers_4_block_layers_2_block_branches_0_layers_0_alpha1: R.Tensor((1, 96, 1), dtype="float32") = packed_params[273]
            decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_g1: R.Tensor((96, 1, 1), dtype="float32") = packed_params[274]
            decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_v1: R.Tensor((96, 96, 7), dtype="float32") = packed_params[275]
            decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_bias1: R.Tensor((96,), dtype="float32") = packed_params[276]
            decoder_model_layers_4_block_layers_2_block_branches_0_layers_2_alpha1: R.Tensor((1, 96, 1), dtype="float32") = packed_params[277]
            decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_g1: R.Tensor((96, 1, 1), dtype="float32") = packed_params[278]
            decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_v1: R.Tensor((96, 96, 1), dtype="float32") = packed_params[279]
            decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_bias1: R.Tensor((96,), dtype="float32") = packed_params[280]
            decoder_model_layers_4_block_layers_3_block_branches_0_layers_0_alpha1: R.Tensor((1, 96, 1), dtype="float32") = packed_params[281]
            decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_weight_g1: R.Tensor((96, 1, 1), dtype="float32") = packed_params[282]
            decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_weight_v1: R.Tensor((96, 96, 7), dtype="float32") = packed_params[283]
            decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_bias1: R.Tensor((96,), dtype="float32") = packed_params[284]
            decoder_model_layers_4_block_layers_3_block_branches_0_layers_2_alpha1: R.Tensor((1, 96, 1), dtype="float32") = packed_params[285]
            decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_weight_g1: R.Tensor((96, 1, 1), dtype="float32") = packed_params[286]
            decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_weight_v1: R.Tensor((96, 96, 1), dtype="float32") = packed_params[287]
            decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_bias1: R.Tensor((96,), dtype="float32") = packed_params[288]
            decoder_model_layers_4_block_layers_4_block_branches_0_layers_0_alpha1: R.Tensor((1, 96, 1), dtype="float32") = packed_params[289]
            decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_weight_g1: R.Tensor((96, 1, 1), dtype="float32") = packed_params[290]
            decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_weight_v1: R.Tensor((96, 96, 7), dtype="float32") = packed_params[291]
            decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_bias1: R.Tensor((96,), dtype="float32") = packed_params[292]
            decoder_model_layers_4_block_layers_4_block_branches_0_layers_2_alpha1: R.Tensor((1, 96, 1), dtype="float32") = packed_params[293]
            decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_weight_g1: R.Tensor((96, 1, 1), dtype="float32") = packed_params[294]
            decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_weight_v1: R.Tensor((96, 96, 1), dtype="float32") = packed_params[295]
            decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_bias1: R.Tensor((96,), dtype="float32") = packed_params[296]
            decoder_model_layers_5_alpha1: R.Tensor((1, 96, 1), dtype="float32") = packed_params[297]
            decoder_model_layers_6_weight_g1: R.Tensor((1, 1, 1), dtype="float32") = packed_params[298]
            decoder_model_layers_6_weight_v1: R.Tensor((1, 96, 7), dtype="float32") = packed_params[299]
            decoder_model_layers_6_bias1: R.Tensor((1,), dtype="float32") = packed_params[300]
            lv402: R.Tensor((batch_size, 1024, 1), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_0_downsampling_delay_cache, z, sinfo_args=(R.Tensor((batch_size, 1024, 1), dtype="float32"),))
            lv403: R.Tensor((batch_size, 1024, 7), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_0_cache_cache, lv402, sinfo_args=(R.Tensor((batch_size, 1024, 7), dtype="float32"),))
            lv = R.call_tir(cls.fused_tir_square_sum, (decoder_model_layers_0_weight_v1,), out_sinfo=R.Tensor((1536, 1, 1), dtype="float32"))
            lv1 = R.call_tir(cls.fused_tir_sqrt_divide_multiply, (lv, decoder_model_layers_0_weight_v1, decoder_model_layers_0_weight_g1), out_sinfo=R.Tensor((1536, 1024, 7), dtype="float32"))
            lv409 = R.call_tir(cls.reshape, (decoder_model_layers_0_bias1,), out_sinfo=R.Tensor((1, 1536, 1), dtype="float32"))
            lv2 = R.call_tir(cls.fused_conv1d_add, (lv403, lv1, lv409), out_sinfo=R.Tensor((batch_size, 1536, 1), dtype="float32"))
            reshape94 = R.call_tir(cls.reshape1, (lv2,), out_sinfo=R.Tensor((batch_size, 1536, 1), dtype="float32"))
            lv410 = R.call_tir(cls.snake5, (reshape94, decoder_model_layers_1_block_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 1536, 1), dtype="float32"))
            reshape95 = R.call_tir(cls.reshape1, (lv410,), out_sinfo=R.Tensor((batch_size, 1536, 1), dtype="float32"))
            lv3 = R.call_tir(cls.fused_tir_square1_sum1, (decoder_model_layers_1_block_layers_1_weight_v1,), out_sinfo=R.Tensor((1536, 1, 1), dtype="float32"))
            lv4 = R.call_tir(cls.fused_tir_sqrt_divide1_multiply1, (lv3, decoder_model_layers_1_block_layers_1_weight_v1, decoder_model_layers_1_block_layers_1_weight_g1), out_sinfo=R.Tensor((1536, 768, 16), dtype="float32"))
            conv1d_transpose = R.call_tir(cls.conv1d_transpose, (reshape95, lv4), out_sinfo=R.Tensor((batch_size, 768, 16), dtype="float32"))
            lv415: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_1_cache_cache, conv1d_transpose, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            lv5 = R.call_tir(cls.fused_expand_dims_add1, (decoder_model_layers_1_block_layers_1_bias1, lv415), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv416: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_2_block_paddings_0_cache, lv5, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            reshape96 = R.call_tir(cls.reshape2, (lv416,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv417 = R.call_tir(cls.snake6, (reshape96, decoder_model_layers_1_block_layers_2_block_branches_0_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            reshape97 = R.call_tir(cls.reshape2, (lv417,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv418: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, reshape97, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            lv419: R.Tensor((batch_size, 768, 14), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache, lv418, sinfo_args=(R.Tensor((batch_size, 768, 14), dtype="float32"),))
            lv6 = R.call_tir(cls.fused_tir_square2_sum2, (decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_v1,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv7 = R.call_tir(cls.fused_tir_sqrt1_divide2_multiply2, (lv6, decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_v1, decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_weight_g1), out_sinfo=R.Tensor((768, 768, 7), dtype="float32"))
            lv425 = R.call_tir(cls.reshape3, (decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_bias1,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv8 = R.call_tir(cls.fused_conv1d1_add2, (lv419, lv7, lv425), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            reshape98 = R.call_tir(cls.reshape2, (lv8,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv426 = R.call_tir(cls.snake6, (reshape98, decoder_model_layers_1_block_layers_2_block_branches_0_layers_2_alpha1), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            reshape99 = R.call_tir(cls.reshape2, (lv426,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv427: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, reshape99, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            lv428: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache, lv427, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            lv9 = R.call_tir(cls.fused_tir_square3_sum3, (decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_v1,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv10 = R.call_tir(cls.fused_tir_sqrt1_divide3_multiply3, (lv9, decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_v1, decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_weight_g1), out_sinfo=R.Tensor((768, 768, 1), dtype="float32"))
            lv434 = R.call_tir(cls.reshape3, (decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_bias1,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv435: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_2_block_paddings_1_cache, lv5, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            lv11 = R.call_tir(cls.fused_conv1d2_add2_add3_add4, (lv428, lv10, lv434, lv435), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv436: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_3_block_paddings_0_cache, lv11, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            reshape100 = R.call_tir(cls.reshape2, (lv436,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv437 = R.call_tir(cls.snake6, (reshape100, decoder_model_layers_1_block_layers_3_block_branches_0_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            reshape101 = R.call_tir(cls.reshape2, (lv437,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv438: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, reshape101, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            lv439: R.Tensor((batch_size, 768, 26), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_cache_cache, lv438, sinfo_args=(R.Tensor((batch_size, 768, 26), dtype="float32"),))
            lv12 = R.call_tir(cls.fused_tir_square2_sum2, (decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_weight_v1,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv13 = R.call_tir(cls.fused_tir_sqrt1_divide2_multiply2, (lv12, decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_weight_v1, decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_weight_g1), out_sinfo=R.Tensor((768, 768, 7), dtype="float32"))
            lv445 = R.call_tir(cls.reshape3, (decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_bias1,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv14 = R.call_tir(cls.fused_conv1d3_add2, (lv439, lv13, lv445), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            reshape102 = R.call_tir(cls.reshape2, (lv14,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv446 = R.call_tir(cls.snake6, (reshape102, decoder_model_layers_1_block_layers_3_block_branches_0_layers_2_alpha1), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            reshape103 = R.call_tir(cls.reshape2, (lv446,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv447: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, reshape103, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            lv448: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_cache_cache, lv447, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            lv15 = R.call_tir(cls.fused_tir_square3_sum3, (decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_weight_v1,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv16 = R.call_tir(cls.fused_tir_sqrt1_divide3_multiply3, (lv15, decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_weight_v1, decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_weight_g1), out_sinfo=R.Tensor((768, 768, 1), dtype="float32"))
            lv454 = R.call_tir(cls.reshape3, (decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_bias1,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv455: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_3_block_paddings_1_cache, lv11, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            lv17 = R.call_tir(cls.fused_conv1d2_add2_add3_add4, (lv448, lv16, lv454, lv455), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv456: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_4_block_paddings_0_cache, lv17, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            reshape104 = R.call_tir(cls.reshape2, (lv456,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv457 = R.call_tir(cls.snake6, (reshape104, decoder_model_layers_1_block_layers_4_block_branches_0_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            reshape105 = R.call_tir(cls.reshape2, (lv457,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv458: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, reshape105, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            lv459: R.Tensor((batch_size, 768, 62), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_cache_cache, lv458, sinfo_args=(R.Tensor((batch_size, 768, 62), dtype="float32"),))
            lv18 = R.call_tir(cls.fused_tir_square2_sum2, (decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_weight_v1,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv19 = R.call_tir(cls.fused_tir_sqrt1_divide2_multiply2, (lv18, decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_weight_v1, decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_weight_g1), out_sinfo=R.Tensor((768, 768, 7), dtype="float32"))
            lv465 = R.call_tir(cls.reshape3, (decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_bias1,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv20 = R.call_tir(cls.fused_conv1d4_add2, (lv459, lv19, lv465), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            reshape106 = R.call_tir(cls.reshape2, (lv20,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv466 = R.call_tir(cls.snake6, (reshape106, decoder_model_layers_1_block_layers_4_block_branches_0_layers_2_alpha1), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            reshape107 = R.call_tir(cls.reshape2, (lv466,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv467: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, reshape107, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            lv468: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_cache_cache, lv467, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            lv21 = R.call_tir(cls.fused_tir_square3_sum3, (decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_weight_v1,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv22 = R.call_tir(cls.fused_tir_sqrt1_divide3_multiply3, (lv21, decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_weight_v1, decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_weight_g1), out_sinfo=R.Tensor((768, 768, 1), dtype="float32"))
            lv474 = R.call_tir(cls.reshape3, (decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_bias1,), out_sinfo=R.Tensor((1, 768, 1), dtype="float32"))
            lv475: R.Tensor((batch_size, 768, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_1_block_layers_4_block_paddings_1_cache, lv17, sinfo_args=(R.Tensor((batch_size, 768, 8), dtype="float32"),))
            lv23 = R.call_tir(cls.fused_conv1d2_add2_add3_add4, (lv468, lv22, lv474, lv475), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            reshape108 = R.call_tir(cls.reshape2, (lv23,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv476 = R.call_tir(cls.snake6, (reshape108, decoder_model_layers_2_block_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            reshape109 = R.call_tir(cls.reshape2, (lv476,), out_sinfo=R.Tensor((batch_size, 768, 8), dtype="float32"))
            lv24 = R.call_tir(cls.fused_tir_square4_sum4, (decoder_model_layers_2_block_layers_1_weight_v1,), out_sinfo=R.Tensor((768, 1, 1), dtype="float32"))
            lv25 = R.call_tir(cls.fused_tir_sqrt1_divide4_multiply4, (lv24, decoder_model_layers_2_block_layers_1_weight_v1, decoder_model_layers_2_block_layers_1_weight_g1), out_sinfo=R.Tensor((768, 384, 16), dtype="float32"))
            conv1d_transpose1 = R.call_tir(cls.conv1d_transpose1, (reshape109, lv25), out_sinfo=R.Tensor((batch_size, 384, 72), dtype="float32"))
            lv481: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_1_cache_cache, conv1d_transpose1, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            lv26 = R.call_tir(cls.fused_expand_dims1_add5, (decoder_model_layers_2_block_layers_1_bias1, lv481), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv482: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_2_block_paddings_0_cache, lv26, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            reshape110 = R.call_tir(cls.reshape4, (lv482,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv483 = R.call_tir(cls.snake7, (reshape110, decoder_model_layers_2_block_layers_2_block_branches_0_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            reshape111 = R.call_tir(cls.reshape4, (lv483,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv484: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, reshape111, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            lv485: R.Tensor((batch_size, 384, 70), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache, lv484, sinfo_args=(R.Tensor((batch_size, 384, 70), dtype="float32"),))
            lv27 = R.call_tir(cls.fused_tir_square5_sum5, (decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_v1,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv28 = R.call_tir(cls.fused_tir_sqrt2_divide5_multiply5, (lv27, decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_v1, decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_weight_g1), out_sinfo=R.Tensor((384, 384, 7), dtype="float32"))
            lv491 = R.call_tir(cls.reshape5, (decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_bias1,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv29 = R.call_tir(cls.fused_conv1d5_add6, (lv485, lv28, lv491), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            reshape112 = R.call_tir(cls.reshape4, (lv29,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv492 = R.call_tir(cls.snake7, (reshape112, decoder_model_layers_2_block_layers_2_block_branches_0_layers_2_alpha1), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            reshape113 = R.call_tir(cls.reshape4, (lv492,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv493: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, reshape113, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            lv494: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache, lv493, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            lv30 = R.call_tir(cls.fused_tir_square6_sum6, (decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_v1,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv31 = R.call_tir(cls.fused_tir_sqrt2_divide6_multiply6, (lv30, decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_v1, decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_weight_g1), out_sinfo=R.Tensor((384, 384, 1), dtype="float32"))
            lv500 = R.call_tir(cls.reshape5, (decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_bias1,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv501: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_2_block_paddings_1_cache, lv26, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            lv32 = R.call_tir(cls.fused_conv1d6_add6_add7_add8, (lv494, lv31, lv500, lv501), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv502: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_3_block_paddings_0_cache, lv32, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            reshape114 = R.call_tir(cls.reshape4, (lv502,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv503 = R.call_tir(cls.snake7, (reshape114, decoder_model_layers_2_block_layers_3_block_branches_0_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            reshape115 = R.call_tir(cls.reshape4, (lv503,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv504: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, reshape115, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            lv505: R.Tensor((batch_size, 384, 82), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_cache_cache, lv504, sinfo_args=(R.Tensor((batch_size, 384, 82), dtype="float32"),))
            lv33 = R.call_tir(cls.fused_tir_square5_sum5, (decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_weight_v1,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv34 = R.call_tir(cls.fused_tir_sqrt2_divide5_multiply5, (lv33, decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_weight_v1, decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_weight_g1), out_sinfo=R.Tensor((384, 384, 7), dtype="float32"))
            lv511 = R.call_tir(cls.reshape5, (decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_bias1,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv35 = R.call_tir(cls.fused_conv1d7_add6, (lv505, lv34, lv511), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            reshape116 = R.call_tir(cls.reshape4, (lv35,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv512 = R.call_tir(cls.snake7, (reshape116, decoder_model_layers_2_block_layers_3_block_branches_0_layers_2_alpha1), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            reshape117 = R.call_tir(cls.reshape4, (lv512,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv513: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, reshape117, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            lv514: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_cache_cache, lv513, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            lv36 = R.call_tir(cls.fused_tir_square6_sum6, (decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_weight_v1,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv37 = R.call_tir(cls.fused_tir_sqrt2_divide6_multiply6, (lv36, decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_weight_v1, decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_weight_g1), out_sinfo=R.Tensor((384, 384, 1), dtype="float32"))
            lv520 = R.call_tir(cls.reshape5, (decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_bias1,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv521: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_3_block_paddings_1_cache, lv32, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            lv38 = R.call_tir(cls.fused_conv1d6_add6_add7_add8, (lv514, lv37, lv520, lv521), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv522: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_4_block_paddings_0_cache, lv38, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            reshape118 = R.call_tir(cls.reshape4, (lv522,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv523 = R.call_tir(cls.snake7, (reshape118, decoder_model_layers_2_block_layers_4_block_branches_0_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            reshape119 = R.call_tir(cls.reshape4, (lv523,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv524: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, reshape119, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            lv525: R.Tensor((batch_size, 384, 118), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_cache_cache, lv524, sinfo_args=(R.Tensor((batch_size, 384, 118), dtype="float32"),))
            lv39 = R.call_tir(cls.fused_tir_square5_sum5, (decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_weight_v1,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv40 = R.call_tir(cls.fused_tir_sqrt2_divide5_multiply5, (lv39, decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_weight_v1, decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_weight_g1), out_sinfo=R.Tensor((384, 384, 7), dtype="float32"))
            lv531 = R.call_tir(cls.reshape5, (decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_bias1,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv41 = R.call_tir(cls.fused_conv1d8_add6, (lv525, lv40, lv531), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            reshape120 = R.call_tir(cls.reshape4, (lv41,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv532 = R.call_tir(cls.snake7, (reshape120, decoder_model_layers_2_block_layers_4_block_branches_0_layers_2_alpha1), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            reshape121 = R.call_tir(cls.reshape4, (lv532,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv533: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, reshape121, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            lv534: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_cache_cache, lv533, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            lv42 = R.call_tir(cls.fused_tir_square6_sum6, (decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_weight_v1,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv43 = R.call_tir(cls.fused_tir_sqrt2_divide6_multiply6, (lv42, decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_weight_v1, decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_weight_g1), out_sinfo=R.Tensor((384, 384, 1), dtype="float32"))
            lv540 = R.call_tir(cls.reshape5, (decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_bias1,), out_sinfo=R.Tensor((1, 384, 1), dtype="float32"))
            lv541: R.Tensor((batch_size, 384, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_2_block_layers_4_block_paddings_1_cache, lv38, sinfo_args=(R.Tensor((batch_size, 384, 64), dtype="float32"),))
            lv44 = R.call_tir(cls.fused_conv1d6_add6_add7_add8, (lv534, lv43, lv540, lv541), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            reshape122 = R.call_tir(cls.reshape4, (lv44,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv542 = R.call_tir(cls.snake7, (reshape122, decoder_model_layers_3_block_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            reshape123 = R.call_tir(cls.reshape4, (lv542,), out_sinfo=R.Tensor((batch_size, 384, 64), dtype="float32"))
            lv45 = R.call_tir(cls.fused_tir_square7_sum7, (decoder_model_layers_3_block_layers_1_weight_v1,), out_sinfo=R.Tensor((384, 1, 1), dtype="float32"))
            lv46 = R.call_tir(cls.fused_tir_sqrt2_divide7_multiply7, (lv45, decoder_model_layers_3_block_layers_1_weight_v1, decoder_model_layers_3_block_layers_1_weight_g1), out_sinfo=R.Tensor((384, 192, 8), dtype="float32"))
            conv1d_transpose2 = R.call_tir(cls.conv1d_transpose2, (reshape123, lv46), out_sinfo=R.Tensor((batch_size, 192, 260), dtype="float32"))
            lv547: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_1_cache_cache, conv1d_transpose2, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            lv47 = R.call_tir(cls.fused_expand_dims2_add9, (decoder_model_layers_3_block_layers_1_bias1, lv547), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv548: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_2_block_paddings_0_cache, lv47, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            reshape124 = R.call_tir(cls.reshape6, (lv548,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv549 = R.call_tir(cls.snake8, (reshape124, decoder_model_layers_3_block_layers_2_block_branches_0_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            reshape125 = R.call_tir(cls.reshape6, (lv549,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv550: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, reshape125, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            lv551: R.Tensor((batch_size, 192, 262), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache, lv550, sinfo_args=(R.Tensor((batch_size, 192, 262), dtype="float32"),))
            lv48 = R.call_tir(cls.fused_tir_square8_sum8, (decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_v1,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv49 = R.call_tir(cls.fused_tir_sqrt3_divide8_multiply8, (lv48, decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_v1, decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_weight_g1), out_sinfo=R.Tensor((192, 192, 7), dtype="float32"))
            lv557 = R.call_tir(cls.reshape7, (decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_bias1,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv50 = R.call_tir(cls.fused_conv1d9_add10, (lv551, lv49, lv557), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            reshape126 = R.call_tir(cls.reshape6, (lv50,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv558 = R.call_tir(cls.snake8, (reshape126, decoder_model_layers_3_block_layers_2_block_branches_0_layers_2_alpha1), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            reshape127 = R.call_tir(cls.reshape6, (lv558,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv559: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, reshape127, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            lv560: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache, lv559, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            lv51 = R.call_tir(cls.fused_tir_square9_sum9, (decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_v1,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv52 = R.call_tir(cls.fused_tir_sqrt3_divide9_multiply9, (lv51, decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_v1, decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_weight_g1), out_sinfo=R.Tensor((192, 192, 1), dtype="float32"))
            lv566 = R.call_tir(cls.reshape7, (decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_bias1,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv567: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_2_block_paddings_1_cache, lv47, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            lv53 = R.call_tir(cls.fused_conv1d10_add10_add11_add12, (lv560, lv52, lv566, lv567), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv568: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_3_block_paddings_0_cache, lv53, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            reshape128 = R.call_tir(cls.reshape6, (lv568,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv569 = R.call_tir(cls.snake8, (reshape128, decoder_model_layers_3_block_layers_3_block_branches_0_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            reshape129 = R.call_tir(cls.reshape6, (lv569,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv570: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, reshape129, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            lv571: R.Tensor((batch_size, 192, 274), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_cache_cache, lv570, sinfo_args=(R.Tensor((batch_size, 192, 274), dtype="float32"),))
            lv54 = R.call_tir(cls.fused_tir_square8_sum8, (decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_weight_v1,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv55 = R.call_tir(cls.fused_tir_sqrt3_divide8_multiply8, (lv54, decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_weight_v1, decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_weight_g1), out_sinfo=R.Tensor((192, 192, 7), dtype="float32"))
            lv577 = R.call_tir(cls.reshape7, (decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_bias1,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv56 = R.call_tir(cls.fused_conv1d11_add10, (lv571, lv55, lv577), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            reshape130 = R.call_tir(cls.reshape6, (lv56,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv578 = R.call_tir(cls.snake8, (reshape130, decoder_model_layers_3_block_layers_3_block_branches_0_layers_2_alpha1), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            reshape131 = R.call_tir(cls.reshape6, (lv578,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv579: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, reshape131, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            lv580: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_cache_cache, lv579, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            lv57 = R.call_tir(cls.fused_tir_square9_sum9, (decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_weight_v1,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv58 = R.call_tir(cls.fused_tir_sqrt3_divide9_multiply9, (lv57, decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_weight_v1, decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_weight_g1), out_sinfo=R.Tensor((192, 192, 1), dtype="float32"))
            lv586 = R.call_tir(cls.reshape7, (decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_bias1,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv587: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_3_block_paddings_1_cache, lv53, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            lv59 = R.call_tir(cls.fused_conv1d10_add10_add11_add12, (lv580, lv58, lv586, lv587), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv588: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_4_block_paddings_0_cache, lv59, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            reshape132 = R.call_tir(cls.reshape6, (lv588,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv589 = R.call_tir(cls.snake8, (reshape132, decoder_model_layers_3_block_layers_4_block_branches_0_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            reshape133 = R.call_tir(cls.reshape6, (lv589,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv590: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, reshape133, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            lv591: R.Tensor((batch_size, 192, 310), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_cache_cache, lv590, sinfo_args=(R.Tensor((batch_size, 192, 310), dtype="float32"),))
            lv60 = R.call_tir(cls.fused_tir_square8_sum8, (decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_weight_v1,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv61 = R.call_tir(cls.fused_tir_sqrt3_divide8_multiply8, (lv60, decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_weight_v1, decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_weight_g1), out_sinfo=R.Tensor((192, 192, 7), dtype="float32"))
            lv597 = R.call_tir(cls.reshape7, (decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_bias1,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv62 = R.call_tir(cls.fused_conv1d12_add10, (lv591, lv61, lv597), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            reshape134 = R.call_tir(cls.reshape6, (lv62,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv598 = R.call_tir(cls.snake8, (reshape134, decoder_model_layers_3_block_layers_4_block_branches_0_layers_2_alpha1), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            reshape135 = R.call_tir(cls.reshape6, (lv598,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv599: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, reshape135, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            lv600: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_cache_cache, lv599, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            lv63 = R.call_tir(cls.fused_tir_square9_sum9, (decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_weight_v1,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv64 = R.call_tir(cls.fused_tir_sqrt3_divide9_multiply9, (lv63, decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_weight_v1, decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_weight_g1), out_sinfo=R.Tensor((192, 192, 1), dtype="float32"))
            lv606 = R.call_tir(cls.reshape7, (decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_bias1,), out_sinfo=R.Tensor((1, 192, 1), dtype="float32"))
            lv607: R.Tensor((batch_size, 192, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_3_block_layers_4_block_paddings_1_cache, lv59, sinfo_args=(R.Tensor((batch_size, 192, 256), dtype="float32"),))
            lv65 = R.call_tir(cls.fused_conv1d10_add10_add11_add12, (lv600, lv64, lv606, lv607), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            reshape136 = R.call_tir(cls.reshape6, (lv65,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv608 = R.call_tir(cls.snake8, (reshape136, decoder_model_layers_4_block_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            reshape137 = R.call_tir(cls.reshape6, (lv608,), out_sinfo=R.Tensor((batch_size, 192, 256), dtype="float32"))
            lv66 = R.call_tir(cls.fused_tir_square10_sum10, (decoder_model_layers_4_block_layers_1_weight_v1,), out_sinfo=R.Tensor((192, 1, 1), dtype="float32"))
            lv67 = R.call_tir(cls.fused_tir_sqrt3_divide10_multiply10, (lv66, decoder_model_layers_4_block_layers_1_weight_v1, decoder_model_layers_4_block_layers_1_weight_g1), out_sinfo=R.Tensor((192, 96, 4), dtype="float32"))
            conv1d_transpose3 = R.call_tir(cls.conv1d_transpose3, (reshape137, lv67), out_sinfo=R.Tensor((batch_size, 96, 514), dtype="float32"))
            lv613: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_1_cache_cache, conv1d_transpose3, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv68 = R.call_tir(cls.fused_expand_dims3_add13, (decoder_model_layers_4_block_layers_1_bias1, lv613), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv614: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_2_block_paddings_0_cache, lv68, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            reshape138 = R.call_tir(cls.reshape8, (lv614,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv615 = R.call_tir(cls.snake9, (reshape138, decoder_model_layers_4_block_layers_2_block_branches_0_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            reshape139 = R.call_tir(cls.reshape8, (lv615,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv616: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, reshape139, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv617: R.Tensor((batch_size, 96, 518), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache, lv616, sinfo_args=(R.Tensor((batch_size, 96, 518), dtype="float32"),))
            lv69 = R.call_tir(cls.fused_tir_square11_sum11, (decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_v1,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv70 = R.call_tir(cls.fused_tir_sqrt4_divide11_multiply11, (lv69, decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_v1, decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_weight_g1), out_sinfo=R.Tensor((96, 96, 7), dtype="float32"))
            lv623 = R.call_tir(cls.reshape9, (decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_bias1,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv71 = R.call_tir(cls.fused_conv1d13_add14, (lv617, lv70, lv623), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            reshape140 = R.call_tir(cls.reshape8, (lv71,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv624 = R.call_tir(cls.snake9, (reshape140, decoder_model_layers_4_block_layers_2_block_branches_0_layers_2_alpha1), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            reshape141 = R.call_tir(cls.reshape8, (lv624,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv625: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, reshape141, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv626: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache, lv625, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv72 = R.call_tir(cls.fused_tir_square12_sum12, (decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_v1,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv73 = R.call_tir(cls.fused_tir_sqrt4_divide12_multiply12, (lv72, decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_v1, decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_weight_g1), out_sinfo=R.Tensor((96, 96, 1), dtype="float32"))
            lv632 = R.call_tir(cls.reshape9, (decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_bias1,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv633: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_2_block_paddings_1_cache, lv68, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv74 = R.call_tir(cls.fused_conv1d14_add14_add15_add16, (lv626, lv73, lv632, lv633), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv634: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_3_block_paddings_0_cache, lv74, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            reshape142 = R.call_tir(cls.reshape8, (lv634,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv635 = R.call_tir(cls.snake9, (reshape142, decoder_model_layers_4_block_layers_3_block_branches_0_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            reshape143 = R.call_tir(cls.reshape8, (lv635,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv636: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, reshape143, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv637: R.Tensor((batch_size, 96, 530), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_cache_cache, lv636, sinfo_args=(R.Tensor((batch_size, 96, 530), dtype="float32"),))
            lv75 = R.call_tir(cls.fused_tir_square11_sum11, (decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_weight_v1,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv76 = R.call_tir(cls.fused_tir_sqrt4_divide11_multiply11, (lv75, decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_weight_v1, decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_weight_g1), out_sinfo=R.Tensor((96, 96, 7), dtype="float32"))
            lv643 = R.call_tir(cls.reshape9, (decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_bias1,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv77 = R.call_tir(cls.fused_conv1d15_add14, (lv637, lv76, lv643), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            reshape144 = R.call_tir(cls.reshape8, (lv77,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv644 = R.call_tir(cls.snake9, (reshape144, decoder_model_layers_4_block_layers_3_block_branches_0_layers_2_alpha1), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            reshape145 = R.call_tir(cls.reshape8, (lv644,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv645: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, reshape145, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv646: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_cache_cache, lv645, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv78 = R.call_tir(cls.fused_tir_square12_sum12, (decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_weight_v1,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv79 = R.call_tir(cls.fused_tir_sqrt4_divide12_multiply12, (lv78, decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_weight_v1, decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_weight_g1), out_sinfo=R.Tensor((96, 96, 1), dtype="float32"))
            lv652 = R.call_tir(cls.reshape9, (decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_bias1,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv653: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_3_block_paddings_1_cache, lv74, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv80 = R.call_tir(cls.fused_conv1d14_add14_add15_add16, (lv646, lv79, lv652, lv653), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv654: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_4_block_paddings_0_cache, lv80, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            reshape146 = R.call_tir(cls.reshape8, (lv654,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv655 = R.call_tir(cls.snake9, (reshape146, decoder_model_layers_4_block_layers_4_block_branches_0_layers_0_alpha1), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            reshape147 = R.call_tir(cls.reshape8, (lv655,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv656: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, reshape147, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv657: R.Tensor((batch_size, 96, 566), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_cache_cache, lv656, sinfo_args=(R.Tensor((batch_size, 96, 566), dtype="float32"),))
            lv81 = R.call_tir(cls.fused_tir_square11_sum11, (decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_weight_v1,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv82 = R.call_tir(cls.fused_tir_sqrt4_divide11_multiply11, (lv81, decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_weight_v1, decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_weight_g1), out_sinfo=R.Tensor((96, 96, 7), dtype="float32"))
            lv663 = R.call_tir(cls.reshape9, (decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_bias1,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv83 = R.call_tir(cls.fused_conv1d16_add14, (lv657, lv82, lv663), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            reshape148 = R.call_tir(cls.reshape8, (lv83,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv664 = R.call_tir(cls.snake9, (reshape148, decoder_model_layers_4_block_layers_4_block_branches_0_layers_2_alpha1), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            reshape149 = R.call_tir(cls.reshape8, (lv664,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv665: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, reshape149, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv666: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_cache_cache, lv665, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv84 = R.call_tir(cls.fused_tir_square12_sum12, (decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_weight_v1,), out_sinfo=R.Tensor((96, 1, 1), dtype="float32"))
            lv85 = R.call_tir(cls.fused_tir_sqrt4_divide12_multiply12, (lv84, decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_weight_v1, decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_weight_g1), out_sinfo=R.Tensor((96, 96, 1), dtype="float32"))
            lv672 = R.call_tir(cls.reshape9, (decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_bias1,), out_sinfo=R.Tensor((1, 96, 1), dtype="float32"))
            lv673: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_4_block_layers_4_block_paddings_1_cache, lv80, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv86 = R.call_tir(cls.fused_conv1d14_add14_add15_add16, (lv666, lv85, lv672, lv673), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            reshape150 = R.call_tir(cls.reshape8, (lv86,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv674 = R.call_tir(cls.snake9, (reshape150, decoder_model_layers_5_alpha1), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            reshape151 = R.call_tir(cls.reshape8, (lv674,), out_sinfo=R.Tensor((batch_size, 96, 512), dtype="float32"))
            lv675: R.Tensor((batch_size, 96, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_6_downsampling_delay_cache, reshape151, sinfo_args=(R.Tensor((batch_size, 96, 512), dtype="float32"),))
            lv676: R.Tensor((batch_size, 96, 518), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", decoder_model_layers_6_cache_cache, lv675, sinfo_args=(R.Tensor((batch_size, 96, 518), dtype="float32"),))
            lv87 = R.call_tir(cls.fused_tir_square13_sum13, (decoder_model_layers_6_weight_v1,), out_sinfo=R.Tensor((1, 1, 1), dtype="float32"))
            lv88 = R.call_tir(cls.fused_tir_sqrt5_divide13_multiply13, (lv87, decoder_model_layers_6_weight_v1, decoder_model_layers_6_weight_g1), out_sinfo=R.Tensor((1, 96, 7), dtype="float32"))
            lv89 = R.call_tir(cls.fused_conv1d17_reshape10_add17_tir_tanh, (lv676, lv88, decoder_model_layers_6_bias1), out_sinfo=R.Tensor((batch_size, 1, 512), dtype="float32"))
            gv2: R.Tuple(R.Tensor((batch_size, 1, 512), dtype="float32"), R.Tuple(R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object)) = lv89, (_io, encoder_block_layers_0_cache_cache, encoder_block_layers_0_downsampling_delay_cache, encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_cache_cache, encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_cache_cache, encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_1_block_layers_0_block_paddings_0_cache, encoder_block_layers_1_block_layers_0_block_paddings_1_cache, encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_cache_cache, encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_cache_cache, encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_1_block_layers_1_block_paddings_0_cache, encoder_block_layers_1_block_layers_1_block_paddings_1_cache, encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache, encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache, encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_1_block_layers_2_block_paddings_0_cache, encoder_block_layers_1_block_layers_2_block_paddings_1_cache, encoder_block_layers_1_block_layers_4_cache_cache, encoder_block_layers_1_block_layers_4_downsampling_delay_cache, encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_cache_cache, encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_cache_cache, encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_2_block_layers_0_block_paddings_0_cache, encoder_block_layers_2_block_layers_0_block_paddings_1_cache, encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_cache_cache, encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_cache_cache, encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_2_block_layers_1_block_paddings_0_cache, encoder_block_layers_2_block_layers_1_block_paddings_1_cache, encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache, encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache, encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_2_block_layers_2_block_paddings_0_cache, encoder_block_layers_2_block_layers_2_block_paddings_1_cache, encoder_block_layers_2_block_layers_4_cache_cache, encoder_block_layers_2_block_layers_4_downsampling_delay_cache, encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_cache_cache, encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_cache_cache, encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_3_block_layers_0_block_paddings_0_cache, encoder_block_layers_3_block_layers_0_block_paddings_1_cache, encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_cache_cache, encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_cache_cache, encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_3_block_layers_1_block_paddings_0_cache, encoder_block_layers_3_block_layers_1_block_paddings_1_cache, encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache, encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache, encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_3_block_layers_2_block_paddings_0_cache, encoder_block_layers_3_block_layers_2_block_paddings_1_cache, encoder_block_layers_3_block_layers_4_cache_cache, encoder_block_layers_3_block_layers_4_downsampling_delay_cache, encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_cache_cache, encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_cache_cache, encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_4_block_layers_0_block_paddings_0_cache, encoder_block_layers_4_block_layers_0_block_paddings_1_cache, encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_cache_cache, encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_cache_cache, encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_4_block_layers_1_block_paddings_0_cache, encoder_block_layers_4_block_layers_1_block_paddings_1_cache, encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache, encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache, encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_4_block_layers_2_block_paddings_0_cache, encoder_block_layers_4_block_layers_2_block_paddings_1_cache, encoder_block_layers_4_block_layers_4_cache_cache, encoder_block_layers_4_block_layers_4_downsampling_delay_cache, encoder_block_layers_6_cache_cache, encoder_block_layers_6_downsampling_delay_cache, decoder_model_layers_0_cache_cache, decoder_model_layers_0_downsampling_delay_cache, decoder_model_layers_1_block_layers_1_cache_cache, decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache, decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache, decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_1_block_layers_2_block_paddings_0_cache, decoder_model_layers_1_block_layers_2_block_paddings_1_cache, decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_cache_cache, decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_cache_cache, decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_1_block_layers_3_block_paddings_0_cache, decoder_model_layers_1_block_layers_3_block_paddings_1_cache, decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_cache_cache, decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_cache_cache, decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_1_block_layers_4_block_paddings_0_cache, decoder_model_layers_1_block_layers_4_block_paddings_1_cache, decoder_model_layers_2_block_layers_1_cache_cache, decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache, decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache, decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_2_block_layers_2_block_paddings_0_cache, decoder_model_layers_2_block_layers_2_block_paddings_1_cache, decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_cache_cache, decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_cache_cache, decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_2_block_layers_3_block_paddings_0_cache, decoder_model_layers_2_block_layers_3_block_paddings_1_cache, decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_cache_cache, decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_cache_cache, decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_2_block_layers_4_block_paddings_0_cache, decoder_model_layers_2_block_layers_4_block_paddings_1_cache, decoder_model_layers_3_block_layers_1_cache_cache, decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache, decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache, decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_3_block_layers_2_block_paddings_0_cache, decoder_model_layers_3_block_layers_2_block_paddings_1_cache, decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_cache_cache, decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_cache_cache, decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_3_block_layers_3_block_paddings_0_cache, decoder_model_layers_3_block_layers_3_block_paddings_1_cache, decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_cache_cache, decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_cache_cache, decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_3_block_layers_4_block_paddings_0_cache, decoder_model_layers_3_block_layers_4_block_paddings_1_cache, decoder_model_layers_4_block_layers_1_cache_cache, decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache, decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache, decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_4_block_layers_2_block_paddings_0_cache, decoder_model_layers_4_block_layers_2_block_paddings_1_cache, decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_cache_cache, decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_cache_cache, decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_4_block_layers_3_block_paddings_0_cache, decoder_model_layers_4_block_layers_3_block_paddings_1_cache, decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_cache_cache, decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_cache_cache, decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_4_block_layers_4_block_paddings_0_cache, decoder_model_layers_4_block_layers_4_block_paddings_1_cache, decoder_model_layers_6_cache_cache, decoder_model_layers_6_downsampling_delay_cache)
            R.output(gv2)
        return gv2

    @R.function
    def encode(audio_data: R.Tensor(("batch_size", 1, 512), dtype="float32"), _io: R.Object, encoder_block_layers_0_cache_cache: R.Object, encoder_block_layers_0_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_0_block_paddings_0_cache: R.Object, encoder_block_layers_1_block_layers_0_block_paddings_1_cache: R.Object, encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_1_block_paddings_0_cache: R.Object, encoder_block_layers_1_block_layers_1_block_paddings_1_cache: R.Object, encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_1_block_layers_2_block_paddings_0_cache: R.Object, encoder_block_layers_1_block_layers_2_block_paddings_1_cache: R.Object, encoder_block_layers_1_block_layers_4_cache_cache: R.Object, encoder_block_layers_1_block_layers_4_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_0_block_paddings_0_cache: R.Object, encoder_block_layers_2_block_layers_0_block_paddings_1_cache: R.Object, encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_1_block_paddings_0_cache: R.Object, encoder_block_layers_2_block_layers_1_block_paddings_1_cache: R.Object, encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_2_block_layers_2_block_paddings_0_cache: R.Object, encoder_block_layers_2_block_layers_2_block_paddings_1_cache: R.Object, encoder_block_layers_2_block_layers_4_cache_cache: R.Object, encoder_block_layers_2_block_layers_4_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_0_block_paddings_0_cache: R.Object, encoder_block_layers_3_block_layers_0_block_paddings_1_cache: R.Object, encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_1_block_paddings_0_cache: R.Object, encoder_block_layers_3_block_layers_1_block_paddings_1_cache: R.Object, encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_3_block_layers_2_block_paddings_0_cache: R.Object, encoder_block_layers_3_block_layers_2_block_paddings_1_cache: R.Object, encoder_block_layers_3_block_layers_4_cache_cache: R.Object, encoder_block_layers_3_block_layers_4_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_0_block_paddings_0_cache: R.Object, encoder_block_layers_4_block_layers_0_block_paddings_1_cache: R.Object, encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_1_block_paddings_0_cache: R.Object, encoder_block_layers_4_block_layers_1_block_paddings_1_cache: R.Object, encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, encoder_block_layers_4_block_layers_2_block_paddings_0_cache: R.Object, encoder_block_layers_4_block_layers_2_block_paddings_1_cache: R.Object, encoder_block_layers_4_block_layers_4_cache_cache: R.Object, encoder_block_layers_4_block_layers_4_downsampling_delay_cache: R.Object, encoder_block_layers_6_cache_cache: R.Object, encoder_block_layers_6_downsampling_delay_cache: R.Object, decoder_model_layers_0_cache_cache: R.Object, decoder_model_layers_0_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_1_cache_cache: R.Object, decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_2_block_paddings_0_cache: R.Object, decoder_model_layers_1_block_layers_2_block_paddings_1_cache: R.Object, decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_3_block_paddings_0_cache: R.Object, decoder_model_layers_1_block_layers_3_block_paddings_1_cache: R.Object, decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_1_block_layers_4_block_paddings_0_cache: R.Object, decoder_model_layers_1_block_layers_4_block_paddings_1_cache: R.Object, decoder_model_layers_2_block_layers_1_cache_cache: R.Object, decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_2_block_layers_2_block_paddings_0_cache: R.Object, decoder_model_layers_2_block_layers_2_block_paddings_1_cache: R.Object, decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_2_block_layers_3_block_paddings_0_cache: R.Object, decoder_model_layers_2_block_layers_3_block_paddings_1_cache: R.Object, decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_2_block_layers_4_block_paddings_0_cache: R.Object, decoder_model_layers_2_block_layers_4_block_paddings_1_cache: R.Object, decoder_model_layers_3_block_layers_1_cache_cache: R.Object, decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_3_block_layers_2_block_paddings_0_cache: R.Object, decoder_model_layers_3_block_layers_2_block_paddings_1_cache: R.Object, decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_3_block_layers_3_block_paddings_0_cache: R.Object, decoder_model_layers_3_block_layers_3_block_paddings_1_cache: R.Object, decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_3_block_layers_4_block_paddings_0_cache: R.Object, decoder_model_layers_3_block_layers_4_block_paddings_1_cache: R.Object, decoder_model_layers_4_block_layers_1_cache_cache: R.Object, decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_4_block_layers_2_block_paddings_0_cache: R.Object, decoder_model_layers_4_block_layers_2_block_paddings_1_cache: R.Object, decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_4_block_layers_3_block_paddings_0_cache: R.Object, decoder_model_layers_4_block_layers_3_block_paddings_1_cache: R.Object, decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_cache_cache: R.Object, decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache: R.Object, decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_cache_cache: R.Object, decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache: R.Object, decoder_model_layers_4_block_layers_4_block_paddings_0_cache: R.Object, decoder_model_layers_4_block_layers_4_block_paddings_1_cache: R.Object, decoder_model_layers_6_cache_cache: R.Object, decoder_model_layers_6_downsampling_delay_cache: R.Object, packed_params: R.Tuple(R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 1, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 7), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((64, 1, 1), dtype="float32"), R.Tensor((64, 64, 1), dtype="float32"), R.Tensor((64,), dtype="float32"), R.Tensor((1, 64, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 64, 4), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 7), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((128, 1, 1), dtype="float32"), R.Tensor((128, 128, 1), dtype="float32"), R.Tensor((128,), dtype="float32"), R.Tensor((1, 128, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 128, 8), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 7), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((256, 1, 1), dtype="float32"), R.Tensor((256, 256, 1), dtype="float32"), R.Tensor((256,), dtype="float32"), R.Tensor((1, 256, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 256, 16), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 7), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((512, 1, 1), dtype="float32"), R.Tensor((512, 512, 1), dtype="float32"), R.Tensor((512,), dtype="float32"), R.Tensor((1, 512, 1), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 512, 16), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1, 1024, 1), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 1024, 3), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((8, 1, 1), dtype="float32"), R.Tensor((8, 1024, 1), dtype="float32"), R.Tensor((8,), dtype="float32"), R.Tensor((1024, 1, 1), dtype="float32"), R.Tensor((1024, 8, 1), dtype="float32"), R.Tensor((1024,), dtype="float32"), R.Tensor((1024, 8), dtype="float32"), R.Tensor((1536, 1, 1), dtype="float32"), R.Tensor((1536, 1024, 7), dtype="float32"), R.Tensor((1536,), dtype="float32"), R.Tensor((1, 1536, 1), dtype="float32"), R.Tensor((1536, 1, 1), dtype="float32"), R.Tensor((1536, 768, 16), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 7), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 768, 1), dtype="float32"), R.Tensor((768,), dtype="float32"), R.Tensor((1, 768, 1), dtype="float32"), R.Tensor((768, 1, 1), dtype="float32"), R.Tensor((768, 384, 16), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 7), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 384, 1), dtype="float32"), R.Tensor((384,), dtype="float32"), R.Tensor((1, 384, 1), dtype="float32"), R.Tensor((384, 1, 1), dtype="float32"), R.Tensor((384, 192, 8), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 7), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 192, 1), dtype="float32"), R.Tensor((192,), dtype="float32"), R.Tensor((1, 192, 1), dtype="float32"), R.Tensor((192, 1, 1), dtype="float32"), R.Tensor((192, 96, 4), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 7), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((96, 1, 1), dtype="float32"), R.Tensor((96, 96, 1), dtype="float32"), R.Tensor((96,), dtype="float32"), R.Tensor((1, 96, 1), dtype="float32"), R.Tensor((1, 1, 1), dtype="float32"), R.Tensor((1, 96, 7), dtype="float32"), R.Tensor((1,), dtype="float32"))) -> R.Tuple(R.Tuple(R.Tensor(("batch_size", 1024, 1), dtype="float32"), R.Tuple(R.Tensor(("batch_size", 1), dtype="int32"), R.Tensor(("batch_size", 1), dtype="int32"), R.Tensor(("batch_size", 1), dtype="int32"), R.Tensor(("batch_size", 1), dtype="int32"), R.Tensor(("batch_size", 1), dtype="int32"), R.Tensor(("batch_size", 1), dtype="int32"), R.Tensor(("batch_size", 1), dtype="int32"), R.Tensor(("batch_size", 1), dtype="int32"), R.Tensor(("batch_size", 1), dtype="int32"))), R.Tuple(R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object)):
        batch_size = T.int64()
        R.func_attr({"num_input": 166})
        cls = Module
        with R.dataflow():
            encoder_block_layers_0_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[0]
            encoder_block_layers_0_weight_v: R.Tensor((64, 1, 7), dtype="float32") = packed_params[1]
            encoder_block_layers_0_bias: R.Tensor((64,), dtype="float32") = packed_params[2]
            encoder_block_layers_1_block_layers_0_block_branches_0_layers_0_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[3]
            encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[4]
            encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_v: R.Tensor((64, 64, 7), dtype="float32") = packed_params[5]
            encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_bias: R.Tensor((64,), dtype="float32") = packed_params[6]
            encoder_block_layers_1_block_layers_0_block_branches_0_layers_2_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[7]
            encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[8]
            encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_v: R.Tensor((64, 64, 1), dtype="float32") = packed_params[9]
            encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_bias: R.Tensor((64,), dtype="float32") = packed_params[10]
            encoder_block_layers_1_block_layers_1_block_branches_0_layers_0_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[11]
            encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[12]
            encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_weight_v: R.Tensor((64, 64, 7), dtype="float32") = packed_params[13]
            encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_bias: R.Tensor((64,), dtype="float32") = packed_params[14]
            encoder_block_layers_1_block_layers_1_block_branches_0_layers_2_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[15]
            encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[16]
            encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_weight_v: R.Tensor((64, 64, 1), dtype="float32") = packed_params[17]
            encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_bias: R.Tensor((64,), dtype="float32") = packed_params[18]
            encoder_block_layers_1_block_layers_2_block_branches_0_layers_0_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[19]
            encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[20]
            encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_weight_v: R.Tensor((64, 64, 7), dtype="float32") = packed_params[21]
            encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_bias: R.Tensor((64,), dtype="float32") = packed_params[22]
            encoder_block_layers_1_block_layers_2_block_branches_0_layers_2_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[23]
            encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_weight_g: R.Tensor((64, 1, 1), dtype="float32") = packed_params[24]
            encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_weight_v: R.Tensor((64, 64, 1), dtype="float32") = packed_params[25]
            encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_bias: R.Tensor((64,), dtype="float32") = packed_params[26]
            encoder_block_layers_1_block_layers_3_alpha: R.Tensor((1, 64, 1), dtype="float32") = packed_params[27]
            encoder_block_layers_1_block_layers_4_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[28]
            encoder_block_layers_1_block_layers_4_weight_v: R.Tensor((128, 64, 4), dtype="float32") = packed_params[29]
            encoder_block_layers_1_block_layers_4_bias: R.Tensor((128,), dtype="float32") = packed_params[30]
            encoder_block_layers_2_block_layers_0_block_branches_0_layers_0_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[31]
            encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[32]
            encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_v: R.Tensor((128, 128, 7), dtype="float32") = packed_params[33]
            encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_bias: R.Tensor((128,), dtype="float32") = packed_params[34]
            encoder_block_layers_2_block_layers_0_block_branches_0_layers_2_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[35]
            encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[36]
            encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_v: R.Tensor((128, 128, 1), dtype="float32") = packed_params[37]
            encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_bias: R.Tensor((128,), dtype="float32") = packed_params[38]
            encoder_block_layers_2_block_layers_1_block_branches_0_layers_0_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[39]
            encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[40]
            encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_weight_v: R.Tensor((128, 128, 7), dtype="float32") = packed_params[41]
            encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_bias: R.Tensor((128,), dtype="float32") = packed_params[42]
            encoder_block_layers_2_block_layers_1_block_branches_0_layers_2_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[43]
            encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[44]
            encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_weight_v: R.Tensor((128, 128, 1), dtype="float32") = packed_params[45]
            encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_bias: R.Tensor((128,), dtype="float32") = packed_params[46]
            encoder_block_layers_2_block_layers_2_block_branches_0_layers_0_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[47]
            encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[48]
            encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_weight_v: R.Tensor((128, 128, 7), dtype="float32") = packed_params[49]
            encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_bias: R.Tensor((128,), dtype="float32") = packed_params[50]
            encoder_block_layers_2_block_layers_2_block_branches_0_layers_2_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[51]
            encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_weight_g: R.Tensor((128, 1, 1), dtype="float32") = packed_params[52]
            encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_weight_v: R.Tensor((128, 128, 1), dtype="float32") = packed_params[53]
            encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_bias: R.Tensor((128,), dtype="float32") = packed_params[54]
            encoder_block_layers_2_block_layers_3_alpha: R.Tensor((1, 128, 1), dtype="float32") = packed_params[55]
            encoder_block_layers_2_block_layers_4_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[56]
            encoder_block_layers_2_block_layers_4_weight_v: R.Tensor((256, 128, 8), dtype="float32") = packed_params[57]
            encoder_block_layers_2_block_layers_4_bias: R.Tensor((256,), dtype="float32") = packed_params[58]
            encoder_block_layers_3_block_layers_0_block_branches_0_layers_0_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[59]
            encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[60]
            encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_v: R.Tensor((256, 256, 7), dtype="float32") = packed_params[61]
            encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_bias: R.Tensor((256,), dtype="float32") = packed_params[62]
            encoder_block_layers_3_block_layers_0_block_branches_0_layers_2_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[63]
            encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[64]
            encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_v: R.Tensor((256, 256, 1), dtype="float32") = packed_params[65]
            encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_bias: R.Tensor((256,), dtype="float32") = packed_params[66]
            encoder_block_layers_3_block_layers_1_block_branches_0_layers_0_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[67]
            encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[68]
            encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_weight_v: R.Tensor((256, 256, 7), dtype="float32") = packed_params[69]
            encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_bias: R.Tensor((256,), dtype="float32") = packed_params[70]
            encoder_block_layers_3_block_layers_1_block_branches_0_layers_2_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[71]
            encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[72]
            encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_weight_v: R.Tensor((256, 256, 1), dtype="float32") = packed_params[73]
            encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_bias: R.Tensor((256,), dtype="float32") = packed_params[74]
            encoder_block_layers_3_block_layers_2_block_branches_0_layers_0_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[75]
            encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[76]
            encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_weight_v: R.Tensor((256, 256, 7), dtype="float32") = packed_params[77]
            encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_bias: R.Tensor((256,), dtype="float32") = packed_params[78]
            encoder_block_layers_3_block_layers_2_block_branches_0_layers_2_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[79]
            encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_weight_g: R.Tensor((256, 1, 1), dtype="float32") = packed_params[80]
            encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_weight_v: R.Tensor((256, 256, 1), dtype="float32") = packed_params[81]
            encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_bias: R.Tensor((256,), dtype="float32") = packed_params[82]
            encoder_block_layers_3_block_layers_3_alpha: R.Tensor((1, 256, 1), dtype="float32") = packed_params[83]
            encoder_block_layers_3_block_layers_4_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[84]
            encoder_block_layers_3_block_layers_4_weight_v: R.Tensor((512, 256, 16), dtype="float32") = packed_params[85]
            encoder_block_layers_3_block_layers_4_bias: R.Tensor((512,), dtype="float32") = packed_params[86]
            encoder_block_layers_4_block_layers_0_block_branches_0_layers_0_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[87]
            encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[88]
            encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_v: R.Tensor((512, 512, 7), dtype="float32") = packed_params[89]
            encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_bias: R.Tensor((512,), dtype="float32") = packed_params[90]
            encoder_block_layers_4_block_layers_0_block_branches_0_layers_2_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[91]
            encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[92]
            encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_v: R.Tensor((512, 512, 1), dtype="float32") = packed_params[93]
            encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_bias: R.Tensor((512,), dtype="float32") = packed_params[94]
            encoder_block_layers_4_block_layers_1_block_branches_0_layers_0_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[95]
            encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[96]
            encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_weight_v: R.Tensor((512, 512, 7), dtype="float32") = packed_params[97]
            encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_bias: R.Tensor((512,), dtype="float32") = packed_params[98]
            encoder_block_layers_4_block_layers_1_block_branches_0_layers_2_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[99]
            encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[100]
            encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_weight_v: R.Tensor((512, 512, 1), dtype="float32") = packed_params[101]
            encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_bias: R.Tensor((512,), dtype="float32") = packed_params[102]
            encoder_block_layers_4_block_layers_2_block_branches_0_layers_0_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[103]
            encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[104]
            encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_weight_v: R.Tensor((512, 512, 7), dtype="float32") = packed_params[105]
            encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_bias: R.Tensor((512,), dtype="float32") = packed_params[106]
            encoder_block_layers_4_block_layers_2_block_branches_0_layers_2_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[107]
            encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_weight_g: R.Tensor((512, 1, 1), dtype="float32") = packed_params[108]
            encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_weight_v: R.Tensor((512, 512, 1), dtype="float32") = packed_params[109]
            encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_bias: R.Tensor((512,), dtype="float32") = packed_params[110]
            encoder_block_layers_4_block_layers_3_alpha: R.Tensor((1, 512, 1), dtype="float32") = packed_params[111]
            encoder_block_layers_4_block_layers_4_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[112]
            encoder_block_layers_4_block_layers_4_weight_v: R.Tensor((1024, 512, 16), dtype="float32") = packed_params[113]
            encoder_block_layers_4_block_layers_4_bias: R.Tensor((1024,), dtype="float32") = packed_params[114]
            encoder_block_layers_5_alpha: R.Tensor((1, 1024, 1), dtype="float32") = packed_params[115]
            encoder_block_layers_6_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[116]
            encoder_block_layers_6_weight_v: R.Tensor((1024, 1024, 3), dtype="float32") = packed_params[117]
            encoder_block_layers_6_bias: R.Tensor((1024,), dtype="float32") = packed_params[118]
            quantizer_quantizers_0_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[119]
            quantizer_quantizers_0_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[120]
            quantizer_quantizers_0_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[121]
            quantizer_quantizers_0_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[122]
            quantizer_quantizers_0_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[123]
            quantizer_quantizers_0_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[124]
            quantizer_quantizers_0_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[125]
            quantizer_quantizers_1_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[126]
            quantizer_quantizers_1_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[127]
            quantizer_quantizers_1_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[128]
            quantizer_quantizers_1_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[129]
            quantizer_quantizers_1_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[130]
            quantizer_quantizers_1_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[131]
            quantizer_quantizers_1_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[132]
            quantizer_quantizers_2_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[133]
            quantizer_quantizers_2_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[134]
            quantizer_quantizers_2_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[135]
            quantizer_quantizers_2_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[136]
            quantizer_quantizers_2_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[137]
            quantizer_quantizers_2_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[138]
            quantizer_quantizers_2_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[139]
            quantizer_quantizers_3_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[140]
            quantizer_quantizers_3_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[141]
            quantizer_quantizers_3_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[142]
            quantizer_quantizers_3_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[143]
            quantizer_quantizers_3_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[144]
            quantizer_quantizers_3_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[145]
            quantizer_quantizers_3_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[146]
            quantizer_quantizers_4_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[147]
            quantizer_quantizers_4_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[148]
            quantizer_quantizers_4_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[149]
            quantizer_quantizers_4_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[150]
            quantizer_quantizers_4_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[151]
            quantizer_quantizers_4_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[152]
            quantizer_quantizers_4_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[153]
            quantizer_quantizers_5_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[154]
            quantizer_quantizers_5_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[155]
            quantizer_quantizers_5_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[156]
            quantizer_quantizers_5_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[157]
            quantizer_quantizers_5_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[158]
            quantizer_quantizers_5_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[159]
            quantizer_quantizers_5_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[160]
            quantizer_quantizers_6_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[161]
            quantizer_quantizers_6_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[162]
            quantizer_quantizers_6_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[163]
            quantizer_quantizers_6_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[164]
            quantizer_quantizers_6_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[165]
            quantizer_quantizers_6_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[166]
            quantizer_quantizers_6_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[167]
            quantizer_quantizers_7_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[168]
            quantizer_quantizers_7_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[169]
            quantizer_quantizers_7_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[170]
            quantizer_quantizers_7_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[171]
            quantizer_quantizers_7_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[172]
            quantizer_quantizers_7_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[173]
            quantizer_quantizers_7_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[174]
            quantizer_quantizers_8_in_proj_weight_g: R.Tensor((8, 1, 1), dtype="float32") = packed_params[175]
            quantizer_quantizers_8_in_proj_weight_v: R.Tensor((8, 1024, 1), dtype="float32") = packed_params[176]
            quantizer_quantizers_8_in_proj_bias: R.Tensor((8,), dtype="float32") = packed_params[177]
            quantizer_quantizers_8_out_proj_weight_g: R.Tensor((1024, 1, 1), dtype="float32") = packed_params[178]
            quantizer_quantizers_8_out_proj_weight_v: R.Tensor((1024, 8, 1), dtype="float32") = packed_params[179]
            quantizer_quantizers_8_out_proj_bias: R.Tensor((1024,), dtype="float32") = packed_params[180]
            quantizer_quantizers_8_codebook_weight: R.Tensor((1024, 8), dtype="float32") = packed_params[181]
            lv1: R.Tensor((batch_size, 1, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_0_downsampling_delay_cache, audio_data, sinfo_args=(R.Tensor((batch_size, 1, 512), dtype="float32"),))
            lv2: R.Tensor((batch_size, 1, 518), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_0_cache_cache, lv1, sinfo_args=(R.Tensor((batch_size, 1, 518), dtype="float32"),))
            lv90 = R.call_tir(cls.fused_tir_square14_sum14, (encoder_block_layers_0_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv91 = R.call_tir(cls.fused_tir_sqrt6_divide14_multiply14, (lv90, encoder_block_layers_0_weight_v, encoder_block_layers_0_weight_g), out_sinfo=R.Tensor((64, 1, 7), dtype="float32"))
            lv8 = R.call_tir(cls.reshape11, (encoder_block_layers_0_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv92 = R.call_tir(cls.fused_conv1d18_add18, (lv2, lv91, lv8), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv9: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_0_block_paddings_0_cache, lv92, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            reshape = R.call_tir(cls.reshape12, (lv9,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv10 = R.call_tir(cls.snake, (reshape, encoder_block_layers_1_block_layers_0_block_branches_0_layers_0_alpha), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            reshape1 = R.call_tir(cls.reshape12, (lv10,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv11: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, reshape1, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            lv12: R.Tensor((batch_size, 64, 518), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_cache_cache, lv11, sinfo_args=(R.Tensor((batch_size, 64, 518), dtype="float32"),))
            lv93 = R.call_tir(cls.fused_tir_square15_sum15, (encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv94 = R.call_tir(cls.fused_tir_sqrt6_divide15_multiply15, (lv93, encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_v, encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_weight_g), out_sinfo=R.Tensor((64, 64, 7), dtype="float32"))
            lv18 = R.call_tir(cls.reshape11, (encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv95 = R.call_tir(cls.fused_conv1d19_add18, (lv12, lv94, lv18), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            reshape2 = R.call_tir(cls.reshape12, (lv95,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv19 = R.call_tir(cls.snake, (reshape2, encoder_block_layers_1_block_layers_0_block_branches_0_layers_2_alpha), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            reshape3 = R.call_tir(cls.reshape12, (lv19,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv20: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, reshape3, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            lv21: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_cache_cache, lv20, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            lv96 = R.call_tir(cls.fused_tir_square16_sum16, (encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv97 = R.call_tir(cls.fused_tir_sqrt6_divide16_multiply16, (lv96, encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_v, encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_weight_g), out_sinfo=R.Tensor((64, 64, 1), dtype="float32"))
            lv27 = R.call_tir(cls.reshape11, (encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv28: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_0_block_paddings_1_cache, lv92, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            lv98 = R.call_tir(cls.fused_conv1d20_add18_add19_add20, (lv21, lv97, lv27, lv28), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv29: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_1_block_paddings_0_cache, lv98, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            reshape4 = R.call_tir(cls.reshape12, (lv29,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv30 = R.call_tir(cls.snake, (reshape4, encoder_block_layers_1_block_layers_1_block_branches_0_layers_0_alpha), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            reshape5 = R.call_tir(cls.reshape12, (lv30,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv31: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, reshape5, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            lv32: R.Tensor((batch_size, 64, 530), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_cache_cache, lv31, sinfo_args=(R.Tensor((batch_size, 64, 530), dtype="float32"),))
            lv99 = R.call_tir(cls.fused_tir_square15_sum15, (encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv100 = R.call_tir(cls.fused_tir_sqrt6_divide15_multiply15, (lv99, encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_weight_v, encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_weight_g), out_sinfo=R.Tensor((64, 64, 7), dtype="float32"))
            lv38 = R.call_tir(cls.reshape11, (encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv101 = R.call_tir(cls.fused_conv1d21_add18, (lv32, lv100, lv38), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            reshape6 = R.call_tir(cls.reshape12, (lv101,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv39 = R.call_tir(cls.snake, (reshape6, encoder_block_layers_1_block_layers_1_block_branches_0_layers_2_alpha), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            reshape7 = R.call_tir(cls.reshape12, (lv39,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv40: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, reshape7, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            lv41: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_cache_cache, lv40, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            lv102 = R.call_tir(cls.fused_tir_square16_sum16, (encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv103 = R.call_tir(cls.fused_tir_sqrt6_divide16_multiply16, (lv102, encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_weight_v, encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_weight_g), out_sinfo=R.Tensor((64, 64, 1), dtype="float32"))
            lv47 = R.call_tir(cls.reshape11, (encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv48: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_1_block_paddings_1_cache, lv98, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            lv104 = R.call_tir(cls.fused_conv1d20_add18_add19_add20, (lv41, lv103, lv47, lv48), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv49: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_2_block_paddings_0_cache, lv104, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            reshape8 = R.call_tir(cls.reshape12, (lv49,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv50 = R.call_tir(cls.snake, (reshape8, encoder_block_layers_1_block_layers_2_block_branches_0_layers_0_alpha), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            reshape9 = R.call_tir(cls.reshape12, (lv50,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv51: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, reshape9, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            lv52: R.Tensor((batch_size, 64, 566), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache, lv51, sinfo_args=(R.Tensor((batch_size, 64, 566), dtype="float32"),))
            lv105 = R.call_tir(cls.fused_tir_square15_sum15, (encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv106 = R.call_tir(cls.fused_tir_sqrt6_divide15_multiply15, (lv105, encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_weight_v, encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_weight_g), out_sinfo=R.Tensor((64, 64, 7), dtype="float32"))
            lv58 = R.call_tir(cls.reshape11, (encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv107 = R.call_tir(cls.fused_conv1d22_add18, (lv52, lv106, lv58), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            reshape10 = R.call_tir(cls.reshape12, (lv107,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv59 = R.call_tir(cls.snake, (reshape10, encoder_block_layers_1_block_layers_2_block_branches_0_layers_2_alpha), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            reshape11 = R.call_tir(cls.reshape12, (lv59,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv60: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, reshape11, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            lv61: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache, lv60, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            lv108 = R.call_tir(cls.fused_tir_square16_sum16, (encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_weight_v,), out_sinfo=R.Tensor((64, 1, 1), dtype="float32"))
            lv109 = R.call_tir(cls.fused_tir_sqrt6_divide16_multiply16, (lv108, encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_weight_v, encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_weight_g), out_sinfo=R.Tensor((64, 64, 1), dtype="float32"))
            lv67 = R.call_tir(cls.reshape11, (encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_bias,), out_sinfo=R.Tensor((1, 64, 1), dtype="float32"))
            lv68: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_2_block_paddings_1_cache, lv104, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            lv110 = R.call_tir(cls.fused_conv1d20_add18_add19_add20, (lv61, lv109, lv67, lv68), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            reshape12 = R.call_tir(cls.reshape12, (lv110,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv69 = R.call_tir(cls.snake, (reshape12, encoder_block_layers_1_block_layers_3_alpha), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            reshape13 = R.call_tir(cls.reshape12, (lv69,), out_sinfo=R.Tensor((batch_size, 64, 512), dtype="float32"))
            lv70: R.Tensor((batch_size, 64, 512), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_4_downsampling_delay_cache, reshape13, sinfo_args=(R.Tensor((batch_size, 64, 512), dtype="float32"),))
            lv71: R.Tensor((batch_size, 64, 514), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_1_block_layers_4_cache_cache, lv70, sinfo_args=(R.Tensor((batch_size, 64, 514), dtype="float32"),))
            lv111 = R.call_tir(cls.fused_tir_square17_sum17, (encoder_block_layers_1_block_layers_4_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv112 = R.call_tir(cls.fused_tir_sqrt7_divide17_multiply17, (lv111, encoder_block_layers_1_block_layers_4_weight_v, encoder_block_layers_1_block_layers_4_weight_g), out_sinfo=R.Tensor((128, 64, 4), dtype="float32"))
            lv77 = R.call_tir(cls.reshape13, (encoder_block_layers_1_block_layers_4_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv113 = R.call_tir(cls.fused_conv1d23_add21, (lv71, lv112, lv77), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv78: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_0_block_paddings_0_cache, lv113, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            reshape14 = R.call_tir(cls.reshape14, (lv78,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv79 = R.call_tir(cls.snake1, (reshape14, encoder_block_layers_2_block_layers_0_block_branches_0_layers_0_alpha), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            reshape15 = R.call_tir(cls.reshape14, (lv79,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv80: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, reshape15, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            lv81: R.Tensor((batch_size, 128, 262), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_cache_cache, lv80, sinfo_args=(R.Tensor((batch_size, 128, 262), dtype="float32"),))
            lv114 = R.call_tir(cls.fused_tir_square18_sum18, (encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv115 = R.call_tir(cls.fused_tir_sqrt7_divide18_multiply18, (lv114, encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_v, encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_weight_g), out_sinfo=R.Tensor((128, 128, 7), dtype="float32"))
            lv87 = R.call_tir(cls.reshape13, (encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv116 = R.call_tir(cls.fused_conv1d24_add21, (lv81, lv115, lv87), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            reshape16 = R.call_tir(cls.reshape14, (lv116,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv88 = R.call_tir(cls.snake1, (reshape16, encoder_block_layers_2_block_layers_0_block_branches_0_layers_2_alpha), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            reshape17 = R.call_tir(cls.reshape14, (lv88,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv89: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, reshape17, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            lv90_1: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_cache_cache, lv89, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            lv117 = R.call_tir(cls.fused_tir_square19_sum19, (encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv118 = R.call_tir(cls.fused_tir_sqrt7_divide19_multiply19, (lv117, encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_v, encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_weight_g), out_sinfo=R.Tensor((128, 128, 1), dtype="float32"))
            lv96_1 = R.call_tir(cls.reshape13, (encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv97_1: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_0_block_paddings_1_cache, lv113, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            lv119 = R.call_tir(cls.fused_conv1d25_add21_add22_add23, (lv90_1, lv118, lv96_1, lv97_1), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv98_1: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_1_block_paddings_0_cache, lv119, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            reshape18 = R.call_tir(cls.reshape14, (lv98_1,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv99_1 = R.call_tir(cls.snake1, (reshape18, encoder_block_layers_2_block_layers_1_block_branches_0_layers_0_alpha), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            reshape19 = R.call_tir(cls.reshape14, (lv99_1,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv100_1: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, reshape19, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            lv101_1: R.Tensor((batch_size, 128, 274), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_cache_cache, lv100_1, sinfo_args=(R.Tensor((batch_size, 128, 274), dtype="float32"),))
            lv120 = R.call_tir(cls.fused_tir_square18_sum18, (encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv121 = R.call_tir(cls.fused_tir_sqrt7_divide18_multiply18, (lv120, encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_weight_v, encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_weight_g), out_sinfo=R.Tensor((128, 128, 7), dtype="float32"))
            lv107_1 = R.call_tir(cls.reshape13, (encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv122 = R.call_tir(cls.fused_conv1d26_add21, (lv101_1, lv121, lv107_1), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            reshape20 = R.call_tir(cls.reshape14, (lv122,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv108_1 = R.call_tir(cls.snake1, (reshape20, encoder_block_layers_2_block_layers_1_block_branches_0_layers_2_alpha), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            reshape21 = R.call_tir(cls.reshape14, (lv108_1,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv109_1: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, reshape21, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            lv110_1: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_cache_cache, lv109_1, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            lv123 = R.call_tir(cls.fused_tir_square19_sum19, (encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv124 = R.call_tir(cls.fused_tir_sqrt7_divide19_multiply19, (lv123, encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_weight_v, encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_weight_g), out_sinfo=R.Tensor((128, 128, 1), dtype="float32"))
            lv116_1 = R.call_tir(cls.reshape13, (encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv117_1: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_1_block_paddings_1_cache, lv119, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            lv125 = R.call_tir(cls.fused_conv1d25_add21_add22_add23, (lv110_1, lv124, lv116_1, lv117_1), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv118_1: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_2_block_paddings_0_cache, lv125, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            reshape22 = R.call_tir(cls.reshape14, (lv118_1,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv119_1 = R.call_tir(cls.snake1, (reshape22, encoder_block_layers_2_block_layers_2_block_branches_0_layers_0_alpha), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            reshape23 = R.call_tir(cls.reshape14, (lv119_1,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv120_1: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, reshape23, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            lv121_1: R.Tensor((batch_size, 128, 310), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache, lv120_1, sinfo_args=(R.Tensor((batch_size, 128, 310), dtype="float32"),))
            lv126 = R.call_tir(cls.fused_tir_square18_sum18, (encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv127 = R.call_tir(cls.fused_tir_sqrt7_divide18_multiply18, (lv126, encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_weight_v, encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_weight_g), out_sinfo=R.Tensor((128, 128, 7), dtype="float32"))
            lv127_1 = R.call_tir(cls.reshape13, (encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv128 = R.call_tir(cls.fused_conv1d27_add21, (lv121_1, lv127, lv127_1), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            reshape24 = R.call_tir(cls.reshape14, (lv128,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv128_1 = R.call_tir(cls.snake1, (reshape24, encoder_block_layers_2_block_layers_2_block_branches_0_layers_2_alpha), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            reshape25 = R.call_tir(cls.reshape14, (lv128_1,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv129: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, reshape25, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            lv130: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache, lv129, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            lv129_1 = R.call_tir(cls.fused_tir_square19_sum19, (encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_weight_v,), out_sinfo=R.Tensor((128, 1, 1), dtype="float32"))
            lv130_1 = R.call_tir(cls.fused_tir_sqrt7_divide19_multiply19, (lv129_1, encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_weight_v, encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_weight_g), out_sinfo=R.Tensor((128, 128, 1), dtype="float32"))
            lv136 = R.call_tir(cls.reshape13, (encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_bias,), out_sinfo=R.Tensor((1, 128, 1), dtype="float32"))
            lv137: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_2_block_paddings_1_cache, lv125, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            lv131 = R.call_tir(cls.fused_conv1d25_add21_add22_add23, (lv130, lv130_1, lv136, lv137), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            reshape26 = R.call_tir(cls.reshape14, (lv131,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv138 = R.call_tir(cls.snake1, (reshape26, encoder_block_layers_2_block_layers_3_alpha), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            reshape27 = R.call_tir(cls.reshape14, (lv138,), out_sinfo=R.Tensor((batch_size, 128, 256), dtype="float32"))
            lv139: R.Tensor((batch_size, 128, 256), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_4_downsampling_delay_cache, reshape27, sinfo_args=(R.Tensor((batch_size, 128, 256), dtype="float32"),))
            lv140: R.Tensor((batch_size, 128, 260), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_2_block_layers_4_cache_cache, lv139, sinfo_args=(R.Tensor((batch_size, 128, 260), dtype="float32"),))
            lv132 = R.call_tir(cls.fused_tir_square20_sum20, (encoder_block_layers_2_block_layers_4_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv133 = R.call_tir(cls.fused_tir_sqrt8_divide20_multiply20, (lv132, encoder_block_layers_2_block_layers_4_weight_v, encoder_block_layers_2_block_layers_4_weight_g), out_sinfo=R.Tensor((256, 128, 8), dtype="float32"))
            lv146 = R.call_tir(cls.reshape15, (encoder_block_layers_2_block_layers_4_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv134 = R.call_tir(cls.fused_conv1d28_add24, (lv140, lv133, lv146), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv147: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_0_block_paddings_0_cache, lv134, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            reshape28 = R.call_tir(cls.reshape16, (lv147,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv148 = R.call_tir(cls.snake2, (reshape28, encoder_block_layers_3_block_layers_0_block_branches_0_layers_0_alpha), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            reshape29 = R.call_tir(cls.reshape16, (lv148,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv149: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, reshape29, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            lv150: R.Tensor((batch_size, 256, 70), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_cache_cache, lv149, sinfo_args=(R.Tensor((batch_size, 256, 70), dtype="float32"),))
            lv135 = R.call_tir(cls.fused_tir_square21_sum21, (encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv136_1 = R.call_tir(cls.fused_tir_sqrt8_divide21_multiply21, (lv135, encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_v, encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_weight_g), out_sinfo=R.Tensor((256, 256, 7), dtype="float32"))
            lv156 = R.call_tir(cls.reshape15, (encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv137_1 = R.call_tir(cls.fused_conv1d29_add24, (lv150, lv136_1, lv156), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            reshape30 = R.call_tir(cls.reshape16, (lv137_1,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv157 = R.call_tir(cls.snake2, (reshape30, encoder_block_layers_3_block_layers_0_block_branches_0_layers_2_alpha), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            reshape31 = R.call_tir(cls.reshape16, (lv157,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv158: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, reshape31, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            lv159: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_cache_cache, lv158, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            lv138_1 = R.call_tir(cls.fused_tir_square22_sum22, (encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv139_1 = R.call_tir(cls.fused_tir_sqrt8_divide22_multiply22, (lv138_1, encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_v, encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_weight_g), out_sinfo=R.Tensor((256, 256, 1), dtype="float32"))
            lv165 = R.call_tir(cls.reshape15, (encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv166: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_0_block_paddings_1_cache, lv134, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            lv140_1 = R.call_tir(cls.fused_conv1d30_add24_add25_add26, (lv159, lv139_1, lv165, lv166), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv167: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_1_block_paddings_0_cache, lv140_1, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            reshape32 = R.call_tir(cls.reshape16, (lv167,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv168 = R.call_tir(cls.snake2, (reshape32, encoder_block_layers_3_block_layers_1_block_branches_0_layers_0_alpha), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            reshape33 = R.call_tir(cls.reshape16, (lv168,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv169: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, reshape33, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            lv170: R.Tensor((batch_size, 256, 82), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_cache_cache, lv169, sinfo_args=(R.Tensor((batch_size, 256, 82), dtype="float32"),))
            lv141 = R.call_tir(cls.fused_tir_square21_sum21, (encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv142 = R.call_tir(cls.fused_tir_sqrt8_divide21_multiply21, (lv141, encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_weight_v, encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_weight_g), out_sinfo=R.Tensor((256, 256, 7), dtype="float32"))
            lv176 = R.call_tir(cls.reshape15, (encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv143 = R.call_tir(cls.fused_conv1d31_add24, (lv170, lv142, lv176), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            reshape34 = R.call_tir(cls.reshape16, (lv143,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv177 = R.call_tir(cls.snake2, (reshape34, encoder_block_layers_3_block_layers_1_block_branches_0_layers_2_alpha), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            reshape35 = R.call_tir(cls.reshape16, (lv177,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv178: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, reshape35, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            lv179: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_cache_cache, lv178, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            lv144 = R.call_tir(cls.fused_tir_square22_sum22, (encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv145 = R.call_tir(cls.fused_tir_sqrt8_divide22_multiply22, (lv144, encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_weight_v, encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_weight_g), out_sinfo=R.Tensor((256, 256, 1), dtype="float32"))
            lv185 = R.call_tir(cls.reshape15, (encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv186: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_1_block_paddings_1_cache, lv140_1, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            lv146_1 = R.call_tir(cls.fused_conv1d30_add24_add25_add26, (lv179, lv145, lv185, lv186), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv187: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_2_block_paddings_0_cache, lv146_1, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            reshape36 = R.call_tir(cls.reshape16, (lv187,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv188 = R.call_tir(cls.snake2, (reshape36, encoder_block_layers_3_block_layers_2_block_branches_0_layers_0_alpha), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            reshape37 = R.call_tir(cls.reshape16, (lv188,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv189: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, reshape37, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            lv190: R.Tensor((batch_size, 256, 118), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache, lv189, sinfo_args=(R.Tensor((batch_size, 256, 118), dtype="float32"),))
            lv147_1 = R.call_tir(cls.fused_tir_square21_sum21, (encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv148_1 = R.call_tir(cls.fused_tir_sqrt8_divide21_multiply21, (lv147_1, encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_weight_v, encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_weight_g), out_sinfo=R.Tensor((256, 256, 7), dtype="float32"))
            lv196 = R.call_tir(cls.reshape15, (encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv149_1 = R.call_tir(cls.fused_conv1d32_add24, (lv190, lv148_1, lv196), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            reshape38 = R.call_tir(cls.reshape16, (lv149_1,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv197 = R.call_tir(cls.snake2, (reshape38, encoder_block_layers_3_block_layers_2_block_branches_0_layers_2_alpha), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            reshape39 = R.call_tir(cls.reshape16, (lv197,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv198: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, reshape39, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            lv199: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache, lv198, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            lv150_1 = R.call_tir(cls.fused_tir_square22_sum22, (encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_weight_v,), out_sinfo=R.Tensor((256, 1, 1), dtype="float32"))
            lv151 = R.call_tir(cls.fused_tir_sqrt8_divide22_multiply22, (lv150_1, encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_weight_v, encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_weight_g), out_sinfo=R.Tensor((256, 256, 1), dtype="float32"))
            lv205 = R.call_tir(cls.reshape15, (encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_bias,), out_sinfo=R.Tensor((1, 256, 1), dtype="float32"))
            lv206: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_2_block_paddings_1_cache, lv146_1, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            lv152 = R.call_tir(cls.fused_conv1d30_add24_add25_add26, (lv199, lv151, lv205, lv206), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            reshape40 = R.call_tir(cls.reshape16, (lv152,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv207 = R.call_tir(cls.snake2, (reshape40, encoder_block_layers_3_block_layers_3_alpha), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            reshape41 = R.call_tir(cls.reshape16, (lv207,), out_sinfo=R.Tensor((batch_size, 256, 64), dtype="float32"))
            lv208: R.Tensor((batch_size, 256, 64), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_4_downsampling_delay_cache, reshape41, sinfo_args=(R.Tensor((batch_size, 256, 64), dtype="float32"),))
            lv209: R.Tensor((batch_size, 256, 72), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_3_block_layers_4_cache_cache, lv208, sinfo_args=(R.Tensor((batch_size, 256, 72), dtype="float32"),))
            lv153 = R.call_tir(cls.fused_tir_square23_sum23, (encoder_block_layers_3_block_layers_4_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv154 = R.call_tir(cls.fused_tir_sqrt9_divide23_multiply23, (lv153, encoder_block_layers_3_block_layers_4_weight_v, encoder_block_layers_3_block_layers_4_weight_g), out_sinfo=R.Tensor((512, 256, 16), dtype="float32"))
            lv215 = R.call_tir(cls.reshape17, (encoder_block_layers_3_block_layers_4_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv155 = R.call_tir(cls.fused_conv1d33_add27, (lv209, lv154, lv215), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv216: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_0_block_paddings_0_cache, lv155, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            reshape42 = R.call_tir(cls.reshape18, (lv216,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv217 = R.call_tir(cls.snake3, (reshape42, encoder_block_layers_4_block_layers_0_block_branches_0_layers_0_alpha), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            reshape43 = R.call_tir(cls.reshape18, (lv217,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv218: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, reshape43, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            lv219: R.Tensor((batch_size, 512, 14), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_cache_cache, lv218, sinfo_args=(R.Tensor((batch_size, 512, 14), dtype="float32"),))
            lv156_1 = R.call_tir(cls.fused_tir_square24_sum24, (encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv157_1 = R.call_tir(cls.fused_tir_sqrt9_divide24_multiply24, (lv156_1, encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_v, encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_weight_g), out_sinfo=R.Tensor((512, 512, 7), dtype="float32"))
            lv225 = R.call_tir(cls.reshape17, (encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv158_1 = R.call_tir(cls.fused_conv1d34_add27, (lv219, lv157_1, lv225), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            reshape44 = R.call_tir(cls.reshape18, (lv158_1,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv226 = R.call_tir(cls.snake3, (reshape44, encoder_block_layers_4_block_layers_0_block_branches_0_layers_2_alpha), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            reshape45 = R.call_tir(cls.reshape18, (lv226,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv227: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, reshape45, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            lv228: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_cache_cache, lv227, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            lv159_1 = R.call_tir(cls.fused_tir_square25_sum25, (encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv160 = R.call_tir(cls.fused_tir_sqrt9_divide25_multiply25, (lv159_1, encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_v, encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_weight_g), out_sinfo=R.Tensor((512, 512, 1), dtype="float32"))
            lv234 = R.call_tir(cls.reshape17, (encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv235: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_0_block_paddings_1_cache, lv155, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            lv161 = R.call_tir(cls.fused_conv1d35_add27_add28_add29, (lv228, lv160, lv234, lv235), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv236: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_1_block_paddings_0_cache, lv161, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            reshape46 = R.call_tir(cls.reshape18, (lv236,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv237 = R.call_tir(cls.snake3, (reshape46, encoder_block_layers_4_block_layers_1_block_branches_0_layers_0_alpha), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            reshape47 = R.call_tir(cls.reshape18, (lv237,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv238: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, reshape47, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            lv239: R.Tensor((batch_size, 512, 26), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_cache_cache, lv238, sinfo_args=(R.Tensor((batch_size, 512, 26), dtype="float32"),))
            lv162 = R.call_tir(cls.fused_tir_square24_sum24, (encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv163 = R.call_tir(cls.fused_tir_sqrt9_divide24_multiply24, (lv162, encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_weight_v, encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_weight_g), out_sinfo=R.Tensor((512, 512, 7), dtype="float32"))
            lv245 = R.call_tir(cls.reshape17, (encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv164 = R.call_tir(cls.fused_conv1d36_add27, (lv239, lv163, lv245), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            reshape48 = R.call_tir(cls.reshape18, (lv164,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv246 = R.call_tir(cls.snake3, (reshape48, encoder_block_layers_4_block_layers_1_block_branches_0_layers_2_alpha), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            reshape49 = R.call_tir(cls.reshape18, (lv246,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv247: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, reshape49, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            lv248: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_cache_cache, lv247, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            lv165_1 = R.call_tir(cls.fused_tir_square25_sum25, (encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv166_1 = R.call_tir(cls.fused_tir_sqrt9_divide25_multiply25, (lv165_1, encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_weight_v, encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_weight_g), out_sinfo=R.Tensor((512, 512, 1), dtype="float32"))
            lv254 = R.call_tir(cls.reshape17, (encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv255: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_1_block_paddings_1_cache, lv161, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            lv167_1 = R.call_tir(cls.fused_conv1d35_add27_add28_add29, (lv248, lv166_1, lv254, lv255), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv256: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_2_block_paddings_0_cache, lv167_1, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            reshape50 = R.call_tir(cls.reshape18, (lv256,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv257 = R.call_tir(cls.snake3, (reshape50, encoder_block_layers_4_block_layers_2_block_branches_0_layers_0_alpha), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            reshape51 = R.call_tir(cls.reshape18, (lv257,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv258: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, reshape51, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            lv259: R.Tensor((batch_size, 512, 62), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache, lv258, sinfo_args=(R.Tensor((batch_size, 512, 62), dtype="float32"),))
            lv168_1 = R.call_tir(cls.fused_tir_square24_sum24, (encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv169_1 = R.call_tir(cls.fused_tir_sqrt9_divide24_multiply24, (lv168_1, encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_weight_v, encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_weight_g), out_sinfo=R.Tensor((512, 512, 7), dtype="float32"))
            lv265 = R.call_tir(cls.reshape17, (encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv170_1 = R.call_tir(cls.fused_conv1d37_add27, (lv259, lv169_1, lv265), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            reshape52 = R.call_tir(cls.reshape18, (lv170_1,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv266 = R.call_tir(cls.snake3, (reshape52, encoder_block_layers_4_block_layers_2_block_branches_0_layers_2_alpha), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            reshape53 = R.call_tir(cls.reshape18, (lv266,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv267: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, reshape53, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            lv268: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache, lv267, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            lv171 = R.call_tir(cls.fused_tir_square25_sum25, (encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_weight_v,), out_sinfo=R.Tensor((512, 1, 1), dtype="float32"))
            lv172 = R.call_tir(cls.fused_tir_sqrt9_divide25_multiply25, (lv171, encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_weight_v, encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_weight_g), out_sinfo=R.Tensor((512, 512, 1), dtype="float32"))
            lv274 = R.call_tir(cls.reshape17, (encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_bias,), out_sinfo=R.Tensor((1, 512, 1), dtype="float32"))
            lv275: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_2_block_paddings_1_cache, lv167_1, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            lv173 = R.call_tir(cls.fused_conv1d35_add27_add28_add29, (lv268, lv172, lv274, lv275), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            reshape54 = R.call_tir(cls.reshape18, (lv173,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv276 = R.call_tir(cls.snake3, (reshape54, encoder_block_layers_4_block_layers_3_alpha), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            reshape55 = R.call_tir(cls.reshape18, (lv276,), out_sinfo=R.Tensor((batch_size, 512, 8), dtype="float32"))
            lv277: R.Tensor((batch_size, 512, 8), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_4_downsampling_delay_cache, reshape55, sinfo_args=(R.Tensor((batch_size, 512, 8), dtype="float32"),))
            lv278: R.Tensor((batch_size, 512, 16), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_4_block_layers_4_cache_cache, lv277, sinfo_args=(R.Tensor((batch_size, 512, 16), dtype="float32"),))
            lv174 = R.call_tir(cls.fused_tir_square26_sum26, (encoder_block_layers_4_block_layers_4_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv175 = R.call_tir(cls.fused_tir_sqrt10_divide26_multiply26, (lv174, encoder_block_layers_4_block_layers_4_weight_v, encoder_block_layers_4_block_layers_4_weight_g), out_sinfo=R.Tensor((1024, 512, 16), dtype="float32"))
            lv284 = R.call_tir(cls.reshape19, (encoder_block_layers_4_block_layers_4_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv176_1 = R.call_tir(cls.fused_conv1d38_add30, (lv278, lv175, lv284), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            reshape56 = R.call_tir(cls.reshape20, (lv176_1,), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            lv285 = R.call_tir(cls.snake4, (reshape56, encoder_block_layers_5_alpha), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            reshape57 = R.call_tir(cls.reshape20, (lv285,), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            lv286: R.Tensor((batch_size, 1024, 1), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_6_downsampling_delay_cache, reshape57, sinfo_args=(R.Tensor((batch_size, 1024, 1), dtype="float32"),))
            lv287: R.Tensor((batch_size, 1024, 3), dtype="float32") = R.call_pure_packed("vm.builtin.cached_padding_1d_update", encoder_block_layers_6_cache_cache, lv286, sinfo_args=(R.Tensor((batch_size, 1024, 3), dtype="float32"),))
            lv177_1 = R.call_tir(cls.fused_tir_square27_sum27, (encoder_block_layers_6_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv178_1 = R.call_tir(cls.fused_tir_sqrt10_divide27_multiply27, (lv177_1, encoder_block_layers_6_weight_v, encoder_block_layers_6_weight_g), out_sinfo=R.Tensor((1024, 1024, 3), dtype="float32"))
            lv293 = R.call_tir(cls.reshape19, (encoder_block_layers_6_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv179_1 = R.call_tir(cls.fused_conv1d39_add30, (lv287, lv178_1, lv293), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            lv180 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_0_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv181 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv180, quantizer_quantizers_0_in_proj_weight_v, quantizer_quantizers_0_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv299 = R.call_tir(cls.reshape21, (quantizer_quantizers_0_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv182 = R.call_tir(cls.fused_conv1d40_add31, (lv179_1, lv181, lv299), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            permute_dims = R.call_tir(cls.transpose, (lv182,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            reshape58 = R.call_tir(cls.reshape22, (permute_dims,), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv183 = R.call_tir(cls.fused_tir_square29_sum29, (reshape58,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv184 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt12_divide29, (lv183, reshape58), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv185_1 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_0_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv186_1 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv185_1, quantizer_quantizers_0_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims1 = R.call_tir(cls.transpose1, (lv186_1,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv187_1 = R.call_tir(cls.fused_tir_square29_sum29, (lv184,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv188_1 = R.call_tir(cls.fused_tir_square30_sum30, (lv186_1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims2 = R.call_tir(cls.transpose2, (lv188_1,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv189_1 = R.call_tir(cls.fused_matmul_multiply29_subtract_add32, (lv184, permute_dims1, lv187_1, permute_dims2), out_sinfo=R.Tensor((batch_size, 1024), dtype="float32"))
            argsort: R.Tensor((batch_size, 1024), dtype="int32") = R.argsort(lv189_1, axis=1, descending=False, dtype="int32")
            take = R.call_tir(cls.take, (argsort, metadata["relax.expr.Constant"][0]), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape59 = R.call_tir(cls.reshape23, (take,), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape60 = R.call_tir(cls.reshape24, (reshape59,), out_sinfo=R.Tensor((batch_size,), dtype="int32"))
            take1 = R.call_tir(cls.take1, (quantizer_quantizers_0_codebook_weight, reshape60), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            reshape61 = R.call_tir(cls.reshape25, (take1,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            permute_dims3 = R.call_tir(cls.transpose3, (reshape61,), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            lv190_1 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_0_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv191 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv190_1, quantizer_quantizers_0_out_proj_weight_v, quantizer_quantizers_0_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv305 = R.call_tir(cls.reshape19, (quantizer_quantizers_0_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv192 = R.call_tir(cls.fused_conv1d41_add30, (permute_dims3, lv191, lv305), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            subtract1 = R.call_tir(cls.subtract1, (lv179_1, lv192), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            lv193 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_1_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv194 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv193, quantizer_quantizers_1_in_proj_weight_v, quantizer_quantizers_1_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv311 = R.call_tir(cls.reshape21, (quantizer_quantizers_1_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv195 = R.call_tir(cls.fused_conv1d40_add31, (subtract1, lv194, lv311), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            permute_dims4 = R.call_tir(cls.transpose, (lv195,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            reshape62 = R.call_tir(cls.reshape22, (permute_dims4,), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv196_1 = R.call_tir(cls.fused_tir_square29_sum29, (reshape62,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv197_1 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt12_divide29, (lv196_1, reshape62), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv198_1 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_1_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv199_1 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv198_1, quantizer_quantizers_1_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims5 = R.call_tir(cls.transpose1, (lv199_1,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv200 = R.call_tir(cls.fused_tir_square29_sum29, (lv197_1,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv201 = R.call_tir(cls.fused_tir_square30_sum30, (lv199_1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims6 = R.call_tir(cls.transpose2, (lv201,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv202 = R.call_tir(cls.fused_matmul_multiply29_subtract_add32, (lv197_1, permute_dims5, lv200, permute_dims6), out_sinfo=R.Tensor((batch_size, 1024), dtype="float32"))
            argsort1: R.Tensor((batch_size, 1024), dtype="int32") = R.argsort(lv202, axis=1, descending=False, dtype="int32")
            take2 = R.call_tir(cls.take, (argsort1, metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape63 = R.call_tir(cls.reshape23, (take2,), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape64 = R.call_tir(cls.reshape24, (reshape63,), out_sinfo=R.Tensor((batch_size,), dtype="int32"))
            take3 = R.call_tir(cls.take1, (quantizer_quantizers_1_codebook_weight, reshape64), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            reshape65 = R.call_tir(cls.reshape25, (take3,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            permute_dims7 = R.call_tir(cls.transpose3, (reshape65,), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            lv203 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_1_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv204 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv203, quantizer_quantizers_1_out_proj_weight_v, quantizer_quantizers_1_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv317 = R.call_tir(cls.reshape19, (quantizer_quantizers_1_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv205_1 = R.call_tir(cls.fused_conv1d41_add30, (permute_dims7, lv204, lv317), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            subtract3 = R.call_tir(cls.subtract1, (subtract1, lv205_1), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            lv206_1 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_2_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv207_1 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv206_1, quantizer_quantizers_2_in_proj_weight_v, quantizer_quantizers_2_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv323 = R.call_tir(cls.reshape21, (quantizer_quantizers_2_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv208_1 = R.call_tir(cls.fused_conv1d40_add31, (subtract3, lv207_1, lv323), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            permute_dims8 = R.call_tir(cls.transpose, (lv208_1,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            reshape66 = R.call_tir(cls.reshape22, (permute_dims8,), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv209_1 = R.call_tir(cls.fused_tir_square29_sum29, (reshape66,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv210 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt12_divide29, (lv209_1, reshape66), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv211 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_2_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv212 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv211, quantizer_quantizers_2_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims9 = R.call_tir(cls.transpose1, (lv212,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv213 = R.call_tir(cls.fused_tir_square29_sum29, (lv210,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv214 = R.call_tir(cls.fused_tir_square30_sum30, (lv212,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims10 = R.call_tir(cls.transpose2, (lv214,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv215_1 = R.call_tir(cls.fused_matmul_multiply29_subtract_add32, (lv210, permute_dims9, lv213, permute_dims10), out_sinfo=R.Tensor((batch_size, 1024), dtype="float32"))
            argsort2: R.Tensor((batch_size, 1024), dtype="int32") = R.argsort(lv215_1, axis=1, descending=False, dtype="int32")
            take4 = R.call_tir(cls.take, (argsort2, metadata["relax.expr.Constant"][2]), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape67 = R.call_tir(cls.reshape23, (take4,), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape68 = R.call_tir(cls.reshape24, (reshape67,), out_sinfo=R.Tensor((batch_size,), dtype="int32"))
            take5 = R.call_tir(cls.take1, (quantizer_quantizers_2_codebook_weight, reshape68), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            reshape69 = R.call_tir(cls.reshape25, (take5,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            permute_dims11 = R.call_tir(cls.transpose3, (reshape69,), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            lv216_1 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_2_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv217_1 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv216_1, quantizer_quantizers_2_out_proj_weight_v, quantizer_quantizers_2_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv329 = R.call_tir(cls.reshape19, (quantizer_quantizers_2_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv218_1 = R.call_tir(cls.fused_conv1d41_add30, (permute_dims11, lv217_1, lv329), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            subtract5 = R.call_tir(cls.subtract1, (subtract3, lv218_1), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            lv219_1 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_3_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv220 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv219_1, quantizer_quantizers_3_in_proj_weight_v, quantizer_quantizers_3_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv335 = R.call_tir(cls.reshape21, (quantizer_quantizers_3_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv221 = R.call_tir(cls.fused_conv1d40_add31, (subtract5, lv220, lv335), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            permute_dims12 = R.call_tir(cls.transpose, (lv221,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            reshape70 = R.call_tir(cls.reshape22, (permute_dims12,), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv222 = R.call_tir(cls.fused_tir_square29_sum29, (reshape70,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv223 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt12_divide29, (lv222, reshape70), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv224 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_3_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv225_1 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv224, quantizer_quantizers_3_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims13 = R.call_tir(cls.transpose1, (lv225_1,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv226_1 = R.call_tir(cls.fused_tir_square29_sum29, (lv223,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv227_1 = R.call_tir(cls.fused_tir_square30_sum30, (lv225_1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims14 = R.call_tir(cls.transpose2, (lv227_1,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv228_1 = R.call_tir(cls.fused_matmul_multiply29_subtract_add32, (lv223, permute_dims13, lv226_1, permute_dims14), out_sinfo=R.Tensor((batch_size, 1024), dtype="float32"))
            argsort3: R.Tensor((batch_size, 1024), dtype="int32") = R.argsort(lv228_1, axis=1, descending=False, dtype="int32")
            take6 = R.call_tir(cls.take, (argsort3, metadata["relax.expr.Constant"][3]), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape71 = R.call_tir(cls.reshape23, (take6,), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape72 = R.call_tir(cls.reshape24, (reshape71,), out_sinfo=R.Tensor((batch_size,), dtype="int32"))
            take7 = R.call_tir(cls.take1, (quantizer_quantizers_3_codebook_weight, reshape72), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            reshape73 = R.call_tir(cls.reshape25, (take7,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            permute_dims15 = R.call_tir(cls.transpose3, (reshape73,), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            lv229 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_3_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv230 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv229, quantizer_quantizers_3_out_proj_weight_v, quantizer_quantizers_3_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv341 = R.call_tir(cls.reshape19, (quantizer_quantizers_3_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv231 = R.call_tir(cls.fused_conv1d41_add30, (permute_dims15, lv230, lv341), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            subtract7 = R.call_tir(cls.subtract1, (subtract5, lv231), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            lv232 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_4_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv233 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv232, quantizer_quantizers_4_in_proj_weight_v, quantizer_quantizers_4_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv347 = R.call_tir(cls.reshape21, (quantizer_quantizers_4_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv234_1 = R.call_tir(cls.fused_conv1d40_add31, (subtract7, lv233, lv347), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            permute_dims16 = R.call_tir(cls.transpose, (lv234_1,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            reshape74 = R.call_tir(cls.reshape22, (permute_dims16,), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv235_1 = R.call_tir(cls.fused_tir_square29_sum29, (reshape74,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv236_1 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt12_divide29, (lv235_1, reshape74), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv237_1 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_4_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv238_1 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv237_1, quantizer_quantizers_4_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims17 = R.call_tir(cls.transpose1, (lv238_1,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv239_1 = R.call_tir(cls.fused_tir_square29_sum29, (lv236_1,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv240 = R.call_tir(cls.fused_tir_square30_sum30, (lv238_1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims18 = R.call_tir(cls.transpose2, (lv240,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv241 = R.call_tir(cls.fused_matmul_multiply29_subtract_add32, (lv236_1, permute_dims17, lv239_1, permute_dims18), out_sinfo=R.Tensor((batch_size, 1024), dtype="float32"))
            argsort4: R.Tensor((batch_size, 1024), dtype="int32") = R.argsort(lv241, axis=1, descending=False, dtype="int32")
            take8 = R.call_tir(cls.take, (argsort4, metadata["relax.expr.Constant"][4]), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape75 = R.call_tir(cls.reshape23, (take8,), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape76 = R.call_tir(cls.reshape24, (reshape75,), out_sinfo=R.Tensor((batch_size,), dtype="int32"))
            take9 = R.call_tir(cls.take1, (quantizer_quantizers_4_codebook_weight, reshape76), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            reshape77 = R.call_tir(cls.reshape25, (take9,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            permute_dims19 = R.call_tir(cls.transpose3, (reshape77,), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            lv242 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_4_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv243 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv242, quantizer_quantizers_4_out_proj_weight_v, quantizer_quantizers_4_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv353 = R.call_tir(cls.reshape19, (quantizer_quantizers_4_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv244 = R.call_tir(cls.fused_conv1d41_add30, (permute_dims19, lv243, lv353), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            subtract9 = R.call_tir(cls.subtract1, (subtract7, lv244), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            lv245_1 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_5_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv246_1 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv245_1, quantizer_quantizers_5_in_proj_weight_v, quantizer_quantizers_5_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv359 = R.call_tir(cls.reshape21, (quantizer_quantizers_5_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv247_1 = R.call_tir(cls.fused_conv1d40_add31, (subtract9, lv246_1, lv359), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            permute_dims20 = R.call_tir(cls.transpose, (lv247_1,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            reshape78 = R.call_tir(cls.reshape22, (permute_dims20,), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv248_1 = R.call_tir(cls.fused_tir_square29_sum29, (reshape78,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv249 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt12_divide29, (lv248_1, reshape78), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv250 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_5_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv251 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv250, quantizer_quantizers_5_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims21 = R.call_tir(cls.transpose1, (lv251,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv252 = R.call_tir(cls.fused_tir_square29_sum29, (lv249,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv253 = R.call_tir(cls.fused_tir_square30_sum30, (lv251,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims22 = R.call_tir(cls.transpose2, (lv253,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv254_1 = R.call_tir(cls.fused_matmul_multiply29_subtract_add32, (lv249, permute_dims21, lv252, permute_dims22), out_sinfo=R.Tensor((batch_size, 1024), dtype="float32"))
            argsort5: R.Tensor((batch_size, 1024), dtype="int32") = R.argsort(lv254_1, axis=1, descending=False, dtype="int32")
            take10 = R.call_tir(cls.take, (argsort5, metadata["relax.expr.Constant"][5]), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape79 = R.call_tir(cls.reshape23, (take10,), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape80 = R.call_tir(cls.reshape24, (reshape79,), out_sinfo=R.Tensor((batch_size,), dtype="int32"))
            take11 = R.call_tir(cls.take1, (quantizer_quantizers_5_codebook_weight, reshape80), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            reshape81 = R.call_tir(cls.reshape25, (take11,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            permute_dims23 = R.call_tir(cls.transpose3, (reshape81,), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            lv255_1 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_5_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv256_1 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv255_1, quantizer_quantizers_5_out_proj_weight_v, quantizer_quantizers_5_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv365 = R.call_tir(cls.reshape19, (quantizer_quantizers_5_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv257_1 = R.call_tir(cls.fused_conv1d41_add30, (permute_dims23, lv256_1, lv365), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            subtract11 = R.call_tir(cls.subtract1, (subtract9, lv257_1), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            lv258_1 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_6_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv259_1 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv258_1, quantizer_quantizers_6_in_proj_weight_v, quantizer_quantizers_6_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv371 = R.call_tir(cls.reshape21, (quantizer_quantizers_6_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv260 = R.call_tir(cls.fused_conv1d40_add31, (subtract11, lv259_1, lv371), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            permute_dims24 = R.call_tir(cls.transpose, (lv260,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            reshape82 = R.call_tir(cls.reshape22, (permute_dims24,), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv261 = R.call_tir(cls.fused_tir_square29_sum29, (reshape82,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv262 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt12_divide29, (lv261, reshape82), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv263 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_6_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv264 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv263, quantizer_quantizers_6_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims25 = R.call_tir(cls.transpose1, (lv264,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv265_1 = R.call_tir(cls.fused_tir_square29_sum29, (lv262,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv266_1 = R.call_tir(cls.fused_tir_square30_sum30, (lv264,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims26 = R.call_tir(cls.transpose2, (lv266_1,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv267_1 = R.call_tir(cls.fused_matmul_multiply29_subtract_add32, (lv262, permute_dims25, lv265_1, permute_dims26), out_sinfo=R.Tensor((batch_size, 1024), dtype="float32"))
            argsort6: R.Tensor((batch_size, 1024), dtype="int32") = R.argsort(lv267_1, axis=1, descending=False, dtype="int32")
            take12 = R.call_tir(cls.take, (argsort6, metadata["relax.expr.Constant"][6]), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape83 = R.call_tir(cls.reshape23, (take12,), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape84 = R.call_tir(cls.reshape24, (reshape83,), out_sinfo=R.Tensor((batch_size,), dtype="int32"))
            take13 = R.call_tir(cls.take1, (quantizer_quantizers_6_codebook_weight, reshape84), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            reshape85 = R.call_tir(cls.reshape25, (take13,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            permute_dims27 = R.call_tir(cls.transpose3, (reshape85,), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            lv268_1 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_6_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv269 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv268_1, quantizer_quantizers_6_out_proj_weight_v, quantizer_quantizers_6_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv377 = R.call_tir(cls.reshape19, (quantizer_quantizers_6_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv270 = R.call_tir(cls.fused_conv1d41_add30, (permute_dims27, lv269, lv377), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            subtract13 = R.call_tir(cls.subtract1, (subtract11, lv270), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            lv271 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_7_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv272 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv271, quantizer_quantizers_7_in_proj_weight_v, quantizer_quantizers_7_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv383 = R.call_tir(cls.reshape21, (quantizer_quantizers_7_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv273 = R.call_tir(cls.fused_conv1d40_add31, (subtract13, lv272, lv383), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            permute_dims28 = R.call_tir(cls.transpose, (lv273,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            reshape86 = R.call_tir(cls.reshape22, (permute_dims28,), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv274_1 = R.call_tir(cls.fused_tir_square29_sum29, (reshape86,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv275_1 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt12_divide29, (lv274_1, reshape86), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv276_1 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_7_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv277_1 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv276_1, quantizer_quantizers_7_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims29 = R.call_tir(cls.transpose1, (lv277_1,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv278_1 = R.call_tir(cls.fused_tir_square29_sum29, (lv275_1,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv279 = R.call_tir(cls.fused_tir_square30_sum30, (lv277_1,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims30 = R.call_tir(cls.transpose2, (lv279,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv280 = R.call_tir(cls.fused_matmul_multiply29_subtract_add32, (lv275_1, permute_dims29, lv278_1, permute_dims30), out_sinfo=R.Tensor((batch_size, 1024), dtype="float32"))
            argsort7: R.Tensor((batch_size, 1024), dtype="int32") = R.argsort(lv280, axis=1, descending=False, dtype="int32")
            take14 = R.call_tir(cls.take, (argsort7, metadata["relax.expr.Constant"][7]), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape87 = R.call_tir(cls.reshape23, (take14,), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape88 = R.call_tir(cls.reshape24, (reshape87,), out_sinfo=R.Tensor((batch_size,), dtype="int32"))
            take15 = R.call_tir(cls.take1, (quantizer_quantizers_7_codebook_weight, reshape88), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            reshape89 = R.call_tir(cls.reshape25, (take15,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            permute_dims31 = R.call_tir(cls.transpose3, (reshape89,), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            lv281 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_7_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv282 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv281, quantizer_quantizers_7_out_proj_weight_v, quantizer_quantizers_7_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv389 = R.call_tir(cls.reshape19, (quantizer_quantizers_7_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv283 = R.call_tir(cls.fused_conv1d41_add30, (permute_dims31, lv282, lv389), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            subtract15 = R.call_tir(cls.subtract1, (subtract13, lv283), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            lv284_1 = R.call_tir(cls.fused_tir_square28_sum28, (quantizer_quantizers_8_in_proj_weight_v,), out_sinfo=R.Tensor((8, 1, 1), dtype="float32"))
            lv285_1 = R.call_tir(cls.fused_tir_sqrt11_divide28_multiply28, (lv284_1, quantizer_quantizers_8_in_proj_weight_v, quantizer_quantizers_8_in_proj_weight_g), out_sinfo=R.Tensor((8, 1024, 1), dtype="float32"))
            lv395 = R.call_tir(cls.reshape21, (quantizer_quantizers_8_in_proj_bias,), out_sinfo=R.Tensor((1, 8, 1), dtype="float32"))
            lv286_1 = R.call_tir(cls.fused_conv1d40_add31, (subtract15, lv285_1, lv395), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            permute_dims32 = R.call_tir(cls.transpose, (lv286_1,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            reshape90 = R.call_tir(cls.reshape22, (permute_dims32,), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv287_1 = R.call_tir(cls.fused_tir_square29_sum29, (reshape90,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv288 = R.call_tir(cls.fused_broadcast_to_maximum_tir_sqrt12_divide29, (lv287_1, reshape90), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            lv289 = R.call_tir(cls.fused_tir_square30_sum30, (quantizer_quantizers_8_codebook_weight,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            lv290 = R.call_tir(cls.fused_broadcast_to1_maximum1_tir_sqrt13_divide30, (lv289, quantizer_quantizers_8_codebook_weight), out_sinfo=R.Tensor((1024, 8), dtype="float32"))
            permute_dims33 = R.call_tir(cls.transpose1, (lv290,), out_sinfo=R.Tensor((8, 1024), dtype="float32"))
            lv291 = R.call_tir(cls.fused_tir_square29_sum29, (lv288,), out_sinfo=R.Tensor((batch_size, 1), dtype="float32"))
            lv292 = R.call_tir(cls.fused_tir_square30_sum30, (lv290,), out_sinfo=R.Tensor((1024, 1), dtype="float32"))
            permute_dims34 = R.call_tir(cls.transpose2, (lv292,), out_sinfo=R.Tensor((1, 1024), dtype="float32"))
            lv293_1 = R.call_tir(cls.fused_matmul_multiply29_subtract_add32, (lv288, permute_dims33, lv291, permute_dims34), out_sinfo=R.Tensor((batch_size, 1024), dtype="float32"))
            argsort8: R.Tensor((batch_size, 1024), dtype="int32") = R.argsort(lv293_1, axis=1, descending=False, dtype="int32")
            take16 = R.call_tir(cls.take, (argsort8, metadata["relax.expr.Constant"][8]), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape91 = R.call_tir(cls.reshape23, (take16,), out_sinfo=R.Tensor((batch_size, 1), dtype="int32"))
            reshape92 = R.call_tir(cls.reshape24, (reshape91,), out_sinfo=R.Tensor((batch_size,), dtype="int32"))
            take17 = R.call_tir(cls.take1, (quantizer_quantizers_8_codebook_weight, reshape92), out_sinfo=R.Tensor((batch_size, 8), dtype="float32"))
            reshape93 = R.call_tir(cls.reshape25, (take17,), out_sinfo=R.Tensor((batch_size, 1, 8), dtype="float32"))
            permute_dims35 = R.call_tir(cls.transpose3, (reshape93,), out_sinfo=R.Tensor((batch_size, 8, 1), dtype="float32"))
            lv294 = R.call_tir(cls.fused_tir_square31_sum31, (quantizer_quantizers_8_out_proj_weight_v,), out_sinfo=R.Tensor((1024, 1, 1), dtype="float32"))
            lv295 = R.call_tir(cls.fused_tir_sqrt10_divide31_multiply30, (lv294, quantizer_quantizers_8_out_proj_weight_v, quantizer_quantizers_8_out_proj_weight_g), out_sinfo=R.Tensor((1024, 8, 1), dtype="float32"))
            lv401 = R.call_tir(cls.reshape19, (quantizer_quantizers_8_out_proj_bias,), out_sinfo=R.Tensor((1, 1024, 1), dtype="float32"))
            lv296 = R.call_tir(cls.fused_zeros_add33_add33_add33_add33_add33_add33_add33_add33_conv1d41_add30_add33, (lv192, lv205_1, lv218_1, lv231, lv244, lv257_1, lv270, lv283, permute_dims35, lv295, lv401), out_sinfo=R.Tensor((batch_size, 1024, 1), dtype="float32"))
            gv1: R.Tuple(R.Tuple(R.Tensor((batch_size, 1024, 1), dtype="float32"), R.Tuple(R.Tensor((batch_size, 1), dtype="int32"), R.Tensor((batch_size, 1), dtype="int32"), R.Tensor((batch_size, 1), dtype="int32"), R.Tensor((batch_size, 1), dtype="int32"), R.Tensor((batch_size, 1), dtype="int32"), R.Tensor((batch_size, 1), dtype="int32"), R.Tensor((batch_size, 1), dtype="int32"), R.Tensor((batch_size, 1), dtype="int32"), R.Tensor((batch_size, 1), dtype="int32"))), R.Tuple(R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object, R.Object)) = (lv296, (reshape59, reshape63, reshape67, reshape71, reshape75, reshape79, reshape83, reshape87, reshape91)), (_io, encoder_block_layers_0_cache_cache, encoder_block_layers_0_downsampling_delay_cache, encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_cache_cache, encoder_block_layers_1_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_cache_cache, encoder_block_layers_1_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_1_block_layers_0_block_paddings_0_cache, encoder_block_layers_1_block_layers_0_block_paddings_1_cache, encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_cache_cache, encoder_block_layers_1_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_cache_cache, encoder_block_layers_1_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_1_block_layers_1_block_paddings_0_cache, encoder_block_layers_1_block_layers_1_block_paddings_1_cache, encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache, encoder_block_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache, encoder_block_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_1_block_layers_2_block_paddings_0_cache, encoder_block_layers_1_block_layers_2_block_paddings_1_cache, encoder_block_layers_1_block_layers_4_cache_cache, encoder_block_layers_1_block_layers_4_downsampling_delay_cache, encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_cache_cache, encoder_block_layers_2_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_cache_cache, encoder_block_layers_2_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_2_block_layers_0_block_paddings_0_cache, encoder_block_layers_2_block_layers_0_block_paddings_1_cache, encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_cache_cache, encoder_block_layers_2_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_cache_cache, encoder_block_layers_2_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_2_block_layers_1_block_paddings_0_cache, encoder_block_layers_2_block_layers_1_block_paddings_1_cache, encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache, encoder_block_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache, encoder_block_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_2_block_layers_2_block_paddings_0_cache, encoder_block_layers_2_block_layers_2_block_paddings_1_cache, encoder_block_layers_2_block_layers_4_cache_cache, encoder_block_layers_2_block_layers_4_downsampling_delay_cache, encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_cache_cache, encoder_block_layers_3_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_cache_cache, encoder_block_layers_3_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_3_block_layers_0_block_paddings_0_cache, encoder_block_layers_3_block_layers_0_block_paddings_1_cache, encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_cache_cache, encoder_block_layers_3_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_cache_cache, encoder_block_layers_3_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_3_block_layers_1_block_paddings_0_cache, encoder_block_layers_3_block_layers_1_block_paddings_1_cache, encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache, encoder_block_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache, encoder_block_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_3_block_layers_2_block_paddings_0_cache, encoder_block_layers_3_block_layers_2_block_paddings_1_cache, encoder_block_layers_3_block_layers_4_cache_cache, encoder_block_layers_3_block_layers_4_downsampling_delay_cache, encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_cache_cache, encoder_block_layers_4_block_layers_0_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_cache_cache, encoder_block_layers_4_block_layers_0_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_4_block_layers_0_block_paddings_0_cache, encoder_block_layers_4_block_layers_0_block_paddings_1_cache, encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_cache_cache, encoder_block_layers_4_block_layers_1_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_cache_cache, encoder_block_layers_4_block_layers_1_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_4_block_layers_1_block_paddings_0_cache, encoder_block_layers_4_block_layers_1_block_paddings_1_cache, encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache, encoder_block_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache, encoder_block_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, encoder_block_layers_4_block_layers_2_block_paddings_0_cache, encoder_block_layers_4_block_layers_2_block_paddings_1_cache, encoder_block_layers_4_block_layers_4_cache_cache, encoder_block_layers_4_block_layers_4_downsampling_delay_cache, encoder_block_layers_6_cache_cache, encoder_block_layers_6_downsampling_delay_cache, decoder_model_layers_0_cache_cache, decoder_model_layers_0_downsampling_delay_cache, decoder_model_layers_1_block_layers_1_cache_cache, decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_cache_cache, decoder_model_layers_1_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_cache_cache, decoder_model_layers_1_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_1_block_layers_2_block_paddings_0_cache, decoder_model_layers_1_block_layers_2_block_paddings_1_cache, decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_cache_cache, decoder_model_layers_1_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_cache_cache, decoder_model_layers_1_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_1_block_layers_3_block_paddings_0_cache, decoder_model_layers_1_block_layers_3_block_paddings_1_cache, decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_cache_cache, decoder_model_layers_1_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_cache_cache, decoder_model_layers_1_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_1_block_layers_4_block_paddings_0_cache, decoder_model_layers_1_block_layers_4_block_paddings_1_cache, decoder_model_layers_2_block_layers_1_cache_cache, decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_cache_cache, decoder_model_layers_2_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_cache_cache, decoder_model_layers_2_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_2_block_layers_2_block_paddings_0_cache, decoder_model_layers_2_block_layers_2_block_paddings_1_cache, decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_cache_cache, decoder_model_layers_2_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_cache_cache, decoder_model_layers_2_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_2_block_layers_3_block_paddings_0_cache, decoder_model_layers_2_block_layers_3_block_paddings_1_cache, decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_cache_cache, decoder_model_layers_2_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_cache_cache, decoder_model_layers_2_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_2_block_layers_4_block_paddings_0_cache, decoder_model_layers_2_block_layers_4_block_paddings_1_cache, decoder_model_layers_3_block_layers_1_cache_cache, decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_cache_cache, decoder_model_layers_3_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_cache_cache, decoder_model_layers_3_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_3_block_layers_2_block_paddings_0_cache, decoder_model_layers_3_block_layers_2_block_paddings_1_cache, decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_cache_cache, decoder_model_layers_3_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_cache_cache, decoder_model_layers_3_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_3_block_layers_3_block_paddings_0_cache, decoder_model_layers_3_block_layers_3_block_paddings_1_cache, decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_cache_cache, decoder_model_layers_3_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_cache_cache, decoder_model_layers_3_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_3_block_layers_4_block_paddings_0_cache, decoder_model_layers_3_block_layers_4_block_paddings_1_cache, decoder_model_layers_4_block_layers_1_cache_cache, decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_cache_cache, decoder_model_layers_4_block_layers_2_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_cache_cache, decoder_model_layers_4_block_layers_2_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_4_block_layers_2_block_paddings_0_cache, decoder_model_layers_4_block_layers_2_block_paddings_1_cache, decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_cache_cache, decoder_model_layers_4_block_layers_3_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_cache_cache, decoder_model_layers_4_block_layers_3_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_4_block_layers_3_block_paddings_0_cache, decoder_model_layers_4_block_layers_3_block_paddings_1_cache, decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_cache_cache, decoder_model_layers_4_block_layers_4_block_branches_0_layers_1_downsampling_delay_cache, decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_cache_cache, decoder_model_layers_4_block_layers_4_block_branches_0_layers_3_downsampling_delay_cache, decoder_model_layers_4_block_layers_4_block_paddings_0_cache, decoder_model_layers_4_block_layers_4_block_paddings_1_cache, decoder_model_layers_6_cache_cache, decoder_model_layers_6_downsampling_delay_cache)
            R.output(gv1)
        return gv1

# Metadata omitted. Use show_meta=True in script() method to show it.